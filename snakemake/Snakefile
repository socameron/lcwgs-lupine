import os
import glob
import subprocess
import pandas as pd

# Define a function to extract the sample name from a fastq.gz filename
def extract_sample_name(filename):
    # Get the base filename without the directory
    base_filename = os.path.basename(filename)
    
    # Split the filename by underscores
    parts = base_filename.split('_')
    
    # Find the part that contains 'L001' and get the sample name before it
    for i, part in enumerate(parts):
        if 'L001' in part:
            return '_'.join(parts[:i])

# Defining wildcard for haplotype and sequence batch #
wildcard_constraints:
    hap = r'1|2',
    batch = r'1|2'

# Directory containing the fastq.gz files 
# Change 1 to {batch}
directory = "data/batch_1"

# Get a list of all fastq.gz files in the directory
fastq_files = glob.glob(os.path.join(directory, "*.fastq.gz"))

# Initialize a set to store unique sample names
SAMPLES = set()

# Iterate over the fastq.gz files and extract sample names
for filename in fastq_files:
    sample_name = extract_sample_name(filename)
    SAMPLES.add(sample_name)

# Convert the set to a sorted list for consistent order
SAMPLES = sorted(list(SAMPLES))
POPULATIONS = ("HPW", "IDNP-MW", "LCTGP", "MFNP", "PPP", "RLPLV", "SWCP")
POPULATIONS2 = ("CSS", "FMB", "")

with open("results/scaffolds/hap1_scaffolds.txt", "r") as file:
    HAP1SCAFFOLDS = [line.strip() for line in file.readlines()]

with open("results/scaffolds/hap2_scaffolds.txt", "r") as file:
    HAP2SCAFFOLDS = [line.strip() for line in file.readlines()]

# Split scaffold names by comma and create a list
HAP1SCAFFOLDS = [name.strip() for name in HAP1SCAFFOLDS]
HAP2SCAFFOLDS = [name.strip() for name in HAP2SCAFFOLDS]
HAP1SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP1SCAFFOLDS]
HAP2SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP2SCAFFOLDS]

# Function give full scaffold name given prefix of scaffold
def map_prefix_to_full_scaffold(prefix, hap_type):
    scaffold_list = HAP1SCAFFOLDS if hap_type == 1 else HAP2SCAFFOLDS
    for scaffold in scaffold_list:
        if scaffold.startswith(prefix):
            return scaffold
    return None  # Return None or an appropriate default if not found

BH_VARS=[50,40,30,20,10,5]


# Update the SAMPLES list with merged sample names 
# ONLY INCLUDE AFTER rule merge_replicates IS RUN. MUST RUN WORKFLOW AT 2 POINTS. !!!
merged_samples = ["MFNP-48_2_269001_S193", "IDNP-MW-6_2_269002_S194", "RLPLV-13_2_269003_S195"]
samples_to_remove = ["MFNP-48_2-2695929_S1", "MFNP-48_2-2697940_S99", "IDNP-MW-6_2-2698020_S129", "IDNP-MW-6_2-2698118_S177", "RLPLV-13_2-2696026_S48", "RLPLV-13_2-2696110_S82"]
SAMPLES = set([sample for sample in SAMPLES if sample not in samples_to_remove])
SAMPLES.update(merged_samples)
SAMPLES = sorted(list(SAMPLES))

###########################
## BEGINNING OF WORKFLOW ##
###########################


# final files desired (can change)
rule all:
  input:
     expand("results/depths/hap2/{hap2scaffold_prefix}_depth_est.txt.gz", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)


#### DATA PREPARATION ####

# Trim adapter ends off each sequence file using Trimmomatic 
# When batch 2 is available, use wildcard {batch}
rule trim_reads:
  input:
    r1="data/batch_1/{sample}_L001_R1_001.fastq.gz",
    r2="data/batch_1/{sample}_L001_R2_001.fastq.gz",
  output:
    r1="results/fastq_trimmed/batch_1/{sample}_R1.fastq.gz",
    r1_unp="results/fastq_trimmed/batch_1/{sample}_R1_unpaired.fastq.gz",
    r2="results/fastq_trimmed/batch_1/{sample}_R2.fastq.gz",
    r2_unp="results/fastq_trimmed/batch_1/{sample}_R2_unpaired.fastq.gz"
  log:
    "results/logs/trim_reads/{sample}.log"
  envmodules:
    "trimmomatic/0.39"
  params:
    adapters="$EBROOTTRIMMOMATIC/adapters/TruSeq3-PE-2.fa"
  shell:
    "java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar PE {input.r1} {input.r2} "
    "{output.r1} {output.r1_unp} {output.r2} {output.r2_unp} "
    "ILLUMINACLIP:{params.adapters}:2:30:10 "
    "LEADING:3 "
    "TRAILING:3 "
    "SLIDINGWINDOW:4:15 "
    "MINLEN:36 2> {log}"


# Unzip trimmed files as fastqc 0.12.0 cannot properly read compressed files
rule unzip_files:
  input:
    zipped_r1="results/fastq_trimmed/batch_1/{sample}_R1.fastq.gz",
    zipped_r2="results/fastq_trimmed/batch_1/{sample}_R2.fastq.gz"
  output: 
    unzipped_r1="results/fastq_trimmed_unzip/batch_1/{sample}_R1.fastq",
    unzipped_r2="results/fastq_trimmed_unzip/batch_1/{sample}_R2.fastq"
  shell:
    "gunzip -c {input.zipped_r1} > {output.unzipped_r1} && gunzip -c {input.zipped_r2} > {output.unzipped_r2}"


# Run FastQC per each trimmed sequence file
rule fastqc:
  input:
    fastq_r1="results/fastq_trimmed_unzip/batch_1/{sample}_R1.fastq",
    fastq_r2="results/fastq_trimmed_unzip/batch_1/{sample}_R2.fastq"
  output:
    html_report_r1="results/fastqc/batch_1/{sample}_R1_fastqc.html",
    zip_report_r1="results/fastqc/batch_1/{sample}_R1_fastqc.zip",
    html_report_r2="results/fastqc/batch_1/{sample}_R2_fastqc.html",
    zip_report_r2="results/fastqc/batch_1/{sample}_R2_fastqc.zip"
  envmodules:
    "fastqc/0.12.0"
  shell:
    "fastqc {input.fastq_r1} {input.fastq_r2} --outdir results/fastqc"


# Create an aggregated FastQC report using MultiQC.
# Note that we create separate MultiQC reports for both batch 1 and 2
rule multiqc_raw:
  input:
    fastqc_dir="results/fastqc/batch_1"
  output:
    html_report="results/multiqc/multiqc_raw/multiqc_report_batch_1.html"
  params:
    output_dir="results/multiqc/multiqc_raw"
  shell:
    "multiqc -o {params.output_dir} {input.fastqc_dir} "


# Creating faidx index for reference genome
rule faidx_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta",
  output:
    "data/reference/hap{hap}/lupinehap{hap}.fasta.fai",
  log:
    "results/logs/refgen/lupinehap{hap}_faidx.log",
  envmodules:
    "samtools/1.17"
  shell:
    "samtools faidx {input} 2> {log} "


# Rules for indexing reference genomes (haplotypes 1 and 2)
rule index_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa"),
  log:
    "results/logs/refgen/lupinehap{hap}_bwa_index.log"
  envmodules:
    "bwa/0.7.17"
  shell:
    "bwa index {input} 2> {log}"


# Rules for creating dictionary files
rule create_dict:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    "data/reference/hap{hap}/lupinehap{hap}.dict"
  log:
    "results/logs/refgen/hap{hap}_dict.log"
  envmodules:
    "samtools/1.17"
  shell:
    "samtools dict {input} > {output} 2> {log}"


# Removing reads smaller than 70bp so 'bwa mem' works better
# Note for a read to be kept, it must be greater than 70bp in BOTH read 1 and 2.
# There are some tools available in envmodules 'seqkit', but I found none that were compatible
rule filter_short_reads_with_seqkit:
  input:
    r1="results/fastq_trimmed/batch_1/{sample}_R1.fastq.gz",
    r2="results/fastq_trimmed/batch_1/{sample}_R2.fastq.gz"
  output:
    r1_filtered="results/fastq_filtered/batch_1/{sample}_R1_filtered.fastq.gz",
    r2_filtered="results/fastq_filtered/batch_1/{sample}_R2_filtered.fastq.gz"
  envmodules:
    "seqkit/2.3.1"
  log:
    "results/logs/filter_fastq_70bp/batch_1/{sample}_filter_fastq.log"
  shell:
    """
    seqkit seq -m 70 {input.r1} | gzip > {output.r1_filtered}
    seqkit seq -m 70 {input.r2} | gzip > {output.r2_filtered}
    """


# Pair filtered reads back together
# Note: naming convention stays the same! Just different folder
rule pair_filtered_reads:
  input:
    r1_filtered="results/fastq_filtered/batch_1/{sample}_R1_filtered.fastq.gz",
    r2_filtered="results/fastq_filtered/batch_1/{sample}_R2_filtered.fastq.gz"
  output:
    r1_paired="results/fastq_paired/batch_1/{sample}_R1_filtered.fastq.gz",
    r2_paired="results/fastq_paired/batch_1/{sample}_R2_filtered.fastq.gz"
  envmodules:
    "seqkit/2.3.1"
  log:
    "results/logs/pair_fastq_70bp/batch_1/{sample}_pair_fastq.log"
  shell:
    """
    seqkit pair \
    -1 {input.r1_filtered} \
    -2 {input.r2_filtered} \
    -O results/fastq_paired/batch_1
    """


# Mapping/Aligning reads to haplotypes
rule map_reads:
  input:
    r1="results/fastq_paired/batch_1/{sample}_R1_filtered.fastq.gz",
    r2="results/fastq_paired/batch_1/{sample}_R2_filtered.fastq.gz",
    genome="data/reference/hap{hap}/lupinehap{hap}.fasta",
    idx=multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa")
  output:
    "results/bam_raw/hap{hap}/{sample}_hap{hap}.bam"
  log:
    "results/logs/map_reads/hap{hap}/{sample}_hap{hap}.log"
  envmodules:
    "bwa/0.7.17",
    "samtools/1.17"
  threads: 8 
  params:
    RG="-R '@RG\\tID:{sample}\\tSM:{sample}\\tPL:ILLUMINA' "
    # Excluded information on flowcell, lane, and barcode index b/c not thought necessary
  shell:
    """
    (bwa mem {params.RG} -t {threads} {input.genome} {input.r1} {input.r2} |\
    samtools view -u |\
    samtools sort - > {output}) 2> {log}
    """













#### DATA QUALITY CONTROL ####

# Steps in QC :
  # 1. Mark PCR and optical duplicates
  # 2. Identify paralogous regions due to duplication events
      # - Requires ngsParalog and 1st run of ANGSD
      # - ANGSD: SNP call and input SNPs (aka polymorphic sites) into ngsParalog
      # - ngsParalog: probablistic call of paralogous regions
      # - After ngsParalog, calculate p-values based on chi-sq df = 1 with Benjamini-Hochberg correction
  # 3. Verify heterozygote excess is reduced
      # - Re-run ANGSD excluding paralogous regions and PCR duplicates
      #   - Visualize SFS

## STEP 1: MERGE REPLICATES & MARK PCR & OPTICAL DUPLICATES 

# Merge replicates
rule merge_replicates:
  input:
    MFNP_48A="results/bam_raw/hap{hap}/MFNP-48_2-2695929_S1_hap{hap}.bam",
    MFNP_48B="results/bam_raw/hap{hap}/MFNP-48_2-2697940_S99_hap{hap}.bam",
    IDNP_6A="results/bam_raw/hap{hap}/IDNP-MW-6_2-2698020_S129_hap{hap}.bam",
    IDNP_6B="results/bam_raw/hap{hap}/IDNP-MW-6_2-2698118_S177_hap{hap}.bam",
    RLPLV_13A="results/bam_raw/hap{hap}/RLPLV-13_2-2696026_S48_hap{hap}.bam",
    RLPLV_13B="results/bam_raw/hap{hap}/RLPLV-13_2-2696110_S82_hap{hap}.bam"
  output:
    MFNP_48="results/bam_raw/hap{hap}/MFNP-48_2_269001_S193_hap{hap}.bam",
    IDNP_6="results/bam_raw/hap{hap}/IDNP-MW-6_2_269002_S194_hap{hap}.bam",
    RLPLV_13="results/bam_raw/hap{hap}/RLPLV-13_2_269003_S195_hap{hap}.bam"
  params:
    archive="results/bam_raw/archive/hap{hap}"
  envmodules:
    "samtools/1.17"
  shell:
    """
    samtools merge -o {output.MFNP_48} {input.MFNP_48A} {input.MFNP_48B}
    samtools merge -o {output.IDNP_6} {input.IDNP_6A} {input.IDNP_6B}
    samtools merge -o {output.RLPLV_13} {input.RLPLV_13A} {input.RLPLV_13B}
    mkdir -p {params.archive}
    mv {input.MFNP_48A} {input.MFNP_48B} {input.IDNP_6A} {input.IDNP_6B} {input.RLPLV_13A} {input.RLPLV_13B} {params.archive}
    """



# Marking and removing PCR duplicates + index
# Note that: /bam_mkdup/ is where marked duplicates are marked but not removed. This is backed up in the projects folder.
rule mark_remove_duplicates:
  input:
    "results/bam_raw/hap{hap}/{sample}_hap{hap}.bam"
  output:
    bam="results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bam",
    bai="results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bai",
    metrics="results/qc/mkdup_metrics/{sample}_hap{hap}.metrics"
  log:
    "results/logs/mark_remove_duplicates/hap{hap}/{sample}_hap{hap}.log"
  envmodules:
    "gatk/4.2.5.0"
  shell:
    """
    gatk MarkDuplicates \
    --CREATE_INDEX \
    -I {input} \
    -O {output.bam} \
    -M {output.metrics} \
    --REMOVE_DUPLICATES true \
    2> {log}
    """


# Clip overlapping reads
rule clip_overlapping_reads:
  input:
    bam="results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bam"
  output:
    clipped_bam="results/bam_clipped/hap{hap}/{sample}_hap{hap}_clipped.bam",
    clipped_index="results/bam_clipped/hap{hap}/{sample}_hap{hap}_clipped.bai"
  log:
    "results/logs/clip_overlap/hap{hap}/{sample}_clip_overlapping_reads.log"
  shell:
    """
    module --force purge
    module load nixpkgs/16.09 intel/2018.3
    module load bamutil/1.0.14
    bam clipOverlap --in {input.bam} --out {output.clipped_bam} 2> {log}
    module --force purge
    module load StdEnv/2023 samtools/1.18
    samtools index -b {output.clipped_bam} -o {output.clipped_index} --threads 2
    module --force purge
    """


# Create bam list per population for entry into realign indels
rule generate_clipped_bam_list_per_population:
  input:
    expand("results/bam_clipped/hap{hap}/{sample}_hap{hap}_clipped.bam", sample=SAMPLES, hap=(1,2)),
  output:
    "data/lists/hap{hap}/{population}_clipped_hap{hap}.list"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population
    hap = wildcards.hap

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap{hap}_" in bam_file:
                output.write(f"{bam_file}\n")


# Create interval list of indels
rule indel_list:
  input:
    bam_list="data/lists/hap{hap}/{population}_clipped_hap{hap}.list",
    reference="data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    intervals="data/lists/hap{hap}/{population}_hap{hap}_indels.intervals"
  log:    
    "results/logs/indel_list/hap{hap}/{population}_hap{hap}_indel_list.log"
  shell:
    """
    module --force purge
    module load nixpkgs/16.09 gatk/3.8
    java -Xmx16g \
    -jar $EBROOTGATK/GenomeAnalysisTK.jar \
    -T RealignerTargetCreator \
    -R {input.reference} \
    -I {input.bam_list} \
    -o {output.intervals} \
    -drf BadMate \
    2> {log}
    module --force purge
    """


# Realign reads around indels - necessary if using ANGSD
rule realign_indels:
  input:
    bam="results/bam_clipped/hap{hap}/{sample}_hap{hap}_clipped.bam",
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    intervals=lambda wildcards: "data/lists/hap" + wildcards.hap + "/" + next(pop for pop in POPULATIONS if pop in wildcards.sample) + f"_hap{wildcards.hap}_indels.intervals"
  output:
    realigned_bam="results/bam_realign/hap{hap}/{sample}_hap{hap}_realign.bam"
  log:
    "results/logs/realign_indels/hap{hap}/{sample}_hap{hap}_realign_indels.log"
  shell:
    """
    module --force purge
    module load nixpkgs/16.09 gatk/3.8
    java -Xmx16g \
    -jar $EBROOTGATK/GenomeAnalysisTK.jar \
    -T IndelRealigner \
    -I {input.bam} \
    -R {input.ref} \
    -targetIntervals {input.intervals} \
    -o {output.realigned_bam}\
    &> {log}
    module --force purge
    """
  

# Rule to create .txt file of BAM files for each haplotype
rule generate_bam_list_per_haplotype:
  input:
    expand("results/bam_realign/hap{hap}/{sample}_hap{hap}_realign.bam", sample=SAMPLES, hap=(1,2)),
  output:
    "data/lists/hap{hap}/all_realign_hap{hap}.txt"
  run:
    bam_files = input
    output_file = output[0]
    hap = wildcards.hap

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if f"_hap{hap}_" in bam_file:
                output.write(f"{bam_file}\n")


# Merge bams
rule merge_bams:
  input:
    bam_list="data/lists/hap2/all_realign_hap2.txt"
  output:
    merged_bam="results/bam_merge/hap2/merged_hap2.bam"
  envmodules:
    "samtools/1.17"
  shell:
    """
    samtools merge -b {input.bam_list} {output.merged_bam}
    """


# Estimate depth per bp per samplen
# -d flag : set max-depth to infinity
# -A flag : don't skip anamalous read pairs marked w/ FLAG but missing properly paired flag set
# -q flag : minimum mapping quality
# -Q flag : minimum base quality
# -r flag : specify regions for pileup; needs indexed BAM files
# -a flag : all positions including unused reference sequences
# -f flag : faidx-indexed reference fasta file
# -ff flag : exclude SECONDARY, QCFAIL, DUP
# Note: previously used mpileup with all 189 .bam files, but now merged them as one file because the positions are different across samples with flag -s
# Using ngsQC tools 'bamstats', created by Dr. Tyler Linderoth

rule estimate_depth:
  input:
    bam="results/bam_merge/hap2/merged_hap2.bam",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    depth="results/depths/hap2/{hap2scaffold_prefix}_depth_est.txt.gz"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.17"
  shell:
    """
    bamstats mpileup -b {input.bam} -r {params.hap2scaffold}\
    -A -d 77000000 -q 0 -Q 0 --ff UNMAP,SECONDARY,QCFAIL,DUP\
    -s -aa -f {input.ref} | gzip > {output.depth}
    """


# Plot aggregate depths per population
rule plot_aggregate_depths:
  input:
    aggregated_depth=expand("results/depths/hap2/{hap2scaffold_prefix}_depth_est.txt.gz", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    plot="results/plots/hap2/depths/aggregate_depth_histogram.png"
  shell:
    """
    python scripts/plot_depths.py {input.aggregated_depth} {output.plot}
    """








## STEP 2: IDENTIFY PARALOGOUS REGIONS 

# Rule to create .txt file of BAM files per population
rule generate_bam_list_per_population:
  input:
    expand("results/bam_realign/hap{hap}/{sample}_hap{hap}_realign.bam", sample=SAMPLES, hap=(1,2)),
  output:
    "data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population
    hap = wildcards.hap

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap{hap}_" in bam_file:
                output.write(f"{bam_file}\n")


# Call SNPs with stringent rules for input to ngsParalog
# Question:  is -minInd 5 and -SNP_pvalue 0.05 so very liberal SNP calls
rule angsd_raw_SNP:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.arg",
    mafs_file="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.mafs.gz",
    hwe_file="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.hwe.gz",
    depth_sample="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.depthSample",
    depth_global="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign",
    hap="{hap}",
    population="{population}"
  log:
    "results/logs/angsd/hap{hap}/raw_realign/angsd_SNP_raw_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -C 50\
    -GL 1\
    -SNP_pval 0.05\
    -minMapQ 30\
    -minQ 20 \
    -minInd 5\
    -minMaf .05\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    &> {log}
    """


# Create BED files so including only SNPs into ngsParalog
# NOTE: BED files indexed at 0bp because SAMtools to create pileup requires 0bp index
rule convert_mafs_to_bed:
  input:
    mafs_gz="results/angsd/hap{hap}/raw_realign/{population}_raw_SNPs_realign.mafs.gz"
  output:
    bed_file="results/bed/hap{hap}/raw_realign/{population}_raw_SNPs_realign.BED"
  shell:
   """
   gunzip -c {input.mafs_gz} | awk 'NR>1 {{print $1, $2 - 1, $2}}' > {output.bed_file}
   dos2unix {output.bed_file}  # Add this line to convert line endings
   """

  
# Create wildcard for scaffold names - needed to run ngsParalog per scaffold 
rule extract_scaffolds:
  input:
    index="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt"
  shell:
    """
    awk '{{print $1":1-"$2}}' {input.index} > {output.scaffolds}
    """


# Run ngsParalog on all SNPs across the genome (parallelized by running 1 job per scaffold)
rule ngsParalog_hap1: 
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_realign/hap1/{sample}_hap1_realign.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    ref="data/reference/hap1/lupinehap1.fasta",
    bed_file="results/bed/hap1/raw_realign/{population}_raw_SNPs_realign.BED"
  output:
    paralog_output="results/ngs_paralog/hap1/realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap1/realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}.log"
  params:
    hap1scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap1scaffold_prefix, 1)
  envmodules:
    "samtools/1.17"
  shell:
    """
    rm -f {output.paralog_output} #remove existing output file if it exists
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -d 1000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap1scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} -allow_overwrite 1 \
    -minQ 20 -minind 10 -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


rule ngsParalog_hap2:
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_realign/hap2/{sample}_hap2_realign.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    ref="data/reference/hap2/lupinehap2.fasta",
    bed_file="results/bed/hap2/raw_realign/{population}_raw_SNPs_realign.BED"
  output:
    paralog_output="results/ngs_paralog/hap2/realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap2/realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}.log"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.17"
  shell:
    """
    rm -f {output.paralog_output} #remove existing output file if it exists
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -d 1000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap2scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} -allow_overwrite 1 \
    -minQ 20 -minind 10 -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


# Combine all ngsParalog outputs together into one file per population
rule concatenate_paralog_hap1:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap1/realign/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}.lr", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap1/concat_realign/{population}_paralog_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/concat_realign/{population}_paralog_realign_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


rule concatenate_paralog_hap2:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap2/realign/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}.lr", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap2/concat_realign/{population}_paralog_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/concat_realign/{population}_paralog_realign_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


# Print summary of calcLR quantile ranges
rule quantile_summary:
  input:
    "results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr"
  output:
    "results/ngs_paralog/hap{hap}/quantile_summary/{population}_calcLR_quantile_summary.txt"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/calcLR_quantile_summary.R {input} {output}"


# Identify false positives from ngsParalog (grab only true Paralogs) and filter out
# NOTE: R script indexes positions back to 1bp start
rule ngsParalog_false_pos:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr"
  output:
    deviant_snps="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_BH_correction.BED",
    deviant_snps_bp1="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_bp1_BH_correction.lr"
  envmodules:
    "r/4.3.1"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos.R {input.lr_file} {output.deviant_snps} {output.deviant_snps_bp1}
    """


# For varying levels of Benjamini Hochberg critical values
rule ngsParalog_false_pos_BH:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr"
  output:
    deviant_snps_bp1_BH="results/bed/hap{hap}/deviant_SNPs_realign/BH_correction/{population}_deviant_SNPs_bp1_realign_BH{BH_VAR}.lr"
  params:
    BH_VAR="{BH_VAR}"
  envmodules:
    "r/4.3.1"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos_BH.R {input.lr_file} {output.deviant_snps_bp1_BH} {params.BH_VAR}
    """


# Create Manhattan Plots [NOTE: Specific Scaffold # otherwise it'll take forever for whole genome!] 
rule ngsParalog_manhattan:
  input:
    lr_file="results/ngs_paralog/hap{hap}/realign/{population}_scaffolds/{population}-Scaffold_1.lr"
  output:
    plot="results/plots/hap{hap}/ngsParalog/realign/{population}_manhattan_plot_realign_scaffold_1.png"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/ngs_paralog_graphs.R {input.lr_file} {output.plot}"


## dupHMM - Discover paralogous regions rather than individual sites ##
# Requires calcLR .lr files and depth of coverage .tsv files 

# Convert ngsParalog calcLR outputs (.lr files) to BED format
rule convert_calcLR_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr"
  output:
    bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_calcLR_realign.BED"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/convert_calcLR_to_bed.R {input.lr_file} {output.bed}"


# Generate average depth of coverage per site for dupHMM
# Only estimate depth at sites deemed 'paralogous' from raw ngsParalog (pre BH filter)
rule calculate_average_coverage_per_population:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap{hap}/{sample}_hap{hap}_realign.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)], hap=(1,2)),
    raw_lr="results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr"
  output:
    bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_calcLR_realign.BED",
    avg_cov_file="results/coverage_realign/hap{hap}/{population}_average_coverage.tsv"
  threads: 2
  log:
    "results/logs/coverage/hap{hap}/{population}_average_coverage.log"
  envmodules:
    "samtools/1.17"
  shell:
    """
    # Convert .lr file to BED format (0-based start)
    awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}

    # Calculate the average coverage per position for all BAM files together using the BED file
    samtools depth -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
    awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
    sort -k1,1V -k2,2n > {output.avg_cov_file}
    """


# Reduce deviant SNP calls (aka calcLR LRs) to the number of known sites with depth data.
# ANGSD does estimate depth globally and per individual, but NOT per site (see -doDepth 1)
rule filter_paralog_by_coverage:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat_realign/{population}_paralog_realign_WGS.lr",
    cov_file="results/coverage_realign/hap{hap}/{population}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap{hap}/deviant_SNPs_realign/{population}_coverage_only_paralog_realign_WGS.lr"
  log:
    "results/logs/filter_lr/hap{hap}/{population}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}
    """


# Set parameters for dupHMM to infer transitions between dup maps and normal mapping regions
# dupHMM setup is run across the entire genome ! Where as dupHMM run is run across each scaffold instead. 
rule dupHMM_setup:
  input:
    lr_file = "results/bed/hap{hap}/deviant_SNPs_realign/{population}_coverage_only_paralog_realign_WGS.lr",
    cov_file = "results/coverage_realign/hap{hap}/{population}_average_coverage.tsv"
  output:
    param_output = "results/ngs_paralog/hap{hap}/dupHMM_realign/{population}_dupHMM_realign.par"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap{hap}/dupHMM_realign/{population}_dupHMM_realign"
  log:
    "results/logs/ngs_paralog/hap{hap}/dupHMM_realign/{population}_dupHMM_realign_setup.log"
  envmodules:
    "r/4.3.1"
  shell:
    """
    Rscript {params.r_script_path} --lrfile {input.lr_file} \
    --outfile {params.name_output} \
    --covfile {input.cov_file} \
    --lrquantile 0.975 \
    --paramOnly 1 \
    2> {log}
    """


# Now generate coverage per scaffold and per population
rule calculate_coverage_per_population_and_scaffold_hap1:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap1/{sample}_hap1_realign.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap1/realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  output:
    bed="results/bed/hap1/deviant_SNPs_realign/{population}_scaffolds/{population}_{hap1scaffold_prefix}.BED",
    avg_cov_file="results/coverage_realign/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage_realign/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.17"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """


rule calculate_coverage_per_population_and_scaffold_hap2:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap2/{sample}_hap2_realign.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap2/realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  output:
    bed="results/bed/hap2/deviant_SNPs_realign/{population}_scaffolds/{population}_{hap2scaffold_prefix}.BED",
    avg_cov_file="results/coverage_realign/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage_realign/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.17"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """

# Filter .lr files by which there is depth data
# This is similar as before, but we need it by scaffold since...
# dupHMM setup is run across the entire genome ! Where as dupHMM run is run across each scaffold instead. 
rule filter_scaffold_lr_by_coverage_hap1:
  input:
    lr_file="results/ngs_paralog/hap1/realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file="results/coverage_realign/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap1/deviant_SNPs_realign/{population}_scaffolds/{population}_{hap1scaffold_prefix}_coverage_only_paralog_realign.lr"
  log:
    "results/logs/filter_lr/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """


rule filter_scaffold_lr_by_coverage_hap2:
  input:
    lr_file="results/ngs_paralog/hap2/realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file="results/coverage_realign/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap2/deviant_SNPs_realign/{population}_scaffolds/{population}_{hap2scaffold_prefix}_coverage_only_paralog_realign.lr"
  log:
    "results/logs/filter_lr/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """

# Run dupHMM with beginning estimated parameters
# adjust --lrquantile based on manhattan plot! 
# adjust --maxcoverage based on .tsv files! Seems like some coverages are HIGH! ~50.. To be slightly conservative, choosing 20
# NOTE: Some scaffolds only have 1 SNP with coverage and/or paralog data and can't be used in dupHMM so we skip these. 
rule dupHMM_run_hap1:
  input:
    lr_file = "results/ngs_paralog/hap1/realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file = "results/coverage_realign/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap1/dupHMM_realign/{population}_dupHMM_realign.par"
  output:
    paralog_region = "results/ngs_paralog/hap1/dupHMM_realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap1/dupHMM_realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM_realign/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run.log"
  envmodules:
    "r/4.3.1"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.975 \
        --maxcoverage 15 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """


rule dupHMM_run_hap2:
  input:
    lr_file = "results/ngs_paralog/hap2/realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file = "results/coverage_realign/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap2/dupHMM_realign/{population}_dupHMM_realign.par"
  output:
    paralog_region = "results/ngs_paralog/hap2/dupHMM_realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap2/dupHMM_realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM_realign/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run.log"
  envmodules:
    "r/4.3.1"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.975 \
        --maxcoverage 15 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """


# Combine all dupHMM outputs together into one file per population
rule concatenate_dupHMM_hap1:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap1/dupHMM_realign/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}_dupHMM_realign_run.rf", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap1/dupHMM_realign/{population}_dupHMM_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM_realign/{population}_dupHMM_realign_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


rule concatenate_dupHMM_hap2:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap2/dupHMM_realign/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}_dupHMM_realign_run.rf", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap2/dupHMM_realign/{population}_dupHMM_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM_realign/{population}_dupHMM_realign_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


# Convert dupHMM outputs (.lr files) to BED format
rule convert_dupHMM_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/dupHMM_realign/{population}_dupHMM_realign_WGS.lr"
  output:
    bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_dupHMM_realign.BED"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/convert_dupHMM_to_bed.R {input.lr_file} {output.bed}"





















## STEP 3: VERIFY PARALOGS REDUCED -> look at SFS with RE-RUN ANGSD

# Print out all sites (monomorphic or not)
rule extract_all_sites_by_popln:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    all_sites_gz="results/angsd/hap{hap}/all_sites/{population}_all_sites.pos.gz",
    all_sites_arg="results/angsd/hap{hap}/all_sites/{population}_all_sites.arg",
    all_sites_bed="results/angsd/hap{hap}/all_sites/{population}_all_sites.BED"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    angsd_out="results/angsd/hap{hap}/all_sites/{population}_all_sites"
  log:
    angsd_log="results/logs/angsd/hap{hap}/all_sites/{population}_all_sites.log"
  threads: 8
  envmodules:
    "angsd/0.939"
  shell:
    """
    # Extract all sites across the genome
    angsd -bam {input.bam_list} -ref {params.ref} -out {params.angsd_out} \
    -doCounts 1 -dumpCounts 1 -P 8 &> {log.angsd_log}

    # Convert the ANGSD output to a BED format file
    gunzip -c {params.angsd_out}.pos.gz | awk 'NR > 1 {{print $1, $2-1, $2}}' > {output.all_sites_bed}
    """


# Format all BED files accordingly and make sure they're in UNIX format + tab delimited
rule preprocess_bed_files_all_populations:
  input:
    all_sites_bed="results/angsd/hap{hap}/all_sites/{population}_all_sites.BED",
    calcLR_deviant_snps_bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_BH_correction.BED",
    dupHMM_deviant_regs_bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_dupHMM_realign.BED"
  output:
    processed_all_sites_bed="results/angsd/hap{hap}/all_sites/{population}_all_sites_processed.BED",
    processed_calcLR_snps="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_BH_correction_processed.BED",
    processed_dupHMM_regs="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_dupHMM_realign_processed.BED"
  shell:
    """
    # Convert all_sites_bed BED file from DOS to UNIX line endings and process
    dos2unix {input.all_sites_bed}
    awk 'NF >= 3 {{print $1"\t"$2"\t"$3}}' {input.all_sites_bed} > {output.processed_all_sites_bed}

    # Convert calcLR_snps BED file from DOS to UNIX line endings and process
    dos2unix {input.calcLR_deviant_snps_bed}
    awk 'NF >= 3 {{print $1"\t"$2"\t"$3}}' {input.calcLR_deviant_snps_bed} > {output.processed_calcLR_snps}

    # Convert dupHMM_regions BED file from DOS to UNIX line endings and process
    dos2unix {input.dupHMM_deviant_regs_bed}
    awk 'NF >= 3 {{print $1"\t"$2"\t"$3}}' {input.dupHMM_deviant_regs_bed} > {output.processed_dupHMM_regs}
    

    # Optional: Print lines before and after removal for auditing
    echo "Lines before processing in {input.all_sites_bed}:"
    wc -l {input.all_sites_bed}
    echo "Lines after processing in {output.processed_all_sites_bed}:"
    wc -l {output.processed_all_sites_bed}

    echo "Lines before processing in {input.calcLR_deviant_snps_bed}:"
    wc -l {input.calcLR_deviant_snps_bed}
    echo "Lines after processing in {output.processed_calcLR_snps}:"
    wc -l {output.processed_calcLR_snps}

    echo "Lines before processing in {input.dupHMM_deviant_regs_bed}:"
    wc -l {input.dupHMM_deviant_regs_bed}
    echo "Lines after processing in {output.processed_dupHMM_regs}:"
    wc -l {output.processed_dupHMM_regs}
    """


# Upon manual inspection, it seems that population PPP has a paralog data row that is malformed. We remove this manually
# We also incorporate this new check into the preprocessing rule. --> If any row has missing data, we remove that row. 
#sed -i '3272157d' results/bed/hap2/deviant_SNPs_realign/PPP_deviant_SNPs_realign_BH_correction_processed.BED


# Filter out deviant SNPs from all known sites
rule filter_all_sites_by_popln_calcLR:
  input:
    processed_all_sites_bed="results/angsd/hap{hap}/all_sites/{population}_all_sites_processed.BED",
    processed_calcLR_snps="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_BH_correction_processed.BED"
  output:
    filtered_calcLR_bed="results/bed/hap{hap}/filtered_sites/calcLR/{population}_filtered_sites.BED",
    filtered_calcLR_txt="results/bed/hap{hap}/filtered_sites/calcLR/{population}_filtered_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap{hap}/filtered_sites/{population}_filtered_sites.log"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Filter out deviant calcLR SNPs using bedtools and save to BED
    bedtools subtract -a {input.processed_all_sites_bed} -b {input.processed_calcLR_snps} > {output.filtered_calcLR_bed} 2> {log.calcLR_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_calcLR_bed} > {output.filtered_calcLR_txt}
    """


rule filter_all_sites_by_popln_dupHMM:
  input:
    processed_dupHMM_regs="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_dupHMM_realign_processed.BED",
    filtered_calcLR_bed="results/bed/hap{hap}/filtered_sites/calcLR/{population}_filtered_sites.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.BED",
    filtered_dupHMM_txt="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap{hap}/filtered_sites/{population}_filtered_sites_dupHMM_calcLR.log"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Filter out dupHMM regions from the calcLR BED file
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.processed_dupHMM_regs} > {output.filtered_dupHMM_bed} 2> {log.dupHMM_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """

# Index filtered all sites BED file
rule index_all_sites_by_popln_calcLR:
  input: 
    canonical_calcLR_sites="results/bed/hap{hap}/filtered_sites/calcLR/{population}_filtered_sites.txt"
  output: 
    calcLR_index="results/bed/hap{hap}/filtered_sites/calcLR/{population}_filtered_sites.txt.bin"
  envmodules:
    "angsd/0.939"
  shell: 
    "angsd sites index {input.canonical_calcLR_sites}"


rule index_all_sites_by_popln_dupHMM:
  input:
    canonical_dupHMM_sites="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  output:
    dupHMM_index="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  envmodules:
    "angsd/0.939"
  shell:
    "angsd sites index {input.canonical_dupHMM_sites}"


# ANGSD by population: To calculate SFS (check if heterozygote excess reduced) with filtered sites by ngsParalog
# Previously attempted just calcLR outputs but this did not remove paralogs. We now incorporate dupHMM and calcLR
rule angsd_SFS_by_population_on_all_sites:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.saf.gz",
    beagle="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM"
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/by_popln/angsd_canonical_sites_dupHMM_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


# ANGSD by population:  To calculate SFS and compare to filtered data. this is the CONTROL
rule angsd_SFS_control_by_population_on_all_sites:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.saf.gz",
    beagle="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control"
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/by_popln/angsd_canonical_sites_control_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """

# FILTERED SFS: Optimize and calculate SFS with folded spectra (--fold) since ancestral state unknown
# Can bootstrap to get confidence intervals
# We use 'winsfs' rather than 'realSFS' because realSFS is computational heavy (>100GB RAM required) plus less accurate
# Based on winsfs github, we allocate ~150GB RAM for ~1B sites (extracted from gzip and wc -l)
# We set seed at 1 for reproducibility
rule global_SFS_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_dupHMM.saf.idx"
  output:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.sfs"
  log:
    sfs1="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.log"
  threads: 40
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    """


rule fold_global_SFS_by_population:
  input:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.sfs"
  output:
    sfs_folded="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.sfs"
  log:
    sfs2="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.log"
  threads: 40
  shell:
    """
    winsfs view --fold {input.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """


rule global_SFS_control_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_sites_control.saf.idx"
  output:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control.sfs"
  log:
    sfs1="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control.log"
  threads: 40
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    """


rule fold_global_SFS_control_by_population:
  input:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control.sfs"
  output:
    sfs_folded="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded.sfs"
  log:
    sfs2="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded.log"
  threads: 40
  shell:
    """
    winsfs view --fold {input.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """


# Create SFS Plots on filtered data
rule global_SFS_by_population_plots:
  input:
    sfs_file="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.sfs"
  output:
    plot="results/plots/hap{hap}/SFS/{population}_globalSFS_dupHMM_folded.png"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/SFS_1D_graph.R {input.sfs_file} {output.plot}"


# Create SFS plots on control data
rule global_SFS_control_by_population_plots:
  input:
    sfs_file="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded.sfs"
  output:
    plot="results/plots/hap{hap}/SFS/{population}_globalSFS_control_folded.png"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/SFS_1D_graph.R {input.sfs_file} {output.plot}"


# To check F distribution on filtered SNPs using dupHMM and calcLR
rule angsd_HWE_by_population_on_SNPs:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/filtered_sites/dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.saf.gz",
    beagle="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM"
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/by_popln/angsd_canonical_SNPs_dupHMM_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


# Unzip hwe to extract desired variables (i.e F values)
rule unzip_hwe_dupHMM:
  input:
    zipped_hwe="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_canonical_SNPs_dupHMM.hwe.gz"
  output:
    unzip_hwe="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_hwe_canonical_SNPs_dupHMM.lr"
  shell:
    """
    zcat {input.zipped_hwe} > {output.unzip_hwe}
    """


# Create histogram of F values for canonical SNPs (excludes header)
rule hwe_histogram_dupHMM:
  input:
    lr_file="results/angsd/hap{hap}/canonical_realign/by_popln/{population}_hwe_canonical_SNPs_dupHMM.lr"
  output:
    plot="results/plots/hap{hap}/HWE/{population}_hwe_canonical_SNPs_dupHMM.png"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/hetero_excess_header.R {input.lr_file} {output.plot}"






## Filter from Coding Sequences for QC-checks ##
# Should get a good F distribution and SFS if using only known coding sequences
# Can only use haplotype 2 because gene annotation uses that haplotype

rule filter_gff_for_CDS:
  input:
    gff="data/annotation/MCG3698_Lupinus_perennis.annotation.gff"
  output:
    cds_with_col3="results/bed/hap2/annotation/CDS_check.txt",
    cds_without_col3="results/bed/hap2/annotation/lupine_CDS_regions.txt"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/convert_gff_to_txt.R {input.gff} {output.cds_with_col3} {output.cds_without_col3}"


rule index_gff_for_CDS:
  input: 
    gff_regions="results/bed/hap2/annotation/lupine_CDS_regions.txt"
  output: 
    bin_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.bin",
    idx_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.idx"
  envmodules:
    "angsd/0.939"
  shell: 
    """
    angsd sites index {input.gff_regions}
    """

rule angsd_SFS_by_population_on_CDS_sites:
  input:
    bam_list="data/lists/hap2/{population}_realign_hap2.txt",
    cds_regions="results/bed/hap2/annotation/lupine_CDS_regions.txt",
    idx_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.idx",
    bin_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.arg",
    mafs_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.mafs.gz",
    hwe_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.hwe.gz",
    depth_sample="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.depthSample",
    depth_global="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.depthGlobal",
    saf_1="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.saf.idx",
    saf_2="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.saf.pos.gz",
    saf_3="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.saf.gz",
    beagle="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.beagle.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites"
  log:
    "results/logs/angsd/hap2/annotated_filter/by_popln/angsd_annotated_CDS_sites_hap2_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.cds_regions}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


rule global_SFS_by_population_CDS_sites:
  input:
    saf_idx="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_sites.saf.idx"
  output:
    sfs="results/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS.sfs"
  log:
    sfs1="results/logs/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS.log"
  threads: 40
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    """


rule fold_global_SFS_by_population_CDS_sites:
  input:
    sfs="results/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS.sfs"
  output:
    sfs_folded="results/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS_folded.sfs"
  log:
    sfs2="results/logs/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS_folded.log"
  threads: 40
  shell:
    """
    winsfs view --fold {input.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """


# Create SFS Plots on filtered data
rule global_SFS_by_population_CDS_sites_plots:
  input:
    sfs_file="results/winSFS/hap2/globalSFS/{population}_hap2_globalSFS_CDS_folded.sfs"
  output:
    plot="results/plots/hap2/SFS/{population}_globalSFS_CDS.png"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/SFS_1D_graph.R {input.sfs_file} {output.plot}"


rule angsd_HWE_by_population_on_CDS_SNPs:
  input:
    bam_list="data/lists/hap2/{population}_realign_hap2.txt",
    cds_regions="results/bed/hap2/annotation/lupine_CDS_regions.txt",
    idx_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.idx",
    bin_index="results/bed/hap2/annotation/lupine_CDS_regions.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.arg",
    mafs_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.mafs.gz",
    hwe_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.hwe.gz",
    depth_sample="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.depthSample",
    depth_global="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.depthGlobal",
    saf_1="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.saf.idx",
    saf_2="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.saf.gz",
    beagle="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.beagle.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs"
  log:
    "results/logs/angsd/hap2/annotated_filter/by_popln/angsd_annotated_CDS_SNPs_hap2_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.cds_regions}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


# Unzip hwe to extract desired variables (i.e F values)
rule unzip_hwe_CDS:
  input:
    zipped_hwe="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.hwe.gz"
  output:
    unzip_hwe="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.lr"
  shell:
    """
    zcat {input.zipped_hwe} > {output.unzip_hwe}
    """


# Create histogram of F values for canonical SNPs (excludes header)
rule hwe_histogram_CDS:
  input:
    lr_file="results/angsd/hap2/annotated_filter/by_popln/{population}_annotated_CDS_SNPs.lr"
  output:
    plot="results/plots/hap2/HWE/{population}_hwe_annotated_CDS_SNPs.png"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/hetero_excess_header.R {input.lr_file} {output.plot}"































#### POPULATION GENOMICS ####

## BY POPULATION ##

# ANGSD by population : To calculate inbreeding coefficients
# NOTE: ngsF requires variable sites only, so call SNPs; GL must be in -doGlf 3 format
rule angsd_ngsF_by_population_on_SNPs:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/filtered_sites/{population}_filtered_sites.txt",
    bin_index="results/bed/hap{hap}/filtered_sites/{population}_filtered_sites.txt.bin"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.saf.gz",
    glf_gz="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.glf.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs"
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/angsd_canonical_SNPs_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 3\
    &> {log}
    """


# Grab number of variable sites (SNPs) per population
rule count_variable_sites:
  input:
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.mafs.gz"
  output:
    site_count="results/ngsF/hap{hap}/by_popln_inbreeding/{population}_SNPs_count.txt"
  shell:
    """
    zcat {input.mafs_file} | tail -n +2 | wc -l > {output.site_count}
    """


# Estimate inbreeding coefficients
# For low-coverage data, set min_epsilon for lower threshold b/w 1e-5 and 1e-9 so algorithm keeps exploring before stopping
rule ngsF_estimate_inbreeding:
  input:
    GL3="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_SNPs.saf.idx",
    SNP_count="results/ngsF/hap{hap}/by_popln_inbreeding/{population}_SNPs_count.txt"
  output:
    inbreeding_param="results/ngsF/hap{hap}/by_popln_inbreeding/{population}_inbreeding.pars",
    inbreeding_est="results/ngsF/hap{hap}/by_popln_inbreeding/{population}_inbreeding_estimate.ngsF"
  params:
    file_name="results/ngsF/hap{hap}/by_popln_inbreeding/{population}_inbreeding",
    pop_size=27
  log:
    "results/logs/ngsF/hap{hap}/by_popln_inbreeding/{population}_inbreeding_estimate.log"
  threads: 8
  shell:
    """
    ngsF --glf {input.GL3}\
    --n_threads {threads}\
    --calc_LRT 1\
    --out {params.file_name}\
    --n_ind {params.pop_size}\
    --n_sites {input.SNP_count}\
    --init_values r\
    --min_epsilon 1e-8\
    &> {log}
    """


# Re-iterate ngsF for convergence to global maxima



# ANGSD by population: To calculate SFS (check if heterozygote excess reduced)
# NOTE: -doSaf 2 requires inbreeding coefficients, which is supplied by ngsF
rule angsd_SFS_with_inbreeding_by_population_on_all_sites:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/filtered_sites/{population}_filtered_sites.txt",
    bin_index="results/bed/hap{hap}/filtered_sites/{population}_filtered_sites.txt.bin",
    inbreeding_coeff="results/ngsF/hap{hap}/bby_popln_inbreeding/{population}_inbreeding_estimate.ngsF"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.saf.gz",
    beagle="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites"
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/angsd_canonical_sites_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -indF {inbreeding_coeff}\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


# Optimize and calculate SFS with folded spectra (-fold 1) since ancestral state unknown
# Can bootstrap to get confidence intervals
rule global_SFS_with_inbreeding_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/canonical_realign/by_popln_inbreeding/{population}_canonical_sites.saf.idx"
  output:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_inbreeding.sfs"
  log:
    "results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_inbreeding.log"
  threads: 40
  envmodules:
    "angsd/0.939"
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    winsfs view --fold {output.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """






















## ALL POPULATIONS ##

# Run ANGSD on entire dataset, filtering paralogs from each population, and estimate SNPs + save GLs
# Rule to create .txt file of BAM files per population
rule generate_bam_list_all_populations:
  input:
    expand("results/bam_realign/hap{hap}/{sample}_hap{hap}_realign.bam", sample=SAMPLES, hap=(1,2)),
  output:
    "data/lists/hap{hap}/all_populations_realign_hap{hap}.txt"
  run:
    bam_files = input
    output_file = output[0]
    hap = wildcards.hap

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if f"_hap{hap}_" in bam_file:
                output.write(f"{bam_file}\n")



# Join together paralogs into one .BED file
# Include checks for correct UNIX file format and newline endings
# For some reason, 5 rows of data are incomplete so we remove filter these out. 
rule combine_population_calcLR_bed_files:
  input:
    lambda wildcards: expand("results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_realign_BH_correction.BED", 
                             hap=wildcards.hap, 
                             population=POPULATIONS),
  output:
    "results/bed/hap{hap}/combined/hap{hap}_combined_deviant_SNPs_realign_BH_correction.BED"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Combine all BED files for a specific haplotype and remove duplicates
    bedtools merge -i <(sort -k1,1 -k2,2n $(echo {{' '.join(input)}})) > {output}
    """


rule combine_population_dupHMM_bed_files:
  input:
    lambda wildcards: expand("results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_dupHMM_realign.BED", 
                             hap=wildcards.hap, 
                             population=POPULATIONS),
  output:
    "results/bed/hap{hap}/combined/hap{hap}_combined_dupHMM_regions.BED"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Combine all BED files for a specific haplotype and remove duplicates
    bedtools merge -i <(sort -k1,1 -k2,2n $(echo {{' '.join(input)}})) > {output}
    """

# Extract all known sites from ALL populations. This is to create list sites and later filter from paralogs
rule extract_all_sites_all_populations:
  input:
    bam_list="data/lists/hap{hap}/all_populations_realign_hap{hap}.txt"
  output:
    all_sites_gz="results/angsd/hap{hap}/all_poplns/all_sites.pos.gz",
    all_sites_arg="results/angsd/hap{hap}/all_poplns/all_sites.arg",
    all_sites_bed="results/angsd/hap{hap}/all_poplns/all_sites.BED"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    angsd_out="results/angsd/hap{hap}/all_poplns/all_sites"
  log:
    angsd_log="results/logs/angsd/hap{hap}/all_poplns/all_sites.log"
  threads: 8
  envmodules:
    "angsd/0.939"
  shell:
    """
    # Extract all sites across the genome for all populations
    angsd -bam {input.bam_list} -ref {params.ref} -out {params.angsd_out} \
    -doCounts 1 -dumpCounts 1 -P {threads} &> {log.angsd_log}

    # Convert the ANGSD output to a BED format file and check for completeness of data
    gzip -cd {params.angsd_out}.pos.gz | awk 'NR > 1 && NF >= 3 {{print $1"\t"$2-1"\t"$2}}' > {output.all_sites_bed}
    dos2unix {output.all_sites_bed}
    """


rule filter_all_sites_all_populations_calcLR:
  input:
    all_sites_bed="results/angsd/hap{hap}/all_poplns/all_sites.BED",
    deviant_snps="results/bed/hap{hap}/combined/hap{hap}_combined_deviant_SNPs_realign_BH_correction.BED"
  output:
    filtered_sites_bed="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.BED",
    filtered_sites_txt="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.log"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.all_sites_bed} -b {input.deviant_snps} > {output.filtered_sites_bed} \
    2> {log.calcLR_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_sites_bed} > {output.filtered_sites_txt}
    """


rule filter_all_sites_all_populations_dupHMM:
  input:
    filtered_calcLR_bed="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.BED",
    dupHMM_sites="results/bed/hap{hap}/combined/hap{hap}_combined_dupHMM_regions.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.BED",
    filtered_dupHMM_txt="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap{hap}/filtered_sites_all_poplns/dupHMM_filtered_sites.log"
  envmodules:
    "bedtools/2.30.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.dupHMM_sites} > {output.filtered_dupHMM_bed} \
    2> {log.dupHMM_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3, $3 + 1}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """


rule index_all_sites_all_popln_calcLR:
  input: 
    calcLR_sites="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap{hap}/filtered_sites_all_poplns/calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.939"
  shell: 
    """
    angsd sites index {input.calcLR_sites}
    """


rule index_all_sites_all_popln_dupHMM:
  input: 
    dupHMM_sites="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.939"
  shell: 
    """
    angsd sites index {input.dupHMM_sites}
    """


# Estimate SAF, HWE, GL with SNPs on entire population
rule angsd_SNP_on_all_populations:
  input:
    bam_list="data/lists/hap{hap}/all_populations_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt",
    bin_index="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap{hap}/filtered_sites_all_poplns/dupHMM_calcLR_filtered_sites.txt.idx"
  output:
    arg_file="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.saf.gz",
    beagle="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs",
  log:
    "results/logs/angsd/hap{hap}/canonical_realign/all_poplns/angsd_canonical_SNPs_all_poplns.log"
  envmodules:
    "angsd/0.939"
  threads: 16
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -minMapQ 30\
    -minQ 20 \
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 2\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


# Calculate PCA on genotype likelihoods using PCAngsd
rule PCAngsd_all_populations:
  input:
    beagle="results/angsd/hap{hap}/canonical_realign/all_poplns/all_poplns_canonical_SNPs.beagle.gz"
  output:
    cov_matrix="results/pcangsd/hap{hap}/canonical_realign/all_poplns/all_popln_canonical_SNP_PCAngsd.cov"
  params:
    file_name="results/pcangsd/hap{hap}/canonical_realign/all_poplns/all_popln_canonical_SNP_PCAngsd"
  log:
    "results/logs/pcangsd/hap{hap}/canonical_realign/all_poplns/all_popln_canonical_SNP_PCAngsd.log"
  threads: 8
  envmodules:
    "python/3.10.2"
  shell:
    """
    pcangsd -b {input.beagle}\
    -o {params.file_name}\
    -t {threads}\
    --iter 1000\
    2> {log}
    """


# Plot PCAs
rule PCAngsd_all_populations_plots:
  input:
    cov_matrix="results/pcangsd/hap{hap}/canonical_realign/all_poplns/all_popln_canonical_SNP_PCAngsd.cov",
    pop_info="data/lists/Batch1_pop_names.info"
  output:
    plot="results/plots/hap{hap}/PCAngsd/all_popln_canonical_SNP_PCAngsd.png"
  threads: 2
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/PCA_graph.R {input.cov_matrix} {input.pop_info} {output.plot}"


# Calculate geographical distances between pairwise individuals between populations
rule PCA_calc_geo_distances:
  input:
    csv="data/lists/bamlist_hap2_geo_coord.csv"
  output:
    dist="results/pcangsd/hap2/canonical_realign/all_poplns/pairwise_individ_geodist.csv"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/calc_popln_geodist.R {input.csv} {output.dist}"
