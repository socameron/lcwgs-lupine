###########################
##    WILDCARDS SET-UP   ##
###########################

import os
import glob
import subprocess
import pandas as pd
import yaml
import re
import itertools
from pathlib import Path






#### Functions to set up samples list in config from different sequencing batches

# In summary:
  #{sample} refers to the sample name received from the sequencer. It includes replicates and other identifying info.
  #{sample_prefix} refers to a trimmed sample name and merges replicates.

# Rule to update the configuration file
# The python script creates a list of sample names and their corresponding .fastq files for both reads R1 and R2
rule update_config:
  output:
    "snakeprofile/config.yaml"
  shell:
    "python scripts/extract_lcWGS_samples.py"

# Function to get samples from the config file
def get_samples(batch):
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    return config['batches'][batch]

# Function to get batches from the config file
def get_batches():
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    return list(config['batches'].keys())

# Function to get .fastq filenames from all 3 batches from the config file
def get_fastq_paths(batch, sample, read):
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    if batch in config['batches'] and sample in config['batches'][batch]:
        return config['batches'][batch][sample].get(read, f"data/{batch}/{sample}_L001_{read}_001.fastq.gz")
    # Fallback for batch_1 and batch_2:
    return f"data/{batch}/{sample}_L001_{read}_001.fastq.gz"

# Ensure that the config is updated before any other rule runs
# Get all batches from the config
batches = get_batches()

# Get samples for each batch
samples_batch_1 = get_samples('batch_1')
samples_batch_2 = get_samples('batch_2')
samples_batch_3 = get_samples('batch_3')

# Try using this for expand in rule all: 
  #  [f"results/bam_raw/hap2/{batch}/{sample}_hap2.bam"
  #   for batch in get_batches()
  #   for sample in list(get_samples(batch).keys())],

# Get basename of the fastq files
def get_basename(batch, sample, read):
    fastq_path = get_fastq_paths(batch, sample, read)
    if fastq_path:
        return os.path.basename(fastq_path).replace(".fastq.gz", "")
    return f"{sample}_{read}"  # Fallback in case of missing file


# Create list for fastqc outputs
expected_fastqc_outputs = []

for batch in get_batches():
    for sample in get_samples(batch).keys():
        # Compute the basenames from the full FASTQ paths.
        # This should always return a non-None string.
        r1_base = get_basename(batch, sample, "R1")
        r2_base = get_basename(batch, sample, "R2")

        expected_fastqc_outputs.extend([
            f"results/fastqc_raw/{batch}/{r1_base}_fastqc.html",
            f"results/fastqc_raw/{batch}/{r1_base}_fastqc.zip",
            f"results/fastqc_raw/{batch}/{r2_base}_fastqc.html",
            f"results/fastqc_raw/{batch}/{r2_base}_fastqc.zip"
        ])



#### Functions to set up lists of sample prefixes for merging .BAM files from different sequencing batches

# List of haplotypes
haps = ["1", "2"]

# List of populations
POPULATIONS = ["HPW", "IDNP-MW", "LCTGP", "MFNP", "PPP", "RLPLV", "SWCP", "APB", "BSL", "BSNA", "CPB", "FMB", "GRAY", "NBWA", "NGP", "PBBT", "RSB", "UWA"]

def extract_sample_prefix(sample_name):
    for pop in POPULATIONS:
        if sample_name.startswith(pop + "-"):
            # Get the portion before the first underscore.
            prefix = sample_name.split("_")[0]
            # If the prefix includes a replicate suffix, remove it.
            if "-rep" in prefix:
                prefix = prefix.split("-rep")[0]
            return prefix
    return sample_name

# Find replicates across BAM files as inputs into merge_replicates rule
def find_replicates(sample_prefix, hap):
    """
    Finds all BAM files corresponding to a given sample (using the unique sample_prefix)
    across batches (batch_1, batch_2, batch_3). The function expects the BAM file names to
    start exactly with sample_prefix and optionally a "-rep<number>" (for replicates), 
    immediately followed by an underscore and then "hap{hap}".
    """
    # Broad glob pattern to capture candidates
    pattern = f"results/bam_raw/hap{hap}/batch*/{sample_prefix}*_hap{hap}_RG.bam"
    files = glob.glob(pattern, recursive=True)
    
    # Compile a regex to match:
    #   ^                 --> start of the filename
    #   sample_prefix     --> exact sample prefix (escaped)
    #   (?!\d)           --> not immediately followed by a digit (so "PPP-1" won't match "PPP-10")
    #   (?:-rep\d+)?     --> optionally match a replicate indicator like "-rep2"
    #   _hap             --> then an underscore and "hap..."
    regex = re.compile(r'^' + re.escape(sample_prefix) + r'(?:-rep\d+)?_')

    
    # Filter files based on the regex match on the base name
    filtered_files = [f for f in files if regex.search(os.path.basename(f))]
    
    # Write a debug file for inspection
    debug_path = f"debug/debug_find_replicates_{sample_prefix}_hap{hap}_RG.txt"
    os.makedirs(os.path.dirname(debug_path), exist_ok=True)
    with open(debug_path, "w") as f:
        f.write(f"Pattern: {pattern}\n")
        if filtered_files:
            f.write("Found files:\n")
            for file in filtered_files:
                f.write(f"{file}\n")
        else:
            f.write("No files found.\n")
    
    return filtered_files


# Get all unique sample prefixes by extracting from samples listed in config file
def list_sample_prefixes():
    samples_batch_1 = get_samples('batch_1')  # returns a dict of {sample_name: {...}}
    samples_batch_2 = get_samples('batch_2')
    samples_batch_3 = get_samples('batch_3')
    # Combine sample names from all batches
    all_samples = set(
        list(samples_batch_1.keys()) +
        list(samples_batch_2.keys()) +
        list(samples_batch_3.keys())
    )
    # Now extract the prefix from each sample name using your extract_sample_prefix function.
    sample_prefixes = {extract_sample_prefix(sample) for sample in all_samples}
    return list(sample_prefixes)


# List of sample prefixes
sample_prefixes = list_sample_prefixes()
   



# Debugging: Write sample_prefixes and haps to files
#with open("debug_sample_prefixes.txt", "w") as f:
    #f.write("Sample Prefixes:\n")
    #for sp in sample_prefixes:
        #f.write(f"{sp}\n")

#with open("debug_haps.txt", "w") as f:
    #f.write("Haplotypes:\n")
    #for hap in haps:
        #f.write(f"{hap}\n")

# Use sample prefixes and designate to a particular population
def get_population_sample_prefixes(population):
    # Get all sample prefixes from the existing list_sample_prefixes function
    all_sample_prefixes = list_sample_prefixes()
    # Filter the sample prefixes based on the population
    population_sample_prefixes = [prefix for prefix in all_sample_prefixes if prefix.startswith(population)]
    return population_sample_prefixes





#### Functions to create scaffold names list per scaffold
# Get first 24 scaffold names to later estimate LD per scaffold
# Not using other scaffolds as they don't quite align to the expected 24 chromosomes

rule get_scaffold_names:
  input:
    "data/reference/lupinehap1.fasta",
    "data/reference/lupinehap2.fasta"
  output:
    "results/scaffolds/hap1_scaffolds.txt",
    "results/scaffolds/hap2_scaffolds.txt"
  shell:
    """
    python scripts/extract_24_scaffold_names_by_hap.py
    """

with open("results/scaffolds/hap1_scaffolds.txt", "r") as file:
    HAP1SCAFFOLDS = [line.strip() for line in file.readlines()]

with open("results/scaffolds/hap2_scaffolds.txt", "r") as file:
    HAP2SCAFFOLDS = [line.strip() for line in file.readlines()]

# Split scaffold names by comma and create a list
HAP1SCAFFOLDS = [name.strip() for name in HAP1SCAFFOLDS]
HAP2SCAFFOLDS = [name.strip() for name in HAP2SCAFFOLDS]
HAP1SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP1SCAFFOLDS]
HAP2SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP2SCAFFOLDS]

# Function give full scaffold name given prefix of scaffold
def map_prefix_to_full_scaffold(prefix, hap_type):
    scaffold_list = HAP1SCAFFOLDS if hap_type == 1 else HAP2SCAFFOLDS
    for scaffold in scaffold_list:
        if scaffold.startswith(prefix):
            return scaffold
    return None  
# Return None or an appropriate default if not found



#### Bunch of parameters for using ngsParalog, Fst, ANGSD, etc

# Varying levels of Bonferroni-Hotchberg correction for ngsParalog
BH_VARS=[50,40,30,20,10,5]

# Varying levels of site depth to check filter levels
depth_params = [(50, 1500), (50, 2000), (50, 2500), (50, 3000), (100, 1500), (100, 2000), (100, 2500), (100, 3000)]

# Varying levels of minor allele frequency cutoffs
minMAF_params = [0.01, 0.001, 0.0001, 0.00001]

# For identifying populatio pairs for Fst analysis
POP_COMBINATIONS = list(itertools.combinations(POPULATIONS, 2))

# Varying levels of K for admixture analyses
K_values = [16,17,18,19,20]

# These functions are necessary for bcftools/roh because it calls for specific .vcf files pertaining to each population. 
# Function to expand VCF file paths
def generate_vcf_files():
    vcf_files = []
    for population in POPULATIONS:
        sample_prefixes = get_population_sample_prefixes(population)
        vcf_files.extend(
            [f"results/angsd/hap2/canonical/bcfROH_input/{population}/{prefix}_canonical_SNPs.vcf.gz" for prefix in sample_prefixes]
        )
    return vcf_files

# Function to generate annotated BCF file paths
def generate_annotated_vcfs():
    annotated_vcfs = []
    for population in POPULATIONS:
        sample_prefixes = get_population_sample_prefixes(population)
        annotated_vcfs.extend(
            [f"results/bcftools/hap2/roh_analysis/{population}/{prefix}_annotated.vcf.gz" for prefix in sample_prefixes]
        )
    return annotated_vcfs

# Use *generate_vcf_files() or *generate_annotated_vcfs()

def bams_to_fix():
    pops = [
      #"HPW","LCTGP","IDNP-MW","RLPLV","PPP","MFNP","SWCP",
      "APB","BSL","BSNA","CPB","FMB","GRAY","NBWA","NGP","PBBT","RSB","UWA"
    ]
    bams = []
    for pop in pops:
        bl = f"data/lists/hap2/{pop}_realign_hap2.txt"
        if not os.path.exists(bl): continue
        with open(bl) as f:
            for line in f:
                p = line.strip()
                if p: bams.append(p)
    return bams

###########################
## BEGINNING OF WORKFLOW ##
###########################


# Expand the final files using the updated configuration
rule all:
  input:
    expand("results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin", population = POPULATIONS),
    "results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
    #expand("results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin", population = POPULATIONS),
    #expand("results/NeEstimator/{population}_canonical_filtered_10k.vcf.gz", population = POPULATIONS)
    
    
    
    #expand("results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz", population = POPULATIONS)
    #expand("results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz", population = POPULATIONS)



################################
#### LIST OF ON-GOING RULES ####
################################


## STEP 3: IDENTIFY PARALOGOUS REGIONS 

# Call SNPs with liberal rules for input to ngsParalog
# NOTE: -SNP_pvalue 0.05 so very liberal SNP calls
# NOTE: NO filters applied to gather as much as data as possible as input for ngsParalog, although we put missing data minimum to 20% of individuals
rule angsd_raw_SNP:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.arg",
    mafs_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.mafs.gz",
    hwe_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.hwe.gz",
    depth_sample="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.depthSample",
    depth_global="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign",
    hap="{hap}",
    population="{population}",
    minInd=lambda wildcards, input: max(1, int(0.2 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/raw/ngsParalog_input/angsd_SNP_raw_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -C 50\
    -GL 2\
    -SNP_pval 0.1\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd} \
    -baq 2\
    -only_proper_pairs 1\
    -P {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    &> {log}
    """



# Create BED files so including only SNPs into ngsParalog
# NOTE: BED files indexed at 0bp because SAMtools to create pileup requires 0bp index
rule convert_mafs_to_bed:
  input:
    mafs_gz="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_realign.mafs.gz"
  output:
    bed_file="results/bed/hap{hap}/raw_SNPs/{population}_raw_SNPs_realign.BED"
  shell:
   """
   gunzip -c {input.mafs_gz} | awk 'NR>1 {{print $1, $2 - 1, $2}}' > {output.bed_file}
   dos2unix {output.bed_file}  # Add this line to convert line endings
   """


# Run ngsParalog on all SNPs across the genome (parallelized by running 1 job per scaffold)
rule ngsParalog_hap1: 
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_realign/hap1/{sample}_hap1_realign.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    ref="data/reference/hap1/lupinehap1.fasta",
    bed_file="results/bed/hap1/raw_SNPs/{population}_raw_SNPs_realign.BED"
  output:
    paralog_output="results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.log"
  params:
    hap1scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap1scaffold_prefix, 1),
    minInd=lambda wildcards, input: max(1, int(0.8 * len(input.bam_ngsPara)))
  envmodules:
    "samtools/1.20"
  shell:
    """
    rm -f {output.paralog_output} #remove existing output file if it exists
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -A -d 77000000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap1scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} -allow_overwrite 1 \
    -minQ 20 -minind {params.minInd} -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


rule ngsParalog_hap2:
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_realign/hap2/{sample}_hap2_realign.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    ref="data/reference/hap2/lupinehap2.fasta",
    bed_file="results/bed/hap2/raw_SNPs/{population}_raw_SNPs_realign.BED"
  output:
    paralog_output="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.log"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2),
    minInd=lambda wildcards, input: max(1, int(0.8 * len(input.bam_ngsPara)))
  envmodules:
    "samtools/1.20"
  shell:
    """
    rm -f {output.paralog_output} #remove existing output file if it exists
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -A -d 77000000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap2scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} -allow_overwrite 1 \
    -minQ 20 -minind {params.minInd} -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


# Combine all ngsParalog outputs together into one file per population
rule concatenate_paralog_hap1:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}.lr", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap1/concat/{population}_paralog_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/concat/{population}_paralog_realign_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


rule concatenate_paralog_hap2:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}.lr", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap2/concat/{population}_paralog_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/concat/{population}_paralog_realign_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


# Print summary of calcLR quantile ranges
rule quantile_summary:
  input:
    "results/ngs_paralog/hap{hap}/concat/{population}_paralog_realign_WGS.lr"
  output:
    "results/ngs_paralog/hap{hap}/quantile_summary/{population}_calcLR_quantile_summary.txt"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/calcLR_quantile_summary.R {input} {output}"


# Identify false positives from ngsParalog (grab only true Paralogs) and filter out
# NOTE: R script indexes positions back to 1bp start
rule ngsParalog_false_pos:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_realign_WGS.lr"
  output:
    deviant_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected.BED",
    deviant_snps_bp1="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_realign_bp1_BH_corrected.lr"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos.R {input.lr_file} {output.deviant_snps} {output.deviant_snps_bp1}
    """


# Identify false positives for varying levels of Benjamini Hochberg critical values
rule ngsParalog_false_pos_BH:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_realign_WGS.lr"
  output:
    deviant_snps_bp1_BH="results/bed/hap{hap}/deviant_SNPs/BH_correction/{population}_deviant_SNPs_bp1_realign_BH{BH_VAR}.lr"
  params:
    BH_VAR="{BH_VAR}"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos_BH.R {input.lr_file} {output.deviant_snps_bp1_BH} {params.BH_VAR}
    """


# Create Manhattan Plots [NOTE: Specific Scaffold # otherwise it'll take forever for whole genome!] 
rule ngsParalog_manhattan:
  input:
    lr_file="results/ngs_paralog/hap{hap}/by_popln/{population}_scaffolds/{population}-Scaffold_1.lr"
  output:
    plot="results/plots/hap{hap}/ngsParalog/by_popln/{population}_manhattan_plot_realign_scaffold_1.png"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/ngs_paralog_graphs.R {input.lr_file} {output.plot}"


## dupHMM - Discover paralogous regions rather than individual sites ##
# Requires calcLR .lr files and depth of coverage .tsv files 

# Convert ngsParalog calcLR outputs (.lr files) to BED format
# This is NOT BH corrected. This is just here for future use
rule convert_calcLR_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_realign_WGS.lr",
  output:
    bed="results/bed/hap{hap}/deviant_SNPs_realign/{population}_deviant_SNPs_calcLR.BED"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/convert_calcLR_to_bed.R {input.lr_file} {output.bed}"


# Generate average depth of coverage per site for dupHMM
# Only estimate depth at sites deemed 'paralogous' from raw ngsParalog (pre BH filter)
# Use pre BH filter because take all data as input!
# Only applies to hap2 (need rule for hap1)
rule estimate_average_coverage_per_population:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap2/{sample}_hap2_realign.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap2/concat/{population}_paralog_realign_WGS.lr"
  output:
    bed="results/bed/hap2/deviant_SNPs/{population}_deviant_SNPs_calcLR.BED",
    avg_cov_file="results/coverage/hap2/{population}_average_deviant_SNP_coverage.tsv"
  threads: 2
  log:
    "results/logs/coverage/hap2/{population}_average_coverage.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Convert .lr file to BED format (0-based start)
    awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}

    # Calculate the average coverage per position for all BAM files together using the calcLR BED file
    samtools depth -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
    awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
    sort -k1,1V -k2,2n > {output.avg_cov_file}
    """

# Print summary statistics
rule estimate_coverage_statistics:
  input:
    avg_cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    stats="results/coverage_stats/hap{hap}/{population}_coverage_stats.txt"
  shell:
    """
    python scripts/estimate_coverage_stats.py {input.avg_cov_file} > {output.stats}
    """


# Reduce deviant SNP calls (aka calcLR LRs) to the number of known sites with depth data.
# ANGSD estimates depth globally and per individual, but NOT per site (see -doDepth 1)
rule filter_paralog_by_coverage:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_realign_WGS.lr",
    cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    filtered_lr="results/bed/hap{hap}/deviant_SNPs/{population}_coverage_only_paralog_realign_WGS.lr"
  log:
    "results/logs/filter_lr/hap{hap}/{population}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}
    """

# dupHMM_setup is run across the entire genome Whereas dupHMM_run is across each scaffold. 
rule dupHMM_setup:
  input:
    lr_file="results/bed/hap{hap}/deviant_SNPs/{population}_coverage_only_paralog_realign_WGS.lr",
    cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    param_output="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_realign.par"
  params:
    r_script_path="/home/socamero/ngsParalog/dupHMM.R",
    name_output="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_realign"
  log:
    "results/logs/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_realign_setup.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript {params.r_script_path} \
    --lrfile {input.lr_file} \
    --outfile {params.name_output} \
    --covfile {input.cov_file} \
    --lrquantile 0.975 \
    --paramOnly 1 \
    2> {log}
    """


# Now generate coverage per scaffold and per population
# We need to input coverage into dupHMM!
rule estimate_coverage_per_population_and_scaffold_hap1:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap1/{sample}_hap1_realign.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  output:
    bed="results/bed/hap1/deviant_SNPs/{population}_scaffolds/{population}_{hap1scaffold_prefix}.BED",
    avg_cov_file="results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """


rule estimate_coverage_per_population_and_scaffold_hap2:
  input:
    bam_files=lambda wildcards: expand("results/bam_realign/hap2/{sample}_hap2_realign.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  output:
    bed="results/bed/hap2/deviant_SNPs/{population}_scaffolds/{population}_{hap2scaffold_prefix}.BED",
    avg_cov_file="results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """

# Filter .lr files by which there is depth data
# This is similar as before, but we need it by scaffold since...
# dupHMM setup is run across the entire genome ! Where as dupHMM run is run across each scaffold instead. 
rule filter_scaffold_lr_by_coverage_hap1:
  input:
    lr_file="results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file="results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap1/deviant_SNPs/{population}_scaffolds/{population}_{hap1scaffold_prefix}_coverage_only_paralog_realign.lr"
  log:
    "results/logs/filter_lr/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """


rule filter_scaffold_lr_by_coverage_hap2:
  input:
    lr_file="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file="results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap2/deviant_SNPs/{population}_scaffolds/{population}_{hap2scaffold_prefix}_coverage_only_paralog_realign.lr"
  log:
    "results/logs/filter_lr/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_filtered_deviant_SNPs_realign.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """

# Run dupHMM with beginning estimated parameters
# adjust --lrquantile based on manhattan plot! 
# adjust --maxcoverage based on .tsv files! Seems like some coverages are HIGH! ~50.. To be slightly conservative, choosing 25 (see coverage stats)
# NOTE: Some scaffolds only have 1 SNP with coverage and/or paralog data and can't be used in dupHMM so we skip these. 
rule dupHMM_run_hap1:
  input:
    lr_file = "results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file = "results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap1/dupHMM/{population}_dupHMM_realign.par"
  output:
    paralog_region = "results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_realign_run.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} \
        --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.97 \
        --maxcoverage 25 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """

# NOTE: coverage is lower using hap2
rule dupHMM_run_hap2:
  input:
    lr_file = "results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file = "results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap2/dupHMM/{population}_dupHMM_realign.par"
  output:
    paralog_region = "results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_realign_run.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} \
        --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.97 \
        --maxcoverage 20 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """


# Combine all dupHMM outputs together into one file per population
rule concatenate_dupHMM_hap1:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}_dupHMM_realign_run.rf", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap1/dupHMM/{population}_dupHMM_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM/{population}_dupHMM_realign_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


rule concatenate_dupHMM_hap2:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}_dupHMM_realign_run.rf", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap2/dupHMM/{population}_dupHMM_realign_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM/{population}_dupHMM_realign_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


# Convert dupHMM outputs (.lr files) to BED format
rule convert_dupHMM_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_realign_WGS.lr"
  output:
    bed="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM.BED"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/convert_dupHMM_to_bed.R {input.lr_file} {output.bed}"

    
## STEP 4: VERIFY PARALOGS REDUCED -> look at SFS with RE-RUN ANGSD

# Print out all sites (monomorphic or not) to later filter out paralogous regions
rule angsd_raw_sites_by_popln:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    all_sites_gz="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites.pos.gz",
    all_sites_arg="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites.arg",
    all_sites_bed="results/bed/hap{hap}/raw_sites/{population}_all_sites.BED"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    angsd_out="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites"
  log:
    angsd_log="results/logs/angsd/hap{hap}/raw/by_popln/{population}_all_sites.log"
  threads: 12
  envmodules:
    "angsd/0.940"
  shell:
    """
    # Extract all sites across the genome
    angsd -bam {input.bam_list} -ref {params.ref} -out {params.angsd_out} \
    -doCounts 1 -dumpCounts 1 -P {threads} &> {log.angsd_log}

    # Convert the ANGSD output to a BED format file
    gunzip -c {params.angsd_out}.pos.gz | awk 'NR > 1 {{print $1, $2-1, $2}}' > {output.all_sites_bed}
    """


# Format all BED files accordingly and make sure they're in UNIX format + tab delimited
rule preprocess_bed_files_all_populations:
  input: 
    all_sites_bed="results/bed/hap{hap}/raw_sites/{population}_all_sites.BED",
    calcLR_deviant_snps_bed="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected.BED",
    dupHMM_deviant_regs_bed="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM.BED"
  output:
    processed_all_sites_bed="results/bed/hap{hap}/raw_sites/by_popln/{population}_all_sites_processed.BED",
    processed_calcLR_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected_processed.BED",
    processed_dupHMM_regs="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM_processed.BED"
  shell:
    """
    # Convert all_sites_bed BED file from DOS to UNIX line endings and process
    dos2unix {input.all_sites_bed}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input.all_sites_bed} > {output.processed_all_sites_bed}

    # Convert calcLR_snps BED file from DOS to UNIX line endings and process
    dos2unix {input.calcLR_deviant_snps_bed}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input.calcLR_deviant_snps_bed} > {output.processed_calcLR_snps}

    # Convert dupHMM_regions BED file from DOS to UNIX line endings and process
    dos2unix {input.dupHMM_deviant_regs_bed}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input.dupHMM_deviant_regs_bed} > {output.processed_dupHMM_regs}
    

    # Optional: Print lines before and after removal for auditing
    echo "Lines before processing in {input.all_sites_bed}:"
    wc -l {input.all_sites_bed}
    echo "Lines after processing in {output.processed_all_sites_bed}:"
    wc -l {output.processed_all_sites_bed}

    echo "Lines before processing in {input.calcLR_deviant_snps_bed}:"
    wc -l {input.calcLR_deviant_snps_bed}
    echo "Lines after processing in {output.processed_calcLR_snps}:"
    wc -l {output.processed_calcLR_snps}

    echo "Lines before processing in {input.dupHMM_deviant_regs_bed}:"
    wc -l {input.dupHMM_deviant_regs_bed}
    echo "Lines after processing in {output.processed_dupHMM_regs}:"
    wc -l {output.processed_dupHMM_regs}
    """


# Filter out deviant SNPs from all known sites
rule filter_all_sites_by_popln_calcLR:
  input:
    processed_all_sites_bed="results/bed/hap{hap}/raw_sites/by_popln/{population}_all_sites_processed.BED",
    processed_calcLR_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected_processed.BED",
  output:
    filtered_calcLR_bed="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.BED",
    filtered_calcLR_txt="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant calcLR SNPs using bedtools and save to BED
    bedtools subtract -a {input.processed_all_sites_bed} -b {input.processed_calcLR_snps} > {output.filtered_calcLR_bed} 2> {log.calcLR_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_calcLR_bed} > {output.filtered_calcLR_txt}
    """


rule filter_all_sites_by_popln_dupHMM:
  input:
    processed_dupHMM_regs="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM_processed.BED",
    filtered_calcLR_bed="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.BED",
    filtered_dupHMM_txt="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out dupHMM regions from the calcLR BED file
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.processed_dupHMM_regs} > {output.filtered_dupHMM_bed} 2> {log.dupHMM_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """


# Index filtered all sites BED file
rule index_all_sites_by_popln_calcLR:
  input: 
    canonical_calcLR_sites="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt"
  output: 
    calcLR_index="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt.bin"
  envmodules:
    "angsd/0.940"
  shell: 
    "angsd sites index {input.canonical_calcLR_sites}"


rule index_all_sites_by_popln_dupHMM:
  input:
    canonical_dupHMM_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  output:
    dupHMM_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  envmodules:
    "angsd/0.940"
  shell:
    "angsd sites index {input.canonical_dupHMM_sites}"



## For ALL populations

rule angsd_raw_sites_all_poplns:
  input:
    bam_list="data/lists/hap2/all_populations_realign_hap2.txt"
  output:
    all_sites_gz="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.pos.gz",
    all_sites_arg="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.arg"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    angsd_out="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites"
  log:
    angsd_log="results/logs/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.log"
  threads: 12
  envmodules:
    "angsd/0.940"
  shell:
    """
    # Extract all sites across the genome for the given scaffold from all populations
    angsd -bam {input.bam_list} \
          -ref {params.ref} \
          -out {params.angsd_out} \
          -doCounts 1 -dumpCounts 1 \
          -P {threads} \
          -r {wildcards.hap2scaffold} \
          &> {log.angsd_log}
    """

# Convert each scaffold's sites to BED format
rule convert_raw_sites_scaffold:
  input:
    all_sites_gz="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.pos.gz"
  output:
    bed="results/bed/hap2/raw_sites/by_scaffold/{hap2scaffold}_all_sites.BED"
  threads: 4
  shell:
    """
    # Convert the ANGSD output to BED format for the scaffold
    gzip -cd {input.all_sites_gz} | awk 'NR>1 && NF>=3 {{print $1"\t"$2-1"\t"$2}}' > {output.bed}
    dos2unix {output.bed}
    """

# Combine scaffold BED files 
rule combine_raw_sites_scaffolds:
  input:
    beds=expand("results/bed/hap2/raw_sites/by_scaffold/{hap2scaffold}_all_sites.BED", hap2scaffold=HAP2SCAFFOLDS)
  output:
    all_sites_bed="results/bed/hap2/raw_sites/all_poplns/all_sites.BED"
  threads: 4
  shell:
    """    
    # Conmbine BED format file and check for completeness of data
    cat {input.beds} > {output.all_sites_bed}
    dos2unix {output.all_sites_bed}
    """


rule filter_all_sites_all_populations_calcLR:
  input:
    all_sites_bed="results/bed/hap2/raw_sites/all_poplns/all_sites.BED",
    deviant_snps="results/bed/hap2/deviant_sites/hap2_combined_deviant_SNPs_realign_BH_correction.BED"
  output:
    filtered_sites_bed="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.BED",
    filtered_sites_txt="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.all_sites_bed} -b {input.deviant_snps} > {output.filtered_sites_bed} \
    2> {log.calcLR_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_sites_bed} > {output.filtered_sites_txt}
    """


rule filter_all_sites_all_populations_dupHMM:
  input:
    filtered_calcLR_bed="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.BED",
    dupHMM_sites="results/bed/hap2/deviant_sites/hap2_combined_dupHMM_regions.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.BED",
    filtered_dupHMM_txt="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap2/canonical_sites/filtered_dupHMM/dupHMM_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.dupHMM_sites} > {output.filtered_dupHMM_bed} \
    2> {log.dupHMM_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3, $3 + 1}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """


rule index_all_sites_all_popln_calcLR:
  input: 
    calcLR_sites="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.calcLR_sites}
    """


rule index_all_sites_all_popln_dupHMM:
  input: 
    dupHMM_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.dupHMM_sites}
    """