import os
import glob
import subprocess
import pandas as pd

# Define a function to extract the sample name from a fastq.gz filename
def extract_sample_name(filename):
    # Get the base filename without the directory
    base_filename = os.path.basename(filename)
    
    # Split the filename by underscores
    parts = base_filename.split('_')
    
    # Find the part that contains 'L001' and get the sample name before it
    for i, part in enumerate(parts):
        if 'L001' in part:
            return '_'.join(parts[:i])

# Defining wildcard for haplotype and sequence batch #
wildcard_constraints:
    hap = r'1|2',
    batch = r'1|2'

# Directory containing the fastq.gz files 
# Change 1 to {batch}
directory = "data/batch_1"

# Get a list of all fastq.gz files in the directory
fastq_files = glob.glob(os.path.join(directory, "*.fastq.gz"))

# Initialize a set to store unique sample names
SAMPLES = set()

# Iterate over the fastq.gz files and extract sample names
for filename in fastq_files:
    sample_name = extract_sample_name(filename)
    SAMPLES.add(sample_name)

# Convert the set to a sorted list for consistent order
SAMPLES = sorted(list(SAMPLES))
POPULATIONS = ("HPW", "IDNP-MW", "LCTGP", "MFNP", "PPP", "RLPLV", "SWCP")


with open("results/scaffolds/hap1_scaffolds.txt", "r") as file:
    HAP1SCAFFOLDS = [line.strip() for line in file.readlines()]

with open("results/scaffolds/hap2_scaffolds.txt", "r") as file:
    HAP2SCAFFOLDS = [line.strip() for line in file.readlines()]

# Split scaffold names by comma and create a list
HAP1SCAFFOLDS = [name.strip() for name in HAP1SCAFFOLDS]
HAP2SCAFFOLDS = [name.strip() for name in HAP2SCAFFOLDS]
HAP1SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP1SCAFFOLDS]
HAP2SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP2SCAFFOLDS]

# Function give full scaffold name given prefix of scaffold
def map_prefix_to_full_scaffold(prefix, hap_type):
    scaffold_list = HAP1SCAFFOLDS if hap_type == 1 else HAP2SCAFFOLDS
    for scaffold in scaffold_list:
        if scaffold.startswith(prefix):
            return scaffold
    return None  # Return None or an appropriate default if not found





###########################
## BEGINNING OF WORKFLOW ##
###########################


# final files desired (can change)
rule all:
  input:
    expand("results/angsd/hap1/canonical/SNP_canonical_{population}.arg", population=POPULATIONS),
    expand("results/angsd/hap1/canonical/SNP_canonical_{population}.mafs.gz", population=POPULATIONS),
    expand("results/angsd/hap1/canonical/SNP_canonical_{population}.hwe.gz", population=POPULATIONS),
    expand("results/angsd/hap1/canonical/SNP_canonical_{population}.depthSample", population=POPULATIONS),
    expand("results/angsd/hap1/canonical/SNP_canonical_{population}.depthGlobal", population=POPULATIONS)







#### DATA PREPARATION ####

# Trim adapter ends off each sequence file using Trimmomatic 
# When batch 2 is available, use wildcard {batch}
rule trim_reads:
  input:
    r1="data/batch_{batch}/{sample}_L001_R1_001.fastq.gz",
    r2="data/batch_{batch}/{sample}_L001_R2_001.fastq.gz",
  output:
    r1="results/trimmed/{sample}_R1.fastq.gz",
    r1_unp="results/trimmed/{sample}_R1_unpaired.fastq.gz",
    r2="results/trimmed/{sample}_R2.fastq.gz",
    r2_unp="results/trimmed/{sample}_R2_unpaired.fastq.gz"
  log:
    "results/logs/trim_reads/{sample}.log"
  envmodules:
    "trimmomatic/0.39"
  params:
    adapters="$EBROOTTRIMMOMATIC/adapters/TruSeq3-PE-2.fa"
  shell:
    "java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar PE {input.r1} {input.r2} "
    "{output.r1} {output.r1_unp} {output.r2} {output.r2_unp} "
    "ILLUMINACLIP:{params.adapters}:2:30:10 "
    "LEADING:3 "
    "TRAILING:3 "
    "SLIDINGWINDOW:4:15 "
    "MINLEN:36 2> {log}"


# Unzip trimmed files as fastqc 0.12.0 cannot properly read compressed files
rule unzip_files:
  input:
    zipped_r1="results/batch_{batch}/trimmed/{sample}_R1.fastq.gz",
    zipped_r2="results/batch_{batch}trimmed/{sample}_R2.fastq.gz"
  output: 
    unzipped_r1="results/batch_{batch}/trimmed_unzip/{sample}_R1.fastq",
    unzipped_r2="results/batch_{batch}/trimmed_unzip/{sample}_R2.fastq"
  shell:
    "gunzip -c {input.zipped_r1} > {output.unzipped_r1} && gunzip -c {input.zipped_r2} > {output.unzipped_r2}"


# Run FastQC per each trimmed sequence file
rule fastqc:
  input:
    fastq_r1="results/trimmed_unzip/batch_{batch}/{sample}_R1.fastq",
    fastq_r2="results/trimmed_unzip/batch_{batch}/{sample}_R2.fastq"
  output:
    html_report_r1="results/fastqc/batch_{batch}/{sample}_R1_fastqc.html",
    zip_report_r1="results/fastqc/batch_{batch}/{sample}_R1_fastqc.zip",
    html_report_r2="results/fastqc/batch_{batch}/{sample}_R2_fastqc.html",
    zip_report_r2="results/fastqc/batch_{batch}/{sample}_R2_fastqc.zip"
  envmodules:
    "fastqc/0.12.0"
  shell:
    "fastqc {input.fastq_r1} {input.fastq_r2} --outdir results/fastqc"


# Create an aggregated FastQC report using MultiQC.
# Note that we create separate MultiQC reports for both batch 1 and 2
rule multiqc_raw:
  input:
    fastqc_dir="results/fastqc/batch_1"
  output:
    html_report="results/multiqc/multiqc_raw_batch_1/multiqc_report.html"
  params:
    output_dir="results/multiqc/multiqc_raw_batch_1"
  shell:
    "multiqc -o {params.output_dir} {input.fastqc_dir} "


# Creating faidx index for reference genome
rule faidx_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta",
  output:
    "data/reference/hap{hap}/lupinehap{hap}.fasta.fai",
  log:
    "results/logs/refgen/lupinehap{hap}_faidx.log",
  envmodules:
    "samtools/1.17"
  shell:
    "samtools faidx {input} 2> {log} "


# Rules for indexing reference genomes (haplotypes 1 and 2)
rule index_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa"),
  log:
    "results/logs/refgen/lupinehap{hap}_bwa_index.log"
  envmodules:
    "bwa/0.7.17"
  shell:
    "bwa index {input} 2> {log}"


# Rules for creating dictionary files
rule create_dict:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    "data/reference/hap{hap}/lupinehap{hap}.dict"
  log:
    "results/logs/refgen/hap{hap}_dict.log"
  envmodules:
    "samtools/1.17"
  shell:
    "samtools dict {input} > {output} 2> {log}"


# Mapping/Aligning reads to haplotypes
rule map_reads:
  input:
    r1="results/trimmed/{sample}_R1.fastq.gz",
    r2="results/trimmed/{sample}_R2.fastq.gz",
    genome="data/reference/hap{hap}/lupinehap{hap}.fasta",
    idx=multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa")
  output:
    "results/bam/hap{hap}/{sample}_hap{hap}.bam"
  log:
    "results/logs/map_reads/hap{hap}/{sample}_hap{hap}.log"
  benchmark:
    "results/benchmarks/map_reads/{sample}_hap{hap}.bmk"
  envmodules:
    "bwa/0.7.17",
    "samtools/1.17"
  threads: 8 
  params:
    RG="-R '@RG\\tID:{sample}\\tSM:{sample}\\tPL:ILLUMINA' "
    # Excluded information on flowcell, lane, and barcode index b/c not thought necessary
  shell:
    " (bwa mem {params.RG} -t {threads} {input.genome} {input.r1} {input.r2} | "
    " samtools view -u | "
    " samtools sort - > {output}) 2> {log}"













#### DATA QUALITY CONTROL ####
# Need to remove PCR duplicates and duplicated regions due to TEs and genome duplication


# Marking PCR duplicates 
rule mark_duplicates:
  input:
    "results/bam/hap{hap}/{sample}_hap{hap}.bam"
  output:
    bam="results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bam",
    bai="results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bai",
    metrics="results/qc/mkdup_metrics/{sample}_hap{hap}.metrics"
  log:
    "results/logs/mark_duplicates/hap{hap}/{sample}_hap{hap}.log"
  envmodules:
    "gatk/4.2.5.0"
  shell:
    " gatk MarkDuplicates  "
    "  --CREATE_INDEX "
    "  -I {input} "
    "  -O {output.bam} "  
    "  -M {output.metrics} "
    "  2> {log} "


# Rule to create .txt file of BAM files per population
rule generate_bam_list_per_population:
  input:
    expand("results/bam_mkdup/hap{hap}/{sample}_hap{hap}_mkdup.bam", sample=SAMPLES, hap=(1,2)),
  output:
    "data/angsd/{population}_mkdup_hap{hap}.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  params:
    output_dir="data/angsd/hap{hap}"
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population
    hap = wildcards.hap

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap{hap}_" in bam_file:
                output.write(f"{bam_file}\n")












# Call SNPs with stringent rules
rule angsd_raw_SNP:
  input:
    bam_list="data/angsd/{population}_mkdup_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/raw/{population}_raw_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/raw/{population}_raw_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/raw/{population}_raw_SNPs.hwe.gz",
    depth_sample="results/angsd/hap{hap}/raw/{population}_raw_SNPs.depthSample",
    depth_global="results/angsd/hap{hap}/raw/{population}_raw_SNPs.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw/{population}_raw_SNPs",
    threads=8,
    hap="{hap}",
    population="{population}"
  log:
    "results/logs/angsd/hap{hap}/angsd_SNP_raw_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 2\
    -doMajorMinor 1\
    -doMaf 2\
    -SNP_pval 1e-6\
    -minMapQ 30\
    -minQ 20 \
    -minInd 25\
    -minMaf .05\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {params.threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    &> {log}
    """


# Create BED files so including only SNPs into ngsParalog
rule convert_mafs_to_bed_stringent:
  input:
    mafs_gz="results/angsd/hap{hap}/raw/{population}_raw_SNPs.mafs.gz"
  output:
    bed_file="results/bed/hap{hap}/raw/{population}_raw_SNPs.BED"
  shell:
   """
   gunzip -c {input.mafs_gz} | awk 'NR>1 {{print $1, $2 - 1, $2}}' > {output.bed_file}
   dos2unix {output.bed_file}  # Add this line to convert line endings
   """

  
# Create wildcard for scaffold names
rule extract_scaffolds:
  input:
    index="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt"
  shell:
    """
    awk '{{print $1":1-"$2}}' {input.index} > {output.scaffolds}
    """


# Run ngsParalog on all SNPs across the genome
rule ngsParalog_hap1:
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_mkdup/hap1/{sample}_hap1_mkdup.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    ref="data/reference/hap1/lupinehap1.fasta",
    bed_file="results/bed/hap1/raw/{population}_raw_SNPs.BED"
  output:
    paralog_output="results/ngs_paralog/hap1/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap1/{population}_scaffolds/{population}-{hap1scaffold_prefix}.log"
  params:
    hap1scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap1scaffold_prefix, 1)
  envmodules:
    "samtools/1.17"
  shell:
    """
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -d 1000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap1scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} \
    -minQ 20 -minind 10 -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


rule ngsParalog_hap2:
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_mkdup/hap2/{sample}_hap2_mkdup.bam", sample=[s for s in SAMPLES if s.startswith(wildcards.population)]),
    ref="data/reference/hap2/lupinehap2.fasta",
    bed_file="results/bed/hap2/raw/{population}_raw_SNPs.BED"
  output:
    paralog_output="results/ngs_paralog/hap2/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap2/{population}_scaffolds/{population}-{hap2scaffold_prefix}.log"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.17"
  shell:
    """
    samtools mpileup {input.bam_ngsPara} -d 1000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap2scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} \
    -minQ 20 -minind 10 -mincov 1 \
    -runinfo 1 \
    2>> {log}
    if [ ! -s {output.paralog_output} ]; then
        touch {output.paralog_output}
    fi
    """


# Combine all ngsParalog outputs together into one file per population
rule concatenate_paralog_hap1:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap1/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}.lr", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap1/concat/{population}_paralog_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/concat/{population}_paralog_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


rule concatenate_paralog_hap2:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap2/{population}_scaffolds/{{population}}_{hap2scaffold_prefix}.lr", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap2/concat/{population}_paralog_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/concat/{population}_paralog_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


# Identify false positives from ngsParalog (grab only true Paralogs)
rule ngsParalog_false_pos:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_WGS.lr"
  output:
    deviant_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs.BED"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/ngs_paralog_false_pos.R {input.lr_file} {output.deviant_snps}"


# Create Manhattan Plots [NOTE: Specific Scaffold # otherwise it'll take forever for whole genome!] 
rule ngsParalog_manhattan:
  input:
    lr_file="results/ngs_paralog/hap{hap}/{population}_scaffolds/{population}-Scaffold_1.lr"
  output:
    plot="results/ngs_paralog/plots/hap{hap}/{population}_manhattan_plot_scaffold_1.png"
  envmodules:
    "r/4.3.1"
  threads: 2
  shell:
    "Rscript scripts/ngs_paralog_graphs.R {input.lr_file} {output.plot}"


# Run dupHMM to infer transitions between dup maps and normal mapping regions
rule dupHMM:
  input:
    lr_file = "results/ngs_paralog/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file = "[NEED COVERAGE FILE PER SCAFFOLD]"
  output:
    hmm_output = "results/ngs_paralog/dupHMM/{population}_dupHMM_{hap1scaffold_prefix}"
  params:
    r_script_path = "/home/socamero/ngsParalog/ngsParalog/dupHMM.R"
  envmodules:
    "r/4.3.1"
  shell:
    """
    Rscript {params.r_script_path} --lrfile {input.lr_file} \
    --outfile {output.hmm_output} \
    --covfile {input.cov_file} \
    --lrquantile 0.98
    """


# Filter out deviant SNPs from all SNPs discovered
# Filter out deviant SNPs from all SNPs discovered
rule filter_deviant_snps:
  input:
    all_snps = "results/bed/hap{hap}/raw/{population}_raw_SNPs.BED",
    deviant_snps = "results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs.BED"
  output:
    canonical_snps = "results/bed/hap{hap}/canonical_SNPs/{population}_canonical_SNPs.BED"
  envmodules:
    "bedtools/2.30.0"
  log:
    "results/logs/qc/filter_deviants/{population}_hap{hap}_filter_deviants.log"
  shell:
    """
    # Define temporary file names directly in the shell command
    temp_all_snps_bed="temp_all_snps_{wildcards.population}_{wildcards.hap}.bed"
    temp_deviant_snps_bed="temp_deviant_snps_{wildcards.population}_{wildcards.hap}.bed"

    # Trim extra spaces or tabs in BED files and ensure numeric fields are not altered
    awk -v OFS='\\t' '{{ gsub(/^[ \\t]+|[ \\t]+$/, ""); print $1, int($2), int($3) }}' {input.all_snps} > $temp_all_snps_bed
    awk -v OFS='\\t' '{{ gsub(/^[ \\t]+|[ \\t]+$/, ""); print $1, int($2), int($3) }}' {input.deviant_snps} > $temp_deviant_snps_bed

    # Perform the subtraction
    bedtools subtract -a $temp_all_snps_bed -b $temp_deviant_snps_bed > {output.canonical_snps} 2> {log}

    # Cleanup temporary files
    rm $temp_all_snps_bed $temp_deviant_snps_bed
    """





# Calculate HWE to check of SNP filtering reduced heterozygote excess
rule angsd_canonical_SNP:
  input:
    bam_list="data/angsd/{population}_mkdup_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/canonical/SNP_canonical_{population}.arg",
    mafs_file="results/angsd/hap{hap}/canonical/SNP_canonical_{population}.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/SNP_canonical_{population}.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical/SNP_canonical_{population}.depthSample",
    depth_global="results/angsd/hap{hap}/canonical/SNP_canonical_{population}.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/SNP_canonical_{population}",
    threads=8,
    canonical_snps="results/bed/hap{hap}/canonical_SNPs/{population}_canonical_SNPs.BED"
  log:
    "results/logs/angsd/hap{hap}/angsd_SNP_canonical_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.939"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -GL 2\
    -sites {params.canonical_snps}\
    -doMajorMinor 1\
    -doMaf 2\
    -SNP_pval 1e-6\
    -minMapQ 30\
    -minQ 20 \
    -minInd 25\
    -minMaf .05\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {params.threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    &> {log}
    """


# Create histogram of F values
rule excess_heterozygotes_histogram:
  input:
    hwe_file = "results/angsd/hap{hap}/canonical/SNP_canonical_{population}.hwe.gz"
  output:
    plot = "results/angsd/hap{hap}/plots/hetero_excess_canonical_SNPs_hap{hap}.png"
  envmodules:
    "r/4.3.1"
  shell:
    "Rscript scripts/hetero_excess_canonical_SNPs.R {input.hwe_file} {output.plot}"










#### POPULATION GENOMICS ####








## UNUSED RULES ##


# Calling genotype likelihoods (GLs) for variant calling using bam files w/ marked duplicates
rule make_gvcfs:
  input:
    bam="results/mkdup/hap{hap}/{sample}_hap{hap}_filtered.bam",
    bai="results/mkdup/hap{hap}/{sample}_hap{hap}_filtered.bai",
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    idx="data/reference/hap{hap}/lupinehap{hap}.dict",
    fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    gvcf="results/gvcf/hap{hap}/{sample}_hap{hap}.g.vcf.gz",
    idx="results/gvcf/hap{hap}/{sample}_hap{hap}.g.vcf.gz.tbi",
  params:
    java_opts="-Xmx4g",
    tempdir="/tmp/bam"
  log:
    "results/logs/make_gvcfs/{sample}_hap{hap}.log"
  envmodules:
    "gatk/4.2.5.0",
    "StdEnv/2020"
  threads: 4
  shell:
    " gatk --java-options \"{params.java_opts}\" HaplotypeCaller "
    " --native-pair-hmm-threads {threads} "
    " -R {input.ref} "
    " -I {input.bam} "
    " -O {output.gvcf} "
    " --tmp-dir {params.tempdir} "
    " -ERC GVCF > {log} 2> {log} "