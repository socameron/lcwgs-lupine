###########################
##    WILDCARDS SET-UP   ##
###########################

import os
import shlex
import glob
import subprocess
import pandas as pd
import yaml
import re
import itertools
import gzip
from pathlib import Path






#### Functions to set up samples list in config from different sequencing batches

# In summary:
  #{sample} refers to the sample name received from the sequencer. It includes replicates and other identifying info.
  #{sample_prefix} refers to a trimmed sample name and merges replicates.

# Rule to update the configuration file
# The python script creates a list of sample names and their corresponding .fastq files for both reads R1 and R2
rule update_config:
  output:
    "snakeprofile/config.yaml"
  shell:
    "python scripts/extract_lcWGS_samples.py"

# Function to get samples from the config file
def get_samples(batch):
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    return config['batches'][batch]

# Function to get batches from the config file
def get_batches():
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    return list(config['batches'].keys())

# Function to get .fastq filenames from all 3 batches from the config file
def get_fastq_paths(batch, sample, read):
    with open("snakeprofile/config.yaml", 'r') as file:
        config = yaml.safe_load(file)
    if batch in config['batches'] and sample in config['batches'][batch]:
        return config['batches'][batch][sample].get(read, f"data/{batch}/{sample}_L001_{read}_001.fastq.gz")
    # Fallback for batch_1 and batch_2:
    return f"data/{batch}/{sample}_L001_{read}_001.fastq.gz"

# Ensure that the config is updated before any other rule runs
# Get all batches from the config
batches = get_batches()

# Get samples for each batch
samples_batch_1 = get_samples('batch_1')
samples_batch_2 = get_samples('batch_2')
samples_batch_3 = get_samples('batch_3')

# Try using this for expand in rule all: 
  #  [f"results/bam_raw/hap2/{batch}/{sample}_hap2.bam"
  #   for batch in get_batches()
  #   for sample in list(get_samples(batch).keys())],

# Get basename of the fastq files
def get_basename(batch, sample, read):
    fastq_path = get_fastq_paths(batch, sample, read)
    if fastq_path:
        return os.path.basename(fastq_path).replace(".fastq.gz", "")
    return f"{sample}_{read}"  # Fallback in case of missing file


# Create list for fastqc outputs
expected_fastqc_outputs = []

for batch in get_batches():
    for sample in get_samples(batch).keys():
        # Compute the basenames from the full FASTQ paths.
        # This should always return a non-None string.
        r1_base = get_basename(batch, sample, "R1")
        r2_base = get_basename(batch, sample, "R2")

        expected_fastqc_outputs.extend([
            f"results/fastqc_raw/{batch}/{r1_base}_fastqc.html",
            f"results/fastqc_raw/{batch}/{r1_base}_fastqc.zip",
            f"results/fastqc_raw/{batch}/{r2_base}_fastqc.html",
            f"results/fastqc_raw/{batch}/{r2_base}_fastqc.zip"
        ])



#### Functions to set up lists of sample prefixes for merging .BAM files from different sequencing batches

# List of haplotypes
haps = ["1", "2"]

# List of populations
POPULATIONS = ["HPW", "IDNP-MW", "LCTGP", "MFNP", "PPP", "RLPLV", "SWCP", "APB", "BSL", "BSNA", "CPB", "FMB", "GRAY", "NBWA", "NGP", "PBBT", "RSB", "UWA"]

def extract_sample_prefix(sample_name):
    for pop in POPULATIONS:
        if sample_name.startswith(pop + "-"):
            # Get the portion before the first underscore.
            prefix = sample_name.split("_")[0]
            # If the prefix includes a replicate suffix, remove it.
            if "-rep" in prefix:
                prefix = prefix.split("-rep")[0]
            return prefix
    return sample_name

# Find replicates across BAM files as inputs into merge_replicates rule
def find_replicates(sample_prefix, hap):
    """
    Finds all BAM files corresponding to a given sample (using the unique sample_prefix)
    across batches (batch_1, batch_2, batch_3). The function expects the BAM file names to
    start exactly with sample_prefix and optionally a "-rep<number>" (for replicates), 
    immediately followed by an underscore and then "hap{hap}".
    """
    # Broad glob pattern to capture candidates
    pattern = f"results/bam_raw/hap{hap}/batch*/{sample_prefix}*_hap{hap}.bam"
    files = glob.glob(pattern, recursive=True)
    
    # Compile a regex to match:
    #   ^                 --> start of the filename
    #   sample_prefix     --> exact sample prefix (escaped)
    #   (?!\d)           --> not immediately followed by a digit (so "PPP-1" won't match "PPP-10")
    #   (?:-rep\d+)?     --> optionally match a replicate indicator like "-rep2"
    #   _hap             --> then an underscore and "hap..."
    regex = re.compile(r'^' + re.escape(sample_prefix) + r'(?:-rep\d+)?_')

    
    # Filter files based on the regex match on the base name
    filtered_files = [f for f in files if regex.search(os.path.basename(f))]
    
    # Write a debug file for inspection
    debug_path = f"debug/debug_find_replicates_{sample_prefix}_hap{hap}.txt"
    os.makedirs(os.path.dirname(debug_path), exist_ok=True)
    with open(debug_path, "w") as f:
        f.write(f"Pattern: {pattern}\n")
        if filtered_files:
            f.write("Found files:\n")
            for file in filtered_files:
                f.write(f"{file}\n")
        else:
            f.write("No files found.\n")
    
    return filtered_files


# Get all unique sample prefixes by extracting from samples listed in config file
def list_sample_prefixes():
    samples_batch_1 = get_samples('batch_1')  # returns a dict of {sample_name: {...}}
    samples_batch_2 = get_samples('batch_2')
    samples_batch_3 = get_samples('batch_3')
    # Combine sample names from all batches
    all_samples = set(
        list(samples_batch_1.keys()) +
        list(samples_batch_2.keys()) +
        list(samples_batch_3.keys())
    )
    # Now extract the prefix from each sample name using your extract_sample_prefix function.
    sample_prefixes = {extract_sample_prefix(sample) for sample in all_samples}
    return list(sample_prefixes)


# List of sample prefixes
sample_prefixes = list_sample_prefixes()
   



# Debugging: Write sample_prefixes and haps to files
#with open("debug_sample_prefixes.txt", "w") as f:
    #f.write("Sample Prefixes:\n")
    #for sp in sample_prefixes:
        #f.write(f"{sp}\n")

#with open("debug_haps.txt", "w") as f:
    #f.write("Haplotypes:\n")
    #for hap in haps:
        #f.write(f"{hap}\n")

# Use sample prefixes and designate to a particular population
def get_population_sample_prefixes(population):
    # Get all sample prefixes from the existing list_sample_prefixes function
    all_sample_prefixes = list_sample_prefixes()
    # Filter the sample prefixes based on the population
    population_sample_prefixes = [prefix for prefix in all_sample_prefixes if prefix.startswith(population)]
    return population_sample_prefixes





#### Functions to create scaffold names list per scaffold
# Get first 24 scaffold names to later estimate LD per scaffold
# Not using other scaffolds as they don't quite align to the expected 24 chromosomes

rule get_scaffold_names:
  input:
    "data/reference/lupinehap1.fasta",
    "data/reference/lupinehap2.fasta"
  output:
    "results/scaffolds/hap1_scaffolds.txt",
    "results/scaffolds/hap2_scaffolds.txt"
  shell:
    """
    python scripts/extract_24_scaffold_names_by_hap.py
    """

with open("results/scaffolds/hap1_scaffolds.txt", "r") as file:
    HAP1SCAFFOLDS = [line.strip() for line in file.readlines()]

with open("results/scaffolds/hap2_scaffolds.txt", "r") as file:
    HAP2SCAFFOLDS = [line.strip() for line in file.readlines()]

# Split scaffold names by comma and create a list
HAP1SCAFFOLDS = [name.strip() for name in HAP1SCAFFOLDS]
HAP2SCAFFOLDS = [name.strip() for name in HAP2SCAFFOLDS]
HAP1SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP1SCAFFOLDS]
HAP2SCAFFOLD_PREFIXES = [s.split("__")[0] for s in HAP2SCAFFOLDS]

# Function give full scaffold name given prefix of scaffold
def map_prefix_to_full_scaffold(prefix, hap_type):
    scaffold_list = HAP1SCAFFOLDS if hap_type == 1 else HAP2SCAFFOLDS
    for scaffold in scaffold_list:
        if scaffold.startswith(prefix):
            return scaffold
    return None  
# Return None or an appropriate default if not found

wildcard_constraints:
    population="({})".format("|".join(POPULATIONS)),
    hap2scaffold_prefix="({})".format("|".join(HAP2SCAFFOLD_PREFIXES))

#### Bunch of parameters for using ngsParalog, Fst, ANGSD, etc

# Varying levels of Bonferroni-Hotchberg correction for ngsParalog
BH_VARS=[50,40,30,20,10,5]

# Varying levels of site depth to check filter levels
depth_params = [(50, 1500), (50, 2000), (50, 2500), (50, 3000), (100, 1500), (100, 2000), (100, 2500), (100, 3000)]

# Varying levels of minor allele frequency cutoffs
minMAF_params = [0.01, 0.001, 0.0001, 0.00001]

# For identifying populatio pairs for Fst analysis
POP_COMBINATIONS = list(itertools.combinations(POPULATIONS, 2))

# Varying levels of K for admixture analyses
K_values = [16,17,18,19,20]

# # These functions are necessary for bcftools/roh because it calls for individual .vcf files within each popln. 
# Function to expand VCF file paths
def generate_vcf_files():
    vcf_files = []
    for population in POPULATIONS:
        sample_prefixes = get_population_sample_prefixes(population)
        vcf_files.extend(
            [f"results/angsd/hap2/canonical/RZooROH_input/{population}/{prefix}_canonical_SNPs.vcf.gz" for prefix in sample_prefixes]
        )
    return vcf_files

# Function to generate annotated BCF file paths
def generate_annotated_vcfs():
    annotated_vcfs = []
    for population in POPULATIONS:
        sample_prefixes = get_population_sample_prefixes(population)
        annotated_vcfs.extend(
            [f"results/RZooROH/hap2/RZooROH_analysis/{population}/{prefix}_annotated.vcf.gz" for prefix in sample_prefixes]
        )
    return annotated_vcfs

# Use *generate_vcf_files() or *generate_annotated_vcfs()

# Store genome size (# of chromosomes) in config
GENOME_MORGANS = config.get("genome_morgans")

# Grid of K values for currentNe2
K_GRID = [x for x in str(config.get("k_grid", "0,0.2,0.33,0.5,1.0")).split(",") if x != ""]

###########################
## BEGINNING OF WORKFLOW ##
###########################


# Expand the final files using the updated configuration
rule all:
  input:
    #expand("results/ngsF/hap2/by_popln/{population}_ngsF-HMM_inbreeding.indF", population=POPULATIONS)
    #"results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.admix.18.Q",
    #expand("results/theta/hap2/{population}_out.thetasWindow.gz.pestPG", population=POPULATIONS)

    

##########################
#### DATA PREPARATION ####
##########################

# Trim adapter ends off each sequence file using Trimmomatic
rule trim_reads:
  input:
    r1=lambda wildcards: get_fastq_paths(wildcards.batch, wildcards.sample, "R1"),
    r2=lambda wildcards: get_fastq_paths(wildcards.batch, wildcards.sample, "R2")
  output:
    r1="results/fastq_trimmed/{batch}/{sample}_R1.fastq.gz",
    r1_unp="results/fastq_trimmed/{batch}/{sample}_R1_unpaired.fastq.gz",
    r2="results/fastq_trimmed/{batch}/{sample}_R2.fastq.gz",
    r2_unp="results/fastq_trimmed/{batch}/{sample}_R2_unpaired.fastq.gz"
  log:
    "results/logs/trim_reads/{batch}/{sample}.log"
  envmodules:
    "trimmomatic/0.39"
  params:
    adapters="$EBROOTTRIMMOMATIC/adapters/TruSeq3-PE-2.fa"
  shell:
    "java -jar $EBROOTTRIMMOMATIC/trimmomatic-0.39.jar PE {input.r1} {input.r2} "
    "{output.r1} {output.r1_unp} {output.r2} {output.r2_unp} "
    "ILLUMINACLIP:{params.adapters}:2:30:10:1 " #previously had ':True' but this failed even though there's documentation of this online
    "LEADING:3 "
    "TRAILING:3 "
    "SLIDINGWINDOW:4:15 " # removes low quality bases
    "MINLEN:36 2> {log}"


# Unzip trimmed files as fastqc 0.12.0 cannot properly read compressed files. 
# Tried with 0.12.1 and it works!
rule unzip_files:
  input:
    zipped_r1="results/fastq_trimmed/{batch}/{sample}_R1.fastq.gz",
    zipped_r2="results/fastq_trimmed/{batch}/{sample}_R2.fastq.gz"
  output: 
    unzipped_r1="results/fastq_trimmed_unzip/{batch}/{sample}_R1.fastq",
    unzipped_r2="results/fastq_trimmed_unzip/{batch}/{sample}_R2.fastq"
  shell:
    "gunzip -c {input.zipped_r1} > {output.unzipped_r1} && gunzip -c {input.zipped_r2} > {output.unzipped_r2}"

# Run FastQC per each trimmed sequence file
# Attempting zipped files since using fastqc/0.12.1
rule fastqc:
  input:
    fastq_r1="results/fastq_trimmed/{batch}/{sample}_R1.fastq.gz",
    fastq_r2="results/fastq_trimmed/{batch}/{sample}_R2.fastq.gz"
  output:
    html_report_r1="results/fastqc/{batch}/{sample}_R1_fastqc.html",
    zip_report_r1="results/fastqc/{batch}/{sample}_R1_fastqc.zip",
    html_report_r2="results/fastqc/{batch}/{sample}_R2_fastqc.html",
    zip_report_r2="results/fastqc/{batch}/{sample}_R2_fastqc.zip"
  log:
    path="results/logs/fastQC/{batch}/{sample}.log"
  envmodules:
    "fastqc/0.12.1"
  shell:
    "fastqc {input.fastq_r1} {input.fastq_r2} --outdir results/fastqc/{wildcards.batch} 2> {log.path}"

# A little different from above because using the raw fastq paths as well as deriving the filenames to get expected fastqc outputs
# Call in rule all with: expected_fastqc_outputs (this is a predetermined list from above)
rule fastqc_raw:
  input:
    fastq_r1=lambda wildcards: get_fastq_paths(wildcards.batch, wildcards.sample, "R1"),
    fastq_r2=lambda wildcards: get_fastq_paths(wildcards.batch, wildcards.sample, "R2")
  output:
    html_report_r1="results/fastqc_raw/{batch}/{sample}_R1_fastqc.html",
    zip_report_r1="results/fastqc_raw/{batch}/{sample}_R1_fastqc.zip",
    html_report_r2="results/fastqc_raw/{batch}/{sample}_R2_fastqc.html",
    zip_report_r2="results/fastqc_raw/{batch}/{sample}_R2_fastqc.zip"
  params:
    # Compute the default names produced by fastqc from the full input filenames.
    r1_default=lambda wildcards: os.path.basename(get_fastq_paths(wc.batch, wc.sample, "R1")).replace(".fastq.gz", ""),
    r2_default=lambda wildcards: os.path.basename(get_fastq_paths(wc.batch, wc.sample, "R2")).replace(".fastq.gz", "")
  log:
    path="results/logs/fastQC_raw/{batch}/{sample}.log"
  envmodules:
    "fastqc/0.12.1"
  shell:
    """
    # Run FastQC: it will produce files named like:
    #   {params.r1_default}_fastqc.html and {params.r1_default}_fastqc.zip
    #   for read 1, and similarly for read 2.
    fastqc {input.fastq_r1} {input.fastq_r2} --outdir results/fastqc_raw/{wildcards.batch} 2> {log.path}
    
    # Rename the outputs to use your config sample name (the shortened version)
    mv results/fastqc_raw/{wildcards.batch}/{params.r1_default}_fastqc.html {output.html_report_r1}
    mv results/fastqc_raw/{wildcards.batch}/{params.r1_default}_fastqc.zip  {output.zip_report_r1}
    mv results/fastqc_raw/{wildcards.batch}/{params.r2_default}_fastqc.html {output.html_report_r2}
    mv results/fastqc_raw/{wildcards.batch}/{params.r2_default}_fastqc.zip  {output.zip_report_r2}
    """

# Create an aggregated FastQC report using MultiQC.
# Note that we create separate MultiQC reports for batch 1 to 3; and .fastqc files must exist prior to calling it on snakemake
rule multiqc_trimmed:
  input:
    fastqc_dir="results/fastqc/{batch}"
  output:
    html_report="results/multiqc/multiqc_report_trimmed_{batch}.html"
  log:
    path="results/logs/multiqc/{batch}.log"
  params:
    fastqc_dir="results/fastqc/{batch}"
  shell:
    "multiqc -n {output.html_report} {input.fastqc_dir} 2> {log.path}"

rule multiqc_raw:
  input:
    fastqc_dir="results/fastqc_raw/{batch}"
  output:
    html_report="results/multiqc_raw/multiqc_report_raw_{batch}.html"
  log:
    path="results/logs/multiqc_raw/{batch}.log"
  params:
    fastqc_dir="results/fastqc_raw/{batch}"
  shell:
    "multiqc -n {output.html_report} {input.fastqc_dir} 2> {log.path}"

# Creating faidx index for reference genome
rule faidx_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    "data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  log:
    "results/logs/refgen/lupinehap{hap}_faidx.log"
  envmodules:
    "samtools/1.20"
  shell:
    "samtools faidx {input} 2> {log}"

# Rules for indexing reference genomes (haplotypes 1 and 2)
rule index_reference:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa")
  log:
    "results/logs/refgen/lupinehap{hap}_bwa_index.log"
  envmodules:
    "bwa/0.7.18"
  shell:
    "bwa index {input} 2> {log}"

# Rules for creating dictionary files
rule create_dict:
  input:
    "data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    "data/reference/hap{hap}/lupinehap{hap}.dict"
  log:
    "results/logs/refgen/hap{hap}_dict.log"
  envmodules:
    "samtools/1.20"
  shell:
    "samtools dict {input} > {output} 2> {log}"

# Removing reads smaller than 70bp so 'bwa mem' works better
# Note for a read to be kept, it must be greater than 70bp in BOTH read 1 and 2.
# There are some tools available in envmodules 'seqkit', but I found none that were compatible
rule filter_short_reads_with_seqkit:
  input:
    r1="results/fastq_trimmed/{batch}/{sample}_R1.fastq.gz",
    r2="results/fastq_trimmed/{batch}/{sample}_R2.fastq.gz"
  output:
    r1_filtered="results/fastq_filtered/{batch}/{sample}_R1_filtered.fastq.gz",
    r2_filtered="results/fastq_filtered/{batch}/{sample}_R2_filtered.fastq.gz"
  envmodules:
    "StdEnv/2020",
    "seqkit/2.3.1"
  log:
    "results/logs/filter_fastq_70bp/{batch}/{sample}_filter_fastq.log"
  shell:
    """
    seqkit seq -m 70 {input.r1} | gzip > {output.r1_filtered}
    seqkit seq -m 70 {input.r2} | gzip > {output.r2_filtered}
    """

# Pair filtered reads back together
# Note: naming convention stays the same! Just different folder
rule pair_filtered_reads:
  input:
    r1_filtered="results/fastq_filtered/{batch}/{sample}_R1_filtered.fastq.gz",
    r2_filtered="results/fastq_filtered/{batch}/{sample}_R2_filtered.fastq.gz"
  output:
    r1_paired="results/fastq_paired/{batch}/{sample}_R1_filtered.fastq.gz",
    r2_paired="results/fastq_paired/{batch}/{sample}_R2_filtered.fastq.gz"
  envmodules:
    "StdEnv/2020",
    "seqkit/2.3.1"
  log:
    "results/logs/pair_fastq_70bp/{batch}/{sample}_pair_fastq.log"
  shell:
    """
    seqkit pair \
    -1 {input.r1_filtered} \
    -2 {input.r2_filtered} \
    -O results/fastq_paired/{wildcards.batch}
    """

# NOTE: Prior to mapping, some people like to merge fastqs from the same individual/library and remove PCR duplicates prior to mapping using SuperDeduper from HTStream (last used 2020)
# This might be helpful in reducing heterozygote excess
# I have opted not to do this yet as it may be outdated.

# Mapping/Aligning reads to reference haplotypes
rule map_reads:
  input:
    r1="results/fastq_paired/{batch}/{sample}_R1_filtered.fastq.gz",
    r2="results/fastq_paired/{batch}/{sample}_R2_filtered.fastq.gz",
    genome="data/reference/hap{hap}/lupinehap{hap}.fasta",
    idx=multiext("data/reference/hap{hap}/lupinehap{hap}.fasta", ".amb", ".ann", ".bwt", ".pac", ".sa")
  output:
    "results/bam_raw/hap{hap}/{batch}/{sample}_hap{hap}.bam"
  log:
    "results/logs/map_reads/hap{hap}/{batch}/{sample}_hap{hap}.log"
  envmodules:
    "bwa/0.7.18",
    "samtools/1.20"
  threads: 12 
  params:
    # Biological sample (constant across libraries)
    SM=lambda wc: extract_sample_prefix(wc.sample),
    # Per-library identifiers (unique per {sample} string)
    ID=lambda wc: wc.sample,
    LB=lambda wc: extract_sample_prefix(wc.sample) + "_LB",
    PU=lambda wc: f"{wc.batch}.{wc.sample}"
  shell:
    r"""
    RG='@RG\tID:{params.ID}\tSM:{params.SM}\tLB:{params.LB}\tPL:ILLUMINA\tPU:{params.PU}'

    (bwa mem -R "$RG" -t {threads} {input.genome} {input.r1} {input.r2} \
      | samtools view -u \
      | samtools sort -o {output}) 2> {log}
    """

# ID = unique ID | LB = library | PL = platform | PU = platform unit | SM = sample


# To call 'raw' .bam files, we use the get_fastq_paths function in previous rules because the naming conventions are different between batch_1/batch_2 and batch_3. 
# In rule all, you can call:
    #[f"results/bam_raw/hap2/{batch}/{sample}_hap2.bam"
     #for batch in get_batches()
     #for sample in list(get_samples(batch).keys())],

# Sometimes the metadata is randomly not changed (e.g ComputeCanada suddenly replaces all your new files with backups)
rule fix_rg:
  input:
    bam = "results/bam_raw/hap{hap}/{batch}/{sample}_hap{hap}.bam"
  output:
    bam = "results/bam_raw/hap{hap}/{batch}/{sample}_hap{hap}.rg.bam",
    bai = "results/bam_raw/hap{hap}/{batch}/{sample}_hap{hap}.rg.bam.bai"
  log:
    "results/logs/fix_rg/hap{hap}/{batch}/{sample}.log"
  envmodules:
    "samtools/1.20"
  threads: 2
  params:
    SM = lambda wc: extract_sample_prefix(wc.sample),           # e.g. "LCTGP-49"
    ID = lambda wc: wc.sample,                                   # keep lane-unique ID
    LB = lambda wc: extract_sample_prefix(wc.sample) + "_LB",
    PU = lambda wc: f"{wc.batch}.{wc.sample}"
  shell:
    r"""
    samtools addreplacerg \
      -r ID:{params.ID} -r SM:{params.SM} -r LB:{params.LB} -r PL:ILLUMINA -r PU:{params.PU} \
      -o {output.bam} {input.bam} 2> {log}
    samtools index -f {output.bam} 2>> {log}
    """



##############################
#### DATA QUALITY CONTROL ####
##############################

#### DATA QUALITY CONTROL ####

# Steps in QC :
  # 1. Mark PCR and optical duplicates
  # 2. Produce HWE and SFS plots to evaluate mapping problems
  # 3. Identify paralogous regions causing mapping problems
      # - Requires ngsParalog and 1st run of ANGSD
      # - ANGSD: SNP call and input SNPs (aka polymorphic sites) into ngsParalog
      # - ngsParalog: probablistic call of paralogous regions
      # - After ngsParalog, calculate p-values based on chi-sq df = 1 with Benjamini-Hochberg correction
  # 4. Verify heterozygote excess is reduced
      # - Re-run ANGSD excluding paralogous regions and PCR duplicates
      # - Visualize SFS

## STEP 1: MERGE REPLICATES & MARK PCR & OPTICAL DUPLICATES 


# merge replicates if found, otherwise rename and move to hap{hap}/merged/
# NEW: added rm results/bam_raw/hap{hap}/merged/*rep2*.bam because the file remained in the folder. 
rule merge_replicates:
  input:
    lambda wildcards: find_replicates(wildcards.sample_prefix, wildcards.hap)
  output:
    "results/bam_raw/hap{hap}/merged/{sample_prefix}_hap{hap}.bam"
  log:
    "results/logs/merge_replicates/hap{hap}/{sample_prefix}_hap{hap}.log"
  envmodules:
    "samtools/1.17"
  shell:
    """
    module load StdEnv/2020
    module load samtools/1.17
    echo "Input files: {input}" >> {log}
    echo "Output file: {output}" >> {log}
    
    # If no input files are found, exit with an error.
    if [ -z "$(echo {input} | tr -d '[:space:]')" ]; then
      echo "No files found for {wildcards.sample_prefix} in hap{wildcards.hap}" >> {log}
      exit 1
    # If there's only one file, just copy it.
    elif [ $(echo {input} | wc -w) -eq 1 ]; then
      echo "Single file found for {wildcards.sample_prefix} in hap{wildcards.hap}, copying to merged folder." >> {log}
      cp {input} {output}
    else
      echo "Multiple files found for {wildcards.sample_prefix} in hap{wildcards.hap}. Merging..." >> {log}
      samtools merge -f -o {output} {input} 2>> {log}
    fi
    sync

    rm -f results/bam_raw/hap{wildcards.hap}/merged/*rep2*.bam
    """




# Marking and removing PCR duplicates + index
# Note that: /bam_mkdup/ is where marked duplicates are marked AND removed.
# marked duplicates here are now removed (updated March 26, 2025)
rule mark_remove_duplicates:
  input:
    raw_bam="results/bam_raw/hap{hap}/merged/{sample_prefix}_hap{hap}.bam"
  output:
    bam="results/bam_mkdup/hap{hap}/{sample_prefix}_hap{hap}_mkdup.bam",
    bai="results/bam_mkdup/hap{hap}/{sample_prefix}_hap{hap}_mkdup.bai",
    metrics="results/qc/mkdup_metrics/{sample_prefix}_hap{hap}.metrics"
  log:
    "results/logs/mark_remove_duplicates/hap{hap}/{sample_prefix}_hap{hap}.log"
  envmodules:
    "gatk/4.4.0.0"
  shell:
    """
    gatk MarkDuplicates \
    --CREATE_INDEX \
    -I {input.raw_bam} \
    -O {output.bam} \
    -M {output.metrics} \
    --REMOVE_DUPLICATES true \
    2> {log}
    """


# Clip overlapping reads
# Previous we loaded packages nixpkgs/16.09 intel/2018.3 to run bamutil/1.0.14 but these might be depreciated with new changes to the Supercomputer
# We use samtools/1.18 because it is compatible with the output of bamutil/1.0.14
rule clip_overlapping_reads:
  input:
    bam="results/bam_mkdup/hap{hap}/{sample_prefix}_hap{hap}_mkdup.bam"
  output:
    clipped_bam="results/bam_clipped/hap{hap}/{sample_prefix}_hap{hap}_clipped.bam",
    clipped_index="results/bam_clipped/hap{hap}/{sample_prefix}_hap{hap}_clipped.bai"
  log:
    "results/logs/clip_overlap/hap{hap}/{sample_prefix}_clip_overlapping_reads.log"
  shell:
    """
    module --force purge
    module load StdEnv/2020
    module load bamutil/1.0.14
    bam clipOverlap --in {input.bam} --out {output.clipped_bam} 2> {log} || true
    module --force purge
    module load StdEnv/2023 samtools/1.18
    samtools index -b {output.clipped_bam} -o {output.clipped_index} --threads 4
    module --force purge
    """

# After downstream analyses (admixture), it appears that LCTGP-19 and SWCP-19 are mislabeled and need to swap names.
# The first and second raw reads (fastq files) for both SWCP-19 and LCTGP-19 still match. It was just mislabelled at the sequencing step!
# NOTE: The metadata in the RG group is incorrect still and hasn't been updated!
rule rename_specific_bam_files:
  input:
    lctgp_bam="results/bam_clipped/hap{hap}/LCTGP-19_hap{hap}_clipped.bam",
    lctgp_bai="results/bam_clipped/hap{hap}/LCTGP-19_hap{hap}_clipped.bai",
    swcp_bam="results/bam_clipped/hap{hap}/SWCP-19_hap{hap}_clipped.bam",
    swcp_bai="results/bam_clipped/hap{hap}/SWCP-19_hap{hap}_clipped.bai"
  output:
    checkpoint="results/checkpoints/hap{hap}/rename_specific_files_checkpoint.txt"
  log:
    "results/logs/rename_specific_files/hap{hap}/rename_specific_files.log"
  shell:
    """
    # Temporary files to avoid overwriting
    tmp_lctgp_bam="results/bam_clipped/hap{wildcards.hap}/LCTGP-19_temp.bam"
    tmp_lctgp_bai="results/bam_clipped/hap{wildcards.hap}/LCTGP-19_temp.bai"
    tmp_swcp_bam="results/bam_clipped/hap{wildcards.hap}/SWCP-19_temp.bam"
    tmp_swcp_bai="results/bam_clipped/hap{wildcards.hap}/SWCP-19_temp.bai"

    # Copy the files to temporary locations
    cp {input.lctgp_bam} $tmp_lctgp_bam
    cp {input.lctgp_bai} $tmp_lctgp_bai
    cp {input.swcp_bam} $tmp_swcp_bam
    cp {input.swcp_bai} $tmp_swcp_bai

    # Rename LCTGP-19 to SWCP-19 and vice versa using the temporary files
    mv $tmp_lctgp_bam results/bam_clipped/hap{wildcards.hap}/SWCP-19_hap{wildcards.hap}_clipped.bam
    mv $tmp_lctgp_bai results/bam_clipped/hap{wildcards.hap}/SWCP-19_hap{wildcards.hap}_clipped.bai
    mv $tmp_swcp_bam results/bam_clipped/hap{wildcards.hap}/LCTGP-19_hap{wildcards.hap}_clipped.bam
    mv $tmp_swcp_bai results/bam_clipped/hap{wildcards.hap}/LCTGP-19_hap{wildcards.hap}_clipped.bai

    # Remove the temporary files
    rm -f $tmp_lctgp_bam $tmp_lctgp_bai $tmp_swcp_bam $tmp_swcp_bai

    # Create a checkpoint file to indicate renaming is complete
    echo "LCTGP-19 and SWCP-19 file names successfully swapped and renamed!" > {output.checkpoint}
    """


# Create bam list per population for entry into realign indels 
# Set for haplotype 2
rule generate_clipped_bam_list_per_population:
  input:
    expand("results/bam_clipped/hap2/{sample_prefix}_hap2_clipped.bam", sample_prefix=sample_prefixes),
    checkpoint="results/checkpoints/hap2/rename_specific_files_checkpoint.txt"
  output:
    "data/lists/hap2/{population}_clipped_hap2.list"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")


# Create interval list of indels
# We use apptainer, since I've had issues running gatk/3.8 with simply module load on the DRAC clusters
# MUST PRE-INSTALL gatk/3.8 container in login node
# apptainer pull gatk3.sif docker://broadinstitute/gatk3:3.8-1
rule indel_list:
  input:
    bam_list = "data/lists/hap{hap}/{population}_clipped_hap{hap}.list",
    reference = "data/reference/hap{hap}/lupinehap{hap}.fasta",
  output:
    intervals = "data/lists/hap{hap}/{population}_hap{hap}_indels.intervals",
  log:
    "results/logs/indel_list/hap{hap}/{population}_hap{hap}_indel_list.log",
  threads: 4
  envmodules:
    "apptainer/1.3.5",
  shell:
    """
    set -euo pipefail
    mkdir -p "$(dirname {output.intervals})" "$(dirname {log})"

    # Resolve your host scratch root (e.g., /home/socamero/links/scratch)
    HOST_SCRATCH="$(readlink -f /home/socamero/links/scratch)"

    if [[ ! -d "$HOST_SCRATCH" ]]; then
        echo "Host scratch path not found: $HOST_SCRATCH" >&2
        exit 1
    fi

    # Bind current project dir (for relative paths) AND map host scratch -> /links/scratch inside container
    env -u JAVA_TOOL_OPTIONS \
    apptainer exec --cleanenv \
      -B "$PWD:$PWD","$HOST_SCRATCH:/links/scratch" \
      --pwd "$PWD" \
      /home/socamero/gatk3.sif \
      java -Xms2g -Xmx16g -jar /usr/GenomeAnalysisTK.jar \
        -T RealignerTargetCreator \
        -R {input.reference} \
        -I {input.bam_list} \
        -o {output.intervals} \
        -drf BadMate \
        -nt {threads} \
      2> {log}
    """

rule realign_indels:
  input:
    bam = "results/bam_clipped/hap{hap}/{sample_prefix}_hap{hap}_clipped.bam",
    ref = "data/reference/hap{hap}/lupinehap{hap}.fasta",
    intervals = lambda wildcards: "data/lists/hap" + wildcards.hap + "/" + next(pop for pop in POPULATIONS if pop in wildcards.sample_prefix) + f"_hap{wildcards.hap}_indels.intervals",
  output:
    realigned_bam = "results/bam_realign/hap{hap}/{sample_prefix}_hap{hap}_realign.bam",
  log:
    "results/logs/realign_indels/hap{hap}/{sample_prefix}_hap{hap}_realign_indels.log",
  envmodules:
    "apptainer/1.3.5",
  shell:
    """
    set -euo pipefail
    mkdir -p "$(dirname {input.intervals})" "$(dirname {log})"

    # Resolve your host scratch root (e.g., /home/socamero/links/scratch)
    HOST_SCRATCH="$(readlink -f /home/socamero/links/scratch)"

    if [[ ! -d "$HOST_SCRATCH" ]]; then
        echo "Host scratch path not found: $HOST_SCRATCH" >&2
        exit 1
    fi

    # Bind your current project dir (for relative paths) + /links/scratch (for any absolute paths in lists)
    env -u JAVA_TOOL_OPTIONS \
    apptainer exec --cleanenv \
      -B "$PWD:$PWD","$HOST_SCRATCH:/links/scratch" \
      --pwd "$PWD" \
      /home/socamero/gatk3.sif \
      java -Xms2g -Xmx16g -jar /usr/GenomeAnalysisTK.jar \
        -T IndelRealigner \
        -R {input.ref} \
        -I {input.bam} \
        -targetIntervals {input.intervals} \
        -o {output.realigned_bam} \
        -drf BadMate \
      &> {log}
    """

# Rule to create .txt file of BAM files for each haplotype
# Only applies to hap2
rule generate_bam_list_realign:
  input:
    expand("results/bam_realign/hap2/{sample_prefix}_hap2_realign.bam", sample_prefix=sample_prefixes)
  output:
    "data/lists/hap2/all_realign_hap2.txt"
  run:
    bam_files = input
    output_file = output[0]

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")


rule generate_bam_list_clipped:
  input:
    expand("results/bam_clipped/hap2/{sample_prefix}_hap2_clipped.bam", sample_prefix=sample_prefixes)
  output:
    "data/lists/hap2/all_clipped_hap2.txt"
  run:
    bam_files = input
    output_file = output[0]

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")



# Merge bam lists into batches 1+2 and 3 because of different sequencing platforms
# Only applies to hap2
rule generate_bam_list_per_round:
  input:
    expand("results/bam_realign/hap2/{sample_prefix}_hap2_realign.bam", sample_prefix=sample_prefixes)
  output:
    first_round="data/lists/hap2/first_round_hap2.txt",
    second_round="data/lists/hap2/second_round_hap2.txt"
  run:
    first_round_bams = []
    second_round_bams = []

    # Define sequencing rounds
    first_round_populations = {"HPW", "IDNP-MW", "LCTGP", "MFNP", "PPP", "RLPLV", "SWCP"}
    
    # Extract population name dynamically
    def extract_population(sample_prefix):
        for pop in POPULATIONS:
            if sample_prefix.startswith(pop):
                return pop  # Return the correct population name
        return None  # If no match is found, which should not happen

    for bam_file in input:
        sample_prefix = bam_file.split("/")[-1].split("_hap")[0]  # Extract sample prefix
        population = extract_population(sample_prefix)  # Get correct population

        if population in first_round_populations:
            first_round_bams.append(bam_file)
        else:
            second_round_bams.append(bam_file)

    # Write BAM lists for each sequencing round
    with open(output.first_round, "w") as f:
        for bam in first_round_bams:
            f.write(f"{bam}\n")

    with open(output.second_round, "w") as f:
        for bam in second_round_bams:
            f.write(f"{bam}\n")


# Merge all bams to estimate per sample depth
rule merge_bams:
  input:
    bam_list="data/lists/hap{hap}/all_realign_hap{hap}.txt"
  output:
    merged_bam="results/bam_merge/hap{hap}/merged_hap{hap}.bam"
  log:
    "results/logs/merge_bams/merge_bam_{hap}.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    samtools merge -b {input.bam_list} {output.merged_bam} -@ 12 2> {log}
    """

rule index_bams:
  input:
    merged_bam="results/bam_merge/hap{hap}/merged_hap{hap}.bam"
  output:
    merged_bai="results/bam_merge/hap{hap}/merged_hap{hap}.bam.bai"
  log:
    "results/logs/merged_bams/index_bai_{hap}.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    samtools index {input.merged_bam} {output.merged_bai} 2> {log}
    """

ruleorder: 
    estimate_depth_round_1_per_scaffold > estimate_depth_hap2
  
ruleorder:
    estimate_depth_round_2_per_scaffold > estimate_depth_hap2

# Estimate depth per bp per sample
# -d flag : set max-depth to infinity
# -A flag : don't skip anamalous read pairs marked w/ FLAG but missing properly paired flag set
# -q flag : minimum mapping quality
# -Q flag : minimum base quality
# -r flag : specify regions for pileup; needs indexed BAM files
# -a flag : all positions including unused reference sequences
# -f flag : faidx-indexed reference fasta file
# -ff flag : exclude SECONDARY, QCFAIL, DUP
# Note: previously used mpileup with all 189 .bam files, but now merged them as one file because the positions are different across samples with flag -s
# Using ngsQC tools 'bamstats', created by Dr. Tyler Linderoth

# Estimating depth per scaffold!
rule estimate_depth_hap1:
  input:
    bam="results/bam_merge/hap1/merged_hap1.bam",
    bai="results/bam_merge/hap1/merged_hap1.bam.bai",
    ref="data/reference/hap1/lupinehap1.fasta"
  output:
    depth="results/depths/hap1/all_rounds/{hap1scaffold_prefix}_depth_est.txt.gz"
  params:
    hap1scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap1scaffold_prefix, 1)
  envmodules:
    "samtools/1.20"
  shell:
    """
    bamstats {input.bam} -r {params.hap1scaffold}\
    -A -d 77000000 -q 0 -Q 0 --ff UNMAP,SECONDARY,QCFAIL,DUP\
    -s -aa -f {input.ref} | gzip > {output.depth}
    """

rule estimate_depth_hap2:
  input:
    bam="results/bam_merge/hap2/merged_hap2.bam",
    bai="results/bam_merge/hap2/merged_hap2.bam.bai",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    depth="results/depths/hap2/all_rounds/{hap2scaffold_prefix}_depth_est.txt.gz"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.20"
  shell:
    """
    bamstats {input.bam} -r {params.hap2scaffold} \
    -A -d 77000000 -q 0 -Q 0 --ff UNMAP,SECONDARY,QCFAIL,DUP \
    -s -aa -f {input.ref} | gzip > {output.depth}
    """


# Merge bams per sequencing round (and platform)
rule merge_bams_round_1:
  input:
    first_round_bams="data/lists/hap{hap}/first_round_hap{hap}.txt"
  output:
    first_round_merged="results/bam_merge/hap{hap}/first_round_hap{hap}.bam",
    first_round_bai="results/bam_merge/hap{hap}/first_round_hap{hap}.bam.bai"
  log:
    first_round_log="results/logs/merge_bams/first_round_hap{hap}.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    samtools merge -b {input.first_round_bams} {output.first_round_merged} -@ 12 2> {log.first_round_log}
    samtools index {output.first_round_merged} {output.first_round_bai} 2>> {log.first_round_log}
    """

rule merge_bams_round_2:
  input:
    second_round_bams="data/lists/hap{hap}/second_round_hap{hap}.txt"
  output:
    second_round_merged="results/bam_merge/hap{hap}/second_round_hap{hap}.bam",
    second_round_bai="results/bam_merge/hap{hap}/second_round_hap{hap}.bam.bai"
  log:
    second_round_log="results/logs/merge_bams/second_round_hap{hap}.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    samtools merge -b {input.second_round_bams} {output.second_round_merged} -@ 12 2> {log.second_round_log}
    samtools index {output.second_round_merged} {output.second_round_bai} 2>> {log.second_round_log}
    """


# Only applies to hap2 
rule estimate_depth_round_1_per_scaffold:
  input:
    first_round_bam="results/bam_merge/hap2/first_round_hap2.bam",
    first_round_bai="results/bam_merge/hap2/first_round_hap2.bam.bai",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    first_round_depth="results/depths/hap2/round_1/{hap2scaffold_prefix}_depth_est.txt.gz"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Estimate depth for first sequencing round per scaffold
    bamstats {input.first_round_bam} -r {params.hap2scaffold} \
    -A -d 77000000 -q 0 -Q 0 --ff UNMAP,SECONDARY,QCFAIL,DUP \
    -s -aa -f {input.ref} | gzip > {output.first_round_depth}
    """

# Only applies to hap2 
rule estimate_depth_round_2_per_scaffold:
  input:
    second_round_bam="results/bam_merge/hap2/second_round_hap2.bam",
    second_round_bai="results/bam_merge/hap2/second_round_hap2.bam.bai",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    second_round_depth="results/depths/hap2/round_2/{hap2scaffold_prefix}_depth_est.txt.gz"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2)
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Estimate depth for second sequencing round per scaffold
    bamstats {input.second_round_bam} -r {params.hap2scaffold} \
    -A -d 77000000 -q 0 -Q 0 --ff UNMAP,SECONDARY,QCFAIL,DUP \
    -s -aa -f {input.ref} | gzip > {output.second_round_depth}
    """



# Plot aggregated depths per sequencing rounds/platform
rule plot_depth_per_round:
  input:
    first_round_depth=expand("results/depths/hap2/round_1/{hap2scaffold_prefix}_depth_est.txt.gz", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES),
    second_round_depth=expand("results/depths/hap2/round_2/{hap2scaffold_prefix}_depth_est.txt.gz", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    plot="results/plots/hap2/depths/depth_by_round_boxplot.png"
  log:
    "results/logs/plot_depth_per_round.log"
  shell:
    """
    python scripts/plot_depths_by_round.py {input.first_round_depth} {input.second_round_depth} {output.plot} > {log} 2>&1
    """



# Plot aggregate depths per population
rule plot_depth_per_scaffold_hap1:
  input:
    depth_file="results/depths/hap1/all_rounds/{hap1scaffold_prefix}_depth_est.txt.gz"
  output:
    plot="results/plots/hap1/depths/all_rounds/{hap1scaffold_prefix}_depth_histogram.png"
  shell:
    """
    python scripts/plot_depths.py {input.depth_file} {output.plot}
    """

rule plot_depth_per_scaffold_hap2:
  input:
    depth_file="results/depths/hap2/all_rounds/{hap2scaffold_prefix}_depth_est.txt.gz"
  output:
    plot="results/plots/hap2/depths/all_rounds/{hap2scaffold_prefix}_depth_histogram.png"
  shell:
    """
    python scripts/plot_depths.py {input.depth_file} {output.plot}
    """

rule plot_aggregate_depths_hap1:
  input:
    aggregated_depth=expand("results/depths/hap1/all_rounds/{hap1scaffold_prefix}_depth_est.txt.gz", hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    plot="results/plots/hap1/depths/hap1_aggregate_depth_histogram.png"
  shell:
    """
    python scripts/plot_depths_WG.py {input.aggregated_depth} {output.plot}
    """

rule plot_aggregate_depths_hap2:
  input:
    aggregated_depth=expand("results/depths/hap2/all_rounds/{hap2scaffold_prefix}_depth_est.txt.gz", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    plot="results/plots/hap2/depths/hap2_aggregate_depth_histogram.png"
  shell:
    """
    python scripts/plot_depths_WG.py {input.aggregated_depth} {output.plot}
    """



## STEP 2: Quality control check of HWE and SFS

# Rule to create .txt file of BAM files per population
rule generate_bam_hap2_list_per_population:
  input:
    expand("results/bam_realign/hap2/{sample_prefix}_hap2_realign.bam", sample_prefix=sample_prefixes),
  output:
    "data/lists/hap2/{population}_realign_hap2.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")



rule generate_bam_hap1_list_per_population:
  input:
    expand("results/bam_realign/hap1/{sample_prefix}_hap1_realign.bam", sample_prefix=sample_prefixes),
  output:
    "data/lists/hap1/{population}_realign_hap1.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap1_" in bam_file:
                output.write(f"{bam_file}\n")


# ANGSD by population:  To calculate SFS without ngsParalog filtering
# We try different depth levels to get a better distribution of HWE. 

rule angsd_SFS_control_by_population_on_all_sites:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.arg",
    mafs_file="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.mafs.gz",
    depth_sample="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.depthSample",
    depth_global="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.depthGlobal",
    saf_1="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.saf.idx",
    saf_2="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.saf.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    min_MAF=lambda wildcards: wildcards.min_MAF,
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
    #min_depth=lambda wildcards: wildcards.min_depth,
    #max_depth=lambda wildcards: wildcards.max_depth
  log:
    "results/logs/angsd/hap{hap}/raw/by_popln/angsd_raw_sites_control_hap{hap}_{population}_minMAF{min_MAF}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMAF {params.min_MAF}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doSaf 1\
    -anc {params.ref}\
    &> {log}
    """

# -minMaf 0.01 makes everything with a steep SFS

# Rule for generating global SFS with different depth settings
rule global_SFS_control_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/raw/by_popln/{population}_raw_sites_control_minMAF{min_MAF}.saf.idx"
  output:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_minMAF{min_MAF}.sfs"
  log:
    sfs1="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_minMAF{min_MAF}.log"
  threads: 40
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    """


rule fold_global_SFS_control_by_population:
  input:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_minMAF{min_MAF}.sfs"
  output:
    sfs_folded="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded_minMAF{min_MAF}.sfs"
  log:
    sfs2="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded_minMAF{min_MAF}.log"
  threads: 40
  shell:
    """
    winsfs view --fold {input.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """


# Rule for plotting the folded SFS with different depth settings
rule global_SFS_control_by_population_plots:
  input:
    unfolded_sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_minMAF{min_MAF}.sfs",
    folded_sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_control_folded_minMAF{min_MAF}.sfs"
  output:
    unfolded_plot="results/plots/hap{hap}/SFS/{population}_globalSFS_control_unfolded_minMAF{min_MAF}.png",
    folded_plot="results/plots/hap{hap}/SFS/{population}_globalSFS_control_folded_minMAF{min_MAF}.png"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    """
    Rscript scripts/SFS_1D_graph.R {input.folded_sfs} {output.folded_plot}
    Rscript scripts/SFS_1D_graph.R {input.unfolded_sfs} {output.unfolded_plot}
    """


# Running ANGSD HWE analysis with different depth settings
rule angsd_HWE_by_population_on_control_SNPs:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.arg",
    mafs_file="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.mafs.gz",
    hwe_file="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.hwe.gz",
    depth_sample="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.depthSample",
    depth_global="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    min_depth=lambda wildcards: wildcards.min_depth,
    max_depth=lambda wildcards: wildcards.max_depth
  log:
    "results/logs/angsd/hap{hap}/raw/by_popln/angsd_raw_SNPs_control_hap{hap}_{population}_min{min_depth}_max{max_depth}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -minMapQ 30\
    -minQ 20 \
    -minMaf 0.01\
    -minHWEpval 0.01\
    -setMinDepth {params.min_depth}\
    -setMaxDepth {params.max_depth}\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    &> {log}
    """


# Unzipping the HWE output with different depth settings
rule unzip_hwe_control:
  input:
    zipped_hwe="results/angsd/hap{hap}/raw/by_popln/{population}_raw_SNPs_control_min{min_depth}_max{max_depth}.hwe.gz"
  output:
    unzip_hwe="results/angsd/hap{hap}/raw/by_popln/{population}_hwe_raw_SNPs_control_min{min_depth}_max{max_depth}.lr"
  shell:
    """
    zcat {input.zipped_hwe} > {output.unzip_hwe}
    """

# Generating HWE histograms with different depth settings
rule hwe_histogram_control:
  input:
    lr_file="results/angsd/hap{hap}/raw/by_popln/{population}_hwe_raw_SNPs_control_min{min_depth}_max{max_depth}.lr"
  output:
    plot="results/plots/hap{hap}/HWE/{population}_hwe_raw_SNPs_control_min{min_depth}_max{max_depth}.png"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/hetero_excess_header.R {input.lr_file} {output.plot}"



## STEP 3: IDENTIFY PARALOGOUS REGIONS 

rule generate_clipped_bam_hap2_list_per_population:
  input:
    expand("results/bam_clipped/hap2/{sample_prefix}_hap2_clipped.bam", sample_prefix=sample_prefixes),
    checkpoint="results/checkpoints/hap2/rename_specific_files_checkpoint.txt"
  output:
    "data/lists/hap2/{population}_clipped_hap2.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")

rule generate_clipped_bam_hap2_list_per_population:
  input:
    expand("results/bam_clipped/hap2/{sample_prefix}_hap2_clipped.bam", sample_prefix=sample_prefixes),
    checkpoint="results/checkpoints/hap2/rename_specific_files_checkpoint.txt"
  output:
    "data/lists/hap2/{population}_clipped_hap2.txt"
  wildcard_constraints:
    population="|".join(POPULATIONS)
  run:
    bam_files = input
    output_file = output[0]
    population = wildcards.population

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if population in bam_file and f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")



# Call SNPs with liberal rules for input to ngsParalog
# NOTE: -SNP_pvalue 0.05 so very liberal SNP calls
# NOTE: NO filters applied to gather as much as data as possible as input for ngsParalog, except we put missing data to 20% of individuals
rule angsd_raw_SNP:
  input:
    bam_list="data/lists/hap{hap}/{population}_clipped_hap{hap}.txt"
  output:
    arg_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.arg",
    mafs_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.mafs.gz",
    hwe_file="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.hwe.gz",
    depth_sample="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.depthSample",
    depth_global="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.depthGlobal"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped",
    hap="{hap}",
    population="{population}",
    minInd=lambda wildcards, input: max(1, int(0.2 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/raw/ngsParalog_input/angsd_SNP_raw_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -C 50\
    -GL 1\
    -SNP_pval 0.1\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd} \
    -baq 2\
    -only_proper_pairs 1\
    -P {threads}\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    &> {log}
    """



# Create BED files so including only SNPs into ngsParalog
# NOTE: BED files indexed at 0bp because SAMtools to create pileup requires 0bp index
rule convert_mafs_to_bed:
  input:
    mafs_gz="results/angsd/hap{hap}/raw/ngsParalog_input/{population}_raw_SNPs_clipped.mafs.gz"
  output:
    bed_file="results/bed/hap{hap}/raw_SNPs/{population}_raw_SNPs_clipped.BED"
  shell:
   """
   gunzip -c {input.mafs_gz} | awk 'NR>1 {{print $1, $2 - 1, $2}}' > {output.bed_file}
   dos2unix {output.bed_file}  # Add this line to convert line endings
   """


rule ngsParalog_hap2:
  input:
    bam_ngsPara=lambda wildcards: expand("results/bam_clipped/hap2/{sample}_hap2_clipped.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    ref="data/reference/hap2/lupinehap2.fasta",
    bed_file="results/bed/hap2/raw_SNPs/{population}_raw_SNPs_clipped.BED"
  output:
    paralog_output="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  log:
    "results/logs/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.log"
  params:
    hap2scaffold=lambda wildcards: map_prefix_to_full_scaffold(wildcards.hap2scaffold_prefix, 2),
    minInd=lambda wildcards, input: max(1, int(0.8 * len(input.bam_ngsPara)))
  envmodules:
    "samtools/1.20"
  shell:
    """
    rm -f {output.paralog_output} #remove existing output file if it exists
    touch {output.paralog_output}
    samtools mpileup {input.bam_ngsPara} -A -d 77000000 -q 0 -Q 0 --ff UNMAP,QCFAIL,DUP \
    -l {input.bed_file} -r {params.hap2scaffold} -f {input.ref} 2>> {log} | \
    /home/socamero/ngsParalog/ngsParalog calcLR -infile - -outfile {output.paralog_output} -allow_overwrite 1 \
    -minQ 20 -minind {params.minInd} -mincov 1 \
    -runinfo 1 \
    2>> {log} || true
    """


rule concatenate_paralog_hap2:
  input:
    scaffold_files=lambda wildcards: expand("results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}.lr", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    paralog_final="results/ngs_paralog/hap2/concat/{population}_paralog_clipped_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/concat/{population}_paralog_clipped_WGS.log"
  shell:
    """
    cat {input.scaffold_files} >> {output.paralog_final}
    """


# Print summary of calcLR quantile ranges
rule quantile_summary:
  input:
    "results/ngs_paralog/hap{hap}/concat/{population}_paralog_clipped_WGS.lr"
  output:
    "results/ngs_paralog/hap{hap}/quantile_summary/{population}_calcLR_quantile_summary.txt"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/calcLR_quantile_summary.R {input} {output}"


# Identify false positives from ngsParalog (grab only true Paralogs) and filter out
# NOTE: R script indexes positions back to 1bp start
rule ngsParalog_false_pos:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_clipped_WGS.lr"
  output:
    deviant_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected.BED",
    deviant_snps_bp1="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_clipped_bp1_BH_corrected.lr"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos.R {input.lr_file} {output.deviant_snps} {output.deviant_snps_bp1}
    """


# Identify false positives for varying levels of Benjamini Hochberg critical values
rule ngsParalog_false_pos_BH:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_clipped_WGS.lr"
  output:
    deviant_snps_bp1_BH="results/bed/hap{hap}/deviant_SNPs/BH_correction/{population}_deviant_SNPs_bp1_clipped_BH{BH_VAR}.lr"
  params:
    BH_VAR="{BH_VAR}"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/ngs_paralog_false_pos_BH.R {input.lr_file} {output.deviant_snps_bp1_BH} {params.BH_VAR}
    """


# Create Manhattan Plots [NOTE: Specific Scaffold # otherwise it'll take forever for whole genome!] 
rule ngsParalog_manhattan:
  input:
    lr_file="results/ngs_paralog/hap{hap}/by_popln/{population}_scaffolds/{population}-Scaffold_1.lr"
  output:
    plot="results/plots/hap{hap}/ngsParalog/by_popln/{population}_manhattan_plot_clipped_scaffold_1.png"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/ngs_paralog_graphs.R {input.lr_file} {output.plot}"


## dupHMM - Discover paralogous regions rather than individual sites ##
# Requires calcLR .lr files and depth of coverage .tsv files 

# Convert ngsParalog calcLR outputs (.lr files) to BED format
# This is NOT BH corrected. This is just here for future use
rule convert_calcLR_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_clipped_WGS.lr",
  output:
    bed="results/bed/hap{hap}/deviant_SNPs_clipped/{population}_deviant_SNPs_calcLR.BED"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/convert_calcLR_to_bed.R {input.lr_file} {output.bed}"


# Generate average depth of coverage per site for dupHMM
# Only estimate depth at sites deemed 'paralogous' from raw ngsParalog (pre BH filter)
# Use pre BH filter because take all data as input!
# Only applies to hap2 (need rule for hap1)
rule estimate_average_coverage_per_population:
  input:
    bam_files=lambda wildcards: expand("results/bam_clipped/hap2/{sample}_hap2_clipped.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap2/concat/{population}_paralog_clipped_WGS.lr"
  output:
    bed="results/bed/hap2/deviant_SNPs/{population}_deviant_SNPs_calcLR.BED",
    avg_cov_file="results/coverage/hap2/{population}_average_deviant_SNP_coverage.tsv"
  threads: 2
  log:
    "results/logs/coverage/hap2/{population}_average_coverage.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Convert .lr file to BED format (0-based start)
    awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}

    # Calculate the average coverage per position for all BAM files together using the calcLR BED file
    samtools depth -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
    awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
    sort -k1,1V -k2,2n > {output.avg_cov_file}
    """

# Print summary statistics
rule estimate_coverage_statistics:
  input:
    avg_cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    stats="results/coverage/hap{hap}/{population}_coverage_stats.txt"
  shell:
    """
    python scripts/estimate_coverage_stats.py {input.avg_cov_file} > {output.stats}
    """


# Reduce deviant SNP calls (aka calcLR LRs) to the number of known sites with depth data.
# ANGSD estimates depth globally and per individual, but NOT per site (see -doDepth 1)
rule filter_paralog_by_coverage:
  input:
    lr_file="results/ngs_paralog/hap{hap}/concat/{population}_paralog_clipped_WGS.lr",
    cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    filtered_lr="results/bed/hap{hap}/deviant_SNPs/{population}_coverage_only_paralog_clipped_WGS.lr"
  log:
    "results/logs/filter_lr/hap{hap}/{population}_filtered_deviant_SNPs_clipped.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}
    """

# dupHMM_setup is run across the entire genome Whereas dupHMM_run is across each scaffold. 
rule dupHMM_setup:
  input:
    lr_file="results/bed/hap{hap}/deviant_SNPs/{population}_coverage_only_paralog_clipped_WGS.lr",
    cov_file="results/coverage/hap{hap}/{population}_average_deviant_SNP_coverage.tsv"
  output:
    param_output="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_clipped.par"
  params:
    r_script_path="/home/socamero/ngsParalog/dupHMM.R",
    name_output="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_clipped"
  log:
    "results/logs/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_clipped_setup.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript {params.r_script_path} \
    --lrfile {input.lr_file} \
    --outfile {params.name_output} \
    --covfile {input.cov_file} \
    --lrquantile 0.975 \
    --paramOnly 1 \
    2> {log}
    """


# Now generate coverage per scaffold and per population
# We need to input coverage into dupHMM!
rule estimate_coverage_per_population_and_scaffold_hap1:
  input:
    bam_files=lambda wildcards: expand("results/bam_clipped/hap1/{sample}_hap1_clipped.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr"
  output:
    bed="results/bed/hap1/deviant_SNPs/{population}_scaffolds/{population}_{hap1scaffold_prefix}.BED",
    avg_cov_file="results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """


rule estimate_coverage_per_population_and_scaffold_hap2:
  input:
    bam_files=lambda wildcards: expand("results/bam_clipped/hap2/{sample}_hap2_clipped.bam", sample=[s for s in sample_prefixes if s.startswith(wildcards.population)]),
    raw_lr="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr"
  output:
    bed="results/bed/hap2/deviant_SNPs/{population}_scaffolds/{population}_{hap2scaffold_prefix}.BED",
    avg_cov_file="results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  log:
    "results/logs/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.log"
  threads: 2
  envmodules:
    "samtools/1.20"
  shell:
    """
    # Check if the .lr file is empty and create a BED file accordingly
    if [ -s {input.raw_lr} ]; then
      # Convert non-empty .lr file to BED format (0-based start)
      awk '{{print $1 "\\t" ($2-1) "\\t" $2}}' {input.raw_lr} > {output.bed}
    else
      # Create an empty BED file if .lr file is empty
      touch {output.bed}
    fi

    # If the BED file is not empty, calculate the average coverage
    if [ -s {output.bed} ]; then
      samtools depth -@ 2 -q 0 -Q 0 -J -a -b {output.bed} {input.bam_files} | \
      awk '{{cov[$1"\\t"$2]+=$3; count[$1"\\t"$2]++}} END {{for (pos in cov) print pos, cov[pos] / count[pos]}}' | \
      sort -k1,1V -k2,2n > {output.avg_cov_file}
    else
      # Create an empty average coverage file if BED file is empty
      touch {output.avg_cov_file}
    fi
    """

# Filter .lr files by which there is depth data
# This is similar as before, but we need it by scaffold since...
# dupHMM setup is run across the entire genome ! Where as dupHMM run is run across each scaffold instead. 
rule filter_scaffold_lr_by_coverage_hap1:
  input:
    lr_file="results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file="results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap1/deviant_SNPs/{population}_scaffolds/{population}_{hap1scaffold_prefix}_coverage_only_paralog_clipped.lr"
  log:
    "results/logs/filter_lr/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_filtered_deviant_SNPs_clipped.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """


rule filter_scaffold_lr_by_coverage_hap2:
  input:
    lr_file="results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file="results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv"
  output:
    filtered_lr="results/bed/hap2/deviant_SNPs/{population}_scaffolds/{population}_{hap2scaffold_prefix}_coverage_only_paralog_clipped.lr"
  log:
    "results/logs/filter_lr/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_filtered_deviant_SNPs_clipped.log"
  shell:
    """
    # Filter the .lr file based on the coverage file, ensuring no empty lines
    awk 'NR==FNR && $0!="" {{cov[$1" "$2]=1; next}} $0!="" && cov[$1" "$2] {{print}}' {input.cov_file} {input.lr_file} > {output.filtered_lr}

    # If the filtered file is empty, create an empty file
    if [ ! -s {output.filtered_lr} ]; then
      touch {output.filtered_lr}
    fi
    """

# Run dupHMM with beginning estimated parameters
# adjust --lrquantile based on manhattan plot! 
# adjust --maxcoverage based on .tsv files! Seems like some coverages are HIGH! ~50.. To be slightly conservative, choosing 25 (see coverage stats)
# NOTE: Some scaffolds only have 1 SNP with coverage and/or paralog data and can't be used in dupHMM so we skip these. 
rule dupHMM_run_hap1:
  input:
    lr_file = "results/ngs_paralog/hap1/by_popln/{population}_scaffolds/{population}-{hap1scaffold_prefix}.lr",
    cov_file = "results/coverage/hap1/{population}_scaffolds/{population}_{hap1scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap1/dupHMM/{population}_dupHMM_clipped.par"
  output:
    paralog_region = "results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_clipped_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_clipped_run"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{population}-{hap1scaffold_prefix}_dupHMM_clipped_run.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} \
        --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.97 \
        --maxcoverage 25 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """

# NOTE: coverage is lower using hap2
rule dupHMM_run_hap2:
  input:
    lr_file = "results/ngs_paralog/hap2/by_popln/{population}_scaffolds/{population}-{hap2scaffold_prefix}.lr",
    cov_file = "results/coverage/hap2/{population}_scaffolds/{population}_{hap2scaffold_prefix}_average_coverage.tsv",
    parameters = "results/ngs_paralog/hap2/dupHMM/{population}_dupHMM_clipped.par"
  output:
    paralog_region = "results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_clipped_run.rf"
  params:
    r_script_path = "/home/socamero/ngsParalog/dupHMM.R",
    name_output = "results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_clipped_run"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{population}-{hap2scaffold_prefix}_dupHMM_clipped_run.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    if [ -s {input.lr_file} ] && [ $(cat {input.lr_file} | wc -l) -gt 1 ]; then
        Rscript {params.r_script_path} \
        --lrfile {input.lr_file} \
        --outfile {params.name_output} \
        --covfile {input.cov_file} \
        --lrquantile 0.97 \
        --maxcoverage 20 \
        --paramfile {input.parameters} \
        2> {log} || touch {output.paralog_region}
    else
        echo "Skipping .lr file due to insufficient data (empty or only one row): {input.lr_file}" >> {log}
        touch {output.paralog_region}
    fi
    """


# Combine all dupHMM outputs together into one file per population
rule concatenate_dupHMM_hap1:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap1/dupHMM/{population}_scaffolds/{{population}}-{hap1scaffold_prefix}_dupHMM_clipped_run.rf", population=wildcards.population, hap1scaffold_prefix=HAP1SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap1/dupHMM/{population}_dupHMM_clipped_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap1/dupHMM/{population}_dupHMM_clipped_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


rule concatenate_dupHMM_hap2:
  input:
    dupHMM_files=lambda wildcards: expand("results/ngs_paralog/hap2/dupHMM/{population}_scaffolds/{{population}}-{hap2scaffold_prefix}_dupHMM_clipped_run.rf", population=wildcards.population, hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    dupHMM_final="results/ngs_paralog/hap2/dupHMM/{population}_dupHMM_clipped_WGS.lr"
  log:
    "results/logs/ngs_paralog/hap2/dupHMM/{population}_dupHMM_clipped_WGS.log"
  shell:
    """
    cat {input.dupHMM_files} >> {output.dupHMM_final}
    """


# Convert dupHMM outputs (.lr files) to BED format
rule convert_dupHMM_output_to_BED:
  input:
    lr_file="results/ngs_paralog/hap{hap}/dupHMM/{population}_dupHMM_clipped_WGS.lr"
  output:
    bed="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM.BED"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/convert_dupHMM_to_bed.R {input.lr_file} {output.bed}"

    
## STEP 4: VERIFY PARALOGS REDUCED -> look at SFS with RE-RUN ANGSD

# Print out all sites (monomorphic or not) to later filter out paralogous regions
rule angsd_raw_sites_by_popln:
  input:
    bam_list="data/lists/hap{hap}/{population}_clipped_hap{hap}.txt"
  output:
    all_sites_gz="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites.pos.gz",
    all_sites_arg="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites.arg",
    all_sites_bed="results/bed/hap{hap}/raw_sites/{population}_all_sites.BED"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    angsd_out="results/angsd/hap{hap}/raw/by_popln/{population}_all_sites"
  log:
    angsd_log="results/logs/angsd/hap{hap}/raw/by_popln/{population}_all_sites.log"
  threads: 12
  envmodules:
    "angsd/0.940"
  shell:
    """
    # Extract all sites across the genome
    angsd -bam {input.bam_list} -ref {params.ref} -out {params.angsd_out} \
    -doCounts 1 -dumpCounts 1 -P {threads} &> {log.angsd_log}

    # Convert the ANGSD output to a BED format file
    gunzip -c {params.angsd_out}.pos.gz | awk 'NR > 1 {{print $1, $2-1, $2}}' > {output.all_sites_bed}
    """


# Format all BED files accordingly and make sure they're in UNIX format + tab delimited
rule process_all_sites_bed:
  input:
    "results/bed/hap{hap}/raw_sites/{population}_all_sites.BED"
  output:
    "results/bed/hap{hap}/raw_sites/by_popln/{population}_all_sites_processed.BED"
  shell:
    """
    dos2unix {input}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input} > {output}

    echo "Lines before processing in {input}:"
    wc -l {input}
    echo "Lines after processing in {output}:"
    wc -l {output}
    """

rule process_calcLR_deviant_snps:
  input:
    "results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected.BED"
  output:
    "results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected_processed.BED"
  shell:
    """
    dos2unix {input}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input} > {output}

    echo "Lines before processing in {input}:"
    wc -l {input}
    echo "Lines after processing in {output}:"
    wc -l {output}
    """

rule process_dupHMM_deviant_regions:
  input:
    "results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM.BED"
  output:
    "results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM_processed.BED"
  shell:
    """
    dos2unix {input}
    awk 'NF >= 3 {{print $1"\\t"$2"\\t"$3}}' {input} > {output}

    echo "Lines before processing in {input}:"
    wc -l {input}
    echo "Lines after processing in {output}:"
    wc -l {output}
    """


# Filter out deviant SNPs from all known sites
rule filter_all_sites_by_popln_calcLR:
  input:
    processed_all_sites_bed="results/bed/hap{hap}/raw_sites/by_popln/{population}_all_sites_processed.BED",
    processed_calcLR_snps="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected_processed.BED",
  output:
    filtered_calcLR_bed="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.BED",
    filtered_calcLR_txt="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant calcLR SNPs using bedtools and save to BED
    bedtools subtract -a {input.processed_all_sites_bed} -b {input.processed_calcLR_snps} > {output.filtered_calcLR_bed} 2> {log.calcLR_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_calcLR_bed} > {output.filtered_calcLR_txt}
    """


rule filter_all_sites_by_popln_dupHMM:
  input:
    processed_dupHMM_regs="results/bed/hap{hap}/deviant_SNPs/{population}_deviant_SNPs_dupHMM_processed.BED",
    filtered_calcLR_bed="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.BED",
    filtered_dupHMM_txt="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out dupHMM regions from the calcLR BED file
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.processed_dupHMM_regs} > {output.filtered_dupHMM_bed} 2> {log.dupHMM_log}

    # Convert the filtered BED files to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """


# Index filtered all sites BED file
rule index_all_sites_by_popln_calcLR:
  input: 
    canonical_calcLR_sites="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt"
  output: 
    calcLR_index="results/bed/hap{hap}/canonical_sites/filtered_calcLR/{population}_filtered_calcLR_sites.txt.bin"
  envmodules:
    "angsd/0.940"
  shell: 
    "angsd sites index {input.canonical_calcLR_sites}"


rule index_all_sites_by_popln_dupHMM:
  input:
    canonical_dupHMM_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  output:
    dupHMM_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  envmodules:
    "angsd/0.940"
  shell:
    "angsd sites index {input.canonical_dupHMM_sites}"


# ANGSD by population: To calculate SFS (check if heterozygote excess reduced) with filtered sites by ngsParalog
# Previously attempted just calcLR outputs but this did not remove paralogs. We now incorporate dupHMM and calcLR
rule angsd_SFS_by_population_on_all_sites:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  output:
    arg_file="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.arg",
    mafs_file="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.mafs.gz",
    depth_sample="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.depthSample",
    depth_global="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.depthGlobal",
    saf_1="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.saf.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/by_popln/angsd_canonical_sites_dupHMM_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMaf 0.01\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doSaf 1\
    -anc {params.ref}\
    &> {log}
    """


# FILTERED SFS: Optimize and calculate SFS with folded spectra (--fold) since ancestral state unknown
# Can bootstrap to get confidence intervals
# We use 'winsfs' rather than 'realSFS' because realSFS is computational heavy (>100GB RAM required) plus less accurate
# Based on winsfs github, we allocate ~150GB RAM for ~1B sites (extracted from gzip and wc -l)
# We set seed at 1 for reproducibility
rule global_SFS_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_sites_dupHMM.saf.idx"
  output:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.sfs"
  log:
    sfs1="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.log"
  threads: 40
  shell:
    """
    winsfs {input.saf_idx} -t {threads} --seed 1 -v > {output.sfs} 2> {log.sfs1}
    """


rule fold_global_SFS_by_population:
  input:
    sfs="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM.sfs"
  output:
    sfs_folded="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.sfs"
  log:
    sfs2="results/logs/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.log"
  threads: 40
  shell:
    """
    winsfs view --fold {input.sfs} -v > {output.sfs_folded} 2> {log.sfs2}
    """


# Create SFS Plots on filtered data
rule global_SFS_by_population_plots:
  input:
    sfs_file="results/winSFS/hap{hap}/globalSFS/{population}_hap{hap}_globalSFS_dupHMM_folded.sfs"
  output:
    plot="results/plots/hap{hap}/SFS/{population}_globalSFS_dupHMM_folded.png"
  envmodules:
    "r/4.4.0"
  threads: 2
  shell:
    "Rscript scripts/SFS_1D_graph.R {input.sfs_file} {output.plot}"


# To check F distribution on filtered SNPs using dupHMM and calcLR
rule angsd_HWE_by_population_on_dupHMM_SNPs:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.arg",
    mafs_file="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.depthSample",
    depth_global="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.depthGlobal",
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    # Use ~80% of smaples as minimum number of individuals
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/by_popln/angsd_canonical_SNPs_dupHMM_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minHWEpval 0.01\
    -minMaf 0.01\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    &> {log}
    """


# Unzip hwe to extract desired variables (i.e F values)
rule unzip_hwe_dupHMM:
  input:
    zipped_hwe="results/angsd/hap{hap}/canonical/by_popln/{population}_canonical_SNPs_dupHMM.hwe.gz"
  output:
    unzip_hwe="results/angsd/hap{hap}/canonical/by_popln/{population}_hwe_canonical_SNPs_dupHMM.lr"
  shell:
    """
    zcat {input.zipped_hwe} > {output.unzip_hwe}
    """


# Create histogram of F values for canonical SNPs (excludes header)
rule hwe_histogram_dupHMM:
  input:
    lr_file="results/angsd/hap{hap}/canonical/by_popln/{population}_hwe_canonical_SNPs_dupHMM.lr"
  output:
    plot="results/plots/hap{hap}/HWE/{population}_hwe_canonical_SNPs_dupHMM.png"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/hetero_excess_header.R {input.lr_file} {output.plot}"


## ESTIMATE DEPTH USING RepAdapt Scripts (credit Gabriele Nocchi)
# See: https://github.com/RepAdapt/snp_calling_simple_bcftools_slurm/blob/main/08_cnv_2.sh

# The gene annotation is based on haplotype 2, so we use haplotype2
rule prepare_depth_files:
  input:
    fai="data/reference/hap2/lupinehap2.fasta.fai",
    fasta="data/reference/hap2/lupinehap2.fasta",
    gff="data/annotation/MCG3698_Lupinus_perennis.annotation.gff"
  output:
    genome_bed="data/reference/hap2/lupinus_perennis_genome.bed",
    windows_bed="data/reference/hap2/lupinus_perennis_windows.bed",
    windows_list="data/reference/hap2/lupinus_perennis_windows.list",
    genes_bed="data/reference/hap2/lupinus_perennis_genes.bed",
    genes_list="data/reference/hap2/lupinus_perennis_genes.list"
  shell:
    """
    # Create a genome file for bedtools (chromosome and length)
    awk '{{print $1"\\t"$2}}' {input.fai} > {output.genome_bed}
    
    # Create a BED file of 5000 bp windows using the FAI file
    awk -v w=5000 '{{chr = $1; chr_len = $2;
      for (start = 0; start < chr_len; start += w) {{
          end = ((start + w) < chr_len ? (start + w) : chr_len);
          print chr "\\t" start "\\t" end;
      }}
    }}' {input.fai} > {output.windows_bed}
    
    # Create a sorted list of window locations
    awk -F "\\t" '{{print $1":"$2"-"$3}}' {output.windows_bed} | sort -k1,1 > {output.windows_list}
    
    # Create a BED file for each gene using the GFF file
    awk '$3 == "gene" {{print $1"\\t"$4"\\t"$5}}' {input.gff} | uniq > {output.genes_bed}
    
    # Sort the gene BED file based on the order of the reference (from the FAI)
    cut -f1 {input.fai} | while read chr; do 
        awk -v chr="$chr" '$1 == chr {{print $0}}' {output.genes_bed} | sort -k2,2n; 
    done > genes.sorted.bed
    mv genes.sorted.bed {output.genes_bed}
    
    # Create a sorted list of gene locations
    awk -F "\\t" '{{print $1":"$2"-"$3}}' {output.genes_bed} | sort -k1,1 > {output.genes_list}
    """

rule estimate_depth_RepAdapt:
  input:
    bam="results/bam_realign/hap2/{sample_prefix}_hap2_realign.bam"
  output:
    temp_depth="results/depths/RepAdapt_temp/{sample_prefix}.depth"
  log:
    "results/logs/create_temp_depth/{sample_prefix}.log"
  envmodules:
    "samtools/1.20"
  shell:
    """
    samtools depth --reference {input.fasta} -aa {input.bam} > {output.temp_depth}
    """

rule estimate_depth_RepAdapt_stats:
  input:
    temp_depth="results/depths/RepAdapt_temp/{sample_prefix}.depth",
    genome_bed="data/reference/hap2/lupinus_perennis_genome.bed",
    windows_bed="data/reference/hap2/lupinus_perennis_windows.bed",
    windows_list="data/reference/hap2/lupinus_perennis_windows.list",
    genes_bed="data/reference/hap2/lupinus_perennis_genes.bed",
    genes_list="data/reference/hap2/lupinus_perennis_genes.list"
  output:
    wg="results/depths/RepAdapt_method/{sample_prefix}-wg.txt",
    genes_sorted="results/depths/RepAdapt_method/{sample_prefix}-genes.sorted.tsv",
    windows_sorted="results/depths/RepAdapt_method/{sample_prefix}-windows.sorted.tsv"
  params:
    temp_window="results/depths/RepAdapt_temp/{sample_prefix}-windows.tsv",
    temp_genes="results/depths/RepAdapt_temp/{sample_prefix}-genes.tsv"
  log:
    "results/logs/estimate_depth_RepAdapt/{sample_prefix}.log"
  envmodules:
    "samtools/1.20",
    "bedtools/2.31.0"
  shell:
    """
    set +o pipefail #we force the rule because we tested it line by line and it works, just somehow not in one rule

    # Gene depth analysis: compute the mean depth per gene
    awk '{{print $1"\\t"$2"\\t"$2"\\t"$3}}' {input.temp_depth} | bedtools map -a {input.genes_bed} -b stdin -c 4 -o mean -null 0 -g {input.genome_bed} | awk -F "\\t" '{{print $1":"$2"-"$3"\\t"$4}}' | sort -k1,1 > {params.temp_genes} || true

    join -a 1 -e 0 -o '1.1 2.2' -t $'\\t' {input.genes_list} {params.temp_genes} > {output.genes_sorted} || true

    # Window depth analysis: compute the mean depth per window
    awk '{{print $1"\\t"$2"\\t"$2"\\t"$3}}' {input.temp_depth} | bedtools map -a {input.windows_bed} -b stdin -c 4 -o mean -null 0 -g {input.genome_bed} | awk -F "\\t" '{{print $1":"$2"-"$3"\\t"$4}}' | sort -k1,1 > {params.temp_window} || true

    join -a 1 -e 0 -o '1.1 2.2' -t $'\\t' {input.windows_list} {params.temp_window} > {output.windows_sorted} || true

    # Overall genome depth (average depth across all positions)
    awk '{{sum += $3; count++}} END {{if (count > 0) print sum/count; else print "No data"}}' {input.temp_depth} > {output.wg}

    # Cleanup temporary files
    rm -f {input.temp_depth} {params.temp_genes} {params.temp_window}
    """


rule combine_depth_RepAdapt:
  input:
    wg=expand("results/depths/RepAdapt_method/{sample_prefix}-wg.txt", sample_prefix=sample_prefixes),
    genes=expand("results/depths/RepAdapt_method/{sample_prefix}-genes.sorted.tsv", sample_prefix=sample_prefixes),
    windows=expand("results/depths/RepAdapt_method/{sample_prefix}-windows.sorted.tsv", sample_prefix=sample_prefixes)
  output:
    combined_windows="results/depths/RepAdapt_method/combined_windows.tsv",
    combined_genes="results/depths/RepAdapt_method/combined_genes.tsv",
    combined_wg="results/depths/RepAdapt_method/combined_wg.tsv"
  params:
    depth_header="results/depths/RepAdapt_temp/depthheader.txt",
    samples_list="results/depths/RepAdapt_temp/samples.txt",
    genes_temp="results/depths/RepAdapt_temp/combined-genes.temp",
    windows_temp="results/depths/RepAdapt_temp/combined-windows.temp",
    # Create a newline-separated string of sample names from your Python variable.
    sample_names="\n".join(sample_prefixes)
  log:
    "results/logs/estimate_depth_RepAdapt/combine_depths.log"
  shell:
    """
    set -o pipefail

    # Ensure temporary directory exists.
    mkdir -p results/depths/RepAdapt_temp

    # Create a file with the sample names using the Python-provided list.
    echo -e "{params.sample_names}" > {params.samples_list}

    # Create a header using the sample names.
    # Replace newlines with tabs for the header.
    echo -e "location\\t$(cat {params.samples_list} | tr '\n' '\\t')" > {params.depth_header}

    # Combine window depth results:
    while read samp; do 
      cut -f2 results/depths/RepAdapt_method/${{samp}}-windows.sorted.tsv > results/depths/RepAdapt_method/${{samp}}-windows.depthcol; 
    done < {params.samples_list}
    first_sample=$(head -n1 {params.samples_list})
    paste results/depths/RepAdapt_method/${{first_sample}}-windows.sorted.tsv $(tail -n +2 {params.samples_list} | sed 's/.*/results\/depths\/RepAdapt_method\/&-windows.depthcol/') > {params.windows_temp}
    cat {params.depth_header} {params.windows_temp} > {output.combined_windows}

    # Combine gene depth results:
    while read samp; do 
      cut -f2 results/depths/RepAdapt_method/${{samp}}-genes.sorted.tsv > results/depths/RepAdapt_method/${{samp}}-genes.depthcol; 
    done < {params.samples_list}
    first_sample=$(head -n1 {params.samples_list})
    paste results/depths/RepAdapt_method/${{first_sample}}-genes.sorted.tsv $(tail -n +2 {params.samples_list} | sed 's/.*/results\/depths\/RepAdapt_method\/&-genes.depthcol/') > {params.genes_temp}
    cat {params.depth_header} {params.genes_temp} > {output.combined_genes}

    # Combine whole-genome depth results:
    while read samp; do 
      echo -e "${{samp}}\t$(cat results/depths/RepAdapt_method/${{samp}}-wg.txt)"; 
    done < {params.samples_list} > {output.combined_wg}
    """



#####################################
#### POPULATION GENOMIC ANALYSES ####
#####################################


## ANALYSIS 1 : Relatedness Structure or basically identify potential clones
# PROGRAM: ngsRelate | https://github.com/ANGSD/NgsRelate

# NOTE: set -minMaf to 0.001 which is quite liberal compared to 0.01.
# Previously, did not notice much difference in SFS plots between 0.01 and 0.0001
# ANGSD with -doGlf 3 to prepare input for ngsRelate
# Estimate allele frequencies, genotype likelihoods, and call SNPs
rule angsd_for_ngsRelate:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.arg",
    mafs_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.hwe.gz",
    glf_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.glf.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/angsd_canonical_SNPs_dupHMM_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minHWEpval 0.01\
    -minMaf 0.0001\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doCounts 1\
    -doGlf 3\
    &> {log}
    """
  

rule ngsRelate_prep:
  input:
    mafs_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.mafs.gz",
  output:
    freq_file="results/ngsRelate/hap{hap}/{population}_freq"
  envmodules:
    "gcc",
    "htslib"
  log:
    "results/logs/ngsRelate/hap{hap}/{population}_freq_extraction.log"
  shell:
    """
    mkdir -p $(dirname {output.freq_file})
    zcat {input.mafs_file} | cut -f6 | sed 1d > {output.freq_file} 2>{log}
    """


# Identify potential clones using ngsRelate
rule ngsRelate_analysis:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    glf_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.glf.gz",
    freq_file="results/ngsRelate/hap{hap}/{population}_freq"
  output:
    ngsrelate_output="results/ngsRelate/hap{hap}/{population}_ngsrelate.out"
  params:
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap{wildcards.hap}/{wildcards.population}_realign_hap{wildcards.hap}.txt"))
  log:
    "results/logs/ngsRelate/hap{hap}/{population}_ngsrelate_analysis.log"
  threads: 4
  shell:
    """
    ngsRelate -g {input.glf_file} \
              -n {params.pop_size} \
              -f {input.freq_file} \
              -O {output.ngsrelate_output} \
              &> {log}
    """


rule plot_ngsRelate_output:
  input:
    ngsrelate_out=expand("results/ngsRelate/hap2/{population}_ngsrelate.out", population=POPULATIONS)
  output:
    boxplot="results/plots/hap2/ngsRelate/rab_boxplot.png",
    boxplot_nozeroes="results/plots/hap2/ngsRelate/rab_boxplot_no_zeroes.png"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/plot_ngsRelate.R
    """


rule detect_clones:
  input:
    ngsrelate_out="results/ngsRelate/hap{hap}/{population}_ngsrelate.out"
  output:
    clone_summary="results/ngsRelate/hap{hap}/{population}_clone_summary.txt"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/detect_clones.R {input.ngsrelate_out} {output.clone_summary}"


rule ngsRelate_analysis_inbreeding:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    glf_file="results/angsd/hap{hap}/canonical/ngsRelate_input/by_popln/{population}_canonical_SNPs_dupHMM.glf.gz",
    freq_file="results/ngsRelate/hap{hap}/{population}_freq"
  output:
    ngsrelate_output="results/ngsRelate/hap{hap}/{population}_inbreeding.out"
  params:
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap{wildcards.hap}/{wildcards.population}_realign_hap{wildcards.hap}.txt"))
  log:
    "results/logs/ngsRelate/hap{hap}/{population}_ngsrelate_inbreeding_analysis.log"
  threads: 4
  shell:
    """
    ngsRelate -g {input.glf_file} \
              -n {params.pop_size} \
              -F 1\
              -f {input.freq_file} \
              -O {output.ngsrelate_output} \
              &> {log}
    """

rule plot_ngsRelate_inbreeding:
  input:
    ngsrelate_out=expand("results/ngsRelate/hap2/{population}_inbreeding.out", population=POPULATIONS)
  output:
    boxplot="results/plots/hap2/ngsRelate/F_boxplot.png",
    boxplot_nozeroes="results/plots/hap2/ngsRelate/F_boxplot_no_zeroes.png"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/plot_ngsRelate_inbreeding.R
    """


## ANALYSIS 2: Individual-level inbreeding coefficients
# Program: ngsF and ngsF-HMM | https://github.com/fgvieira/ngsF-HMM

# ANGSD with -doGlf 3 to prepare input for ngsF
# Estimate allele frequencies, genotype likelihoods, call SNPs, and get site frequency spectra
# NOTE: ngsF requires variable sites only, so call SNPs
rule angsd_for_ngsF:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin"
  output:
    arg_file="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.hwe.gz",
    saf_1="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.saf.gz",
    glf_gz="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.gz",
    glf_pos="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.pos.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/ngsF_input/by_popln/angsd_canonical_SNPs_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMaf 0.0001\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doSaf 1\
    -doCounts 1\
    -anc {params.ref}\
    -doGlf 3\
    &> {log}
    """


# Grab number of variable sites (SNPs) per population
rule count_variable_sites:
  input:
    mafs_file="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.mafs.gz"
  output:
    site_count="results/ngsF/hap{hap}/by_popln/{population}_SNPs_count.txt"
  shell:
    """
    zcat {input.mafs_file} | wc -l | awk '{{print $1-1}}' > {output.site_count}
    """


# Estimate inbreeding coefficients
# For low-coverage data, set min_epsilon for lower threshold b/w 1e-5 and 1e-9 so algorithm keeps exploring before stopping
rule ngsF_analysis:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    GL3="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.gz",
    SNP_count="results/ngsF/hap{hap}/by_popln/{population}_SNPs_count.txt"
  output:
    ngsF_est="results/ngsF/hap{hap}/by_popln/{population}_ngsF_inbreeding_final.lrt"
  params:
    ngsF_output_base="results/ngsF/hap{hap}/by_popln/{population}_ngsF_inbreeding_iter",
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap{wildcards.hap}/{wildcards.population}_realign_hap{wildcards.hap}.txt")),
    n_sites=lambda wildcards: int(open(f"results/ngsF/hap{wildcards.hap}/by_popln/{wildcards.population}_SNPs_count.txt").read().strip()),
    iterations=5  # Number of iterations
  log:
    "results/logs/ngsF/hap{hap}/by_popln/{population}_inbreeding_estimate.log"
  threads: 4
  shell:
    """
    # Run the first iteration without the --init_values parameter
    echo "Running ngsF iteration 1"
    zcat {input.GL3} |\
    ngsF --glf -\
         --n_threads {threads}\
         --calc_LRT 1\
         --out {params.ngsF_output_base}_1\
         --n_ind {params.pop_size}\
         --n_sites {params.n_sites}\
         --init_values r\
         --min_epsilon 1e-7\
         &>> {log}

    # Loop over the remaining iterations
    for iter in $(seq 2 {params.iterations}); do
        echo "Running ngsF iteration $iter"

        # Use the .pars file from the previous iteration as the initial values for the current iteration
        zcat {input.GL3} |\
        ngsF --glf -\
             --n_threads {threads}\
             --calc_LRT 1\
             --out {params.ngsF_output_base}_$iter\
             --n_ind {params.pop_size}\
             --n_sites {params.n_sites}\
             --init_values {params.ngsF_output_base}_$((iter - 1)).pars\
             --min_epsilon 1e-7\
             &>> {log}
    done

    # Copy the final iteration output to the expected result
    cp {params.ngsF_output_base}_{params.iterations}.lrt {output.ngsF_est}
    """


# Use ngsF-HMM which uses a 2-step hidden-markov model
rule ngsF_HMM_analysis:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    GL3="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.gz",
    GL3_pos="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.pos.gz",
    SNP_count="results/ngsF/hap{hap}/by_popln/{population}_SNPs_count.txt"
  output:
    ngsFHMM_est="results/ngsF/hap{hap}/by_popln/{population}_ngsF-HMM_inbreeding.indF",
    GL3_unzipped="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf",
    GL3_pos_unzipped="results/angsd/hap{hap}/canonical/ngsF_input/by_popln/{population}_canonical_SNPs.glf.pos"
  params:
    ngsFHMM_output_base="results/ngsF/hap{hap}/by_popln/{population}_ngsF-HMM_inbreeding",
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap{wildcards.hap}/{wildcards.population}_realign_hap{wildcards.hap}.txt")),
    n_sites=lambda wildcards: int(open(f"results/ngsF/hap{wildcards.hap}/by_popln/{wildcards.population}_SNPs_count.txt").read().strip())
  log:
    "results/logs/ngsF/hap{hap}/by_popln/{population}_inbreeding_HMM_estimate.log"
  threads: 4
  shell:
    """
    zcat {input.GL3} > {output.GL3_unzipped}
    zcat {input.GL3_pos} > {output.GL3_pos_unzipped}
    ngsF-HMM --geno {output.GL3_unzipped}\
         --n_threads {threads}\
         --pos {output.GL3_pos_unzipped}\
         --out {params.ngsFHMM_output_base}\
         --n_ind {params.pop_size}\
         --n_sites {params.n_sites}\
         --freq r\
         --indF r\
         --loglkl\
         --min_epsilon 1e-7\
         --seed 12345\
         --log 1\
         &>> {log}
    """

rule plot_ngsF_HMM:
  input:
    ngsFHMM_est=expand("results/ngsF/hap2/by_popln/{population}_ngsF-HMM_inbreeding.indF", population=POPULATIONS)
  output:
    plot="results/plots/hap2/ngsF/ngsF-HMM_inbreeding_coeff.tiff"
  log:
    "results/logs/ngsF/hap2/plot_ngsF_HMM_metrics.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/plot_ngsF_HMM.R 
    """






## ANALYSIS 3: PCA for population structure + Admixture
# PROGRAM: PCAngsd | https://github.com/Rosemeis/pcangsd
# PROGRAM 2: EMU | https://github.com/Rosemeis/emu
  # EMU takes into account of non-random missing data long the genome. Since I previously masked paralogs, this could be these regions. 

# Population structure uses all data from all populations
# Therefore, this requires consolidating all .BAM files as well the .BED files from ngsParalog for filtering

rule generate_bam_list_all_populations:
  input:
    expand("results/bam_realign/hap2/{sample_prefix}_hap2_realign.bam", sample_prefix=sample_prefixes)
  output:
    "data/lists/hap2/all_populations_clipped_hap2.txt"
  run:
    bam_files = input
    output_file = output[0]

    with open(output_file, "w") as output:
        for bam_file in bam_files:
            if f"_hap2_" in bam_file:
                output.write(f"{bam_file}\n")


rule combine_population_calcLR_bed_files:
  input:
    lambda wildcards: expand("results/bed/hap2/deviant_SNPs/{population}_deviant_SNPs_calcLR_BH_corrected.BED", population=POPULATIONS)
  output:
    "results/bed/hap2/deviant_sites/hap2_combined_deviant_SNPs_realign_BH_correction.BED"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Combine all BED files for a specific haplotype and remove duplicates
    bedtools merge -i <(sort -k1,1 -k2,2n $(echo {{' '.join(input)}})) > {output}
    """


rule combine_population_dupHMM_bed_files:
  input:
    lambda wildcards: expand("results/bed/hap2/deviant_SNPs/{population}_deviant_SNPs_dupHMM.BED", population=POPULATIONS)
  output:
    "results/bed/hap2/deviant_sites/hap2_combined_dupHMM_regions.BED"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Combine all BED files for a specific haplotype and remove duplicates
    bedtools merge -i <(sort -k1,1 -k2,2n $(echo {{' '.join(input)}})) > {output}
    """


# Extract all known sites from ALL populations. This is to create list sites and later filter from paralogs
# Parallelize across scaffolds as it would take too long for all samples
rule angsd_raw_sites_all_poplns:
  input:
    bam_list="data/lists/hap2/all_populations_clipped_hap2.txt"
  output:
    all_sites_gz="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.pos.gz",
    all_sites_arg="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.arg"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    angsd_out="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites"
  log:
    angsd_log="results/logs/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.log"
  threads: 12
  envmodules:
    "angsd/0.940"
  shell:
    """
    # Extract all sites across the genome for the given scaffold from all populations
    angsd -bam {input.bam_list} \
          -ref {params.ref} \
          -out {params.angsd_out} \
          -doCounts 1 -dumpCounts 1 \
          -P {threads} \
          -r {wildcards.hap2scaffold} \
          &> {log.angsd_log}
    """

# Convert each scaffold's sites to BED format
rule convert_raw_sites_scaffold:
  input:
    all_sites_gz="results/angsd/hap2/raw/all_poplns/{hap2scaffold}_all_sites.pos.gz"
  output:
    bed="results/bed/hap2/raw_sites/by_scaffold/{hap2scaffold}_all_sites.BED"
  threads: 4
  shell:
    """
    # Convert the ANGSD output to BED format for the scaffold
    gzip -cd {input.all_sites_gz} | awk 'NR>1 && NF>=3 {{print $1"\t"$2-1"\t"$2}}' > {output.bed}
    dos2unix {output.bed}
    """

# Combine scaffold BED files 
rule combine_raw_sites_scaffolds:
  input:
    beds=expand("results/bed/hap2/raw_sites/by_scaffold/{hap2scaffold}_all_sites.BED", hap2scaffold=HAP2SCAFFOLDS)
  output:
    all_sites_bed="results/bed/hap2/raw_sites/all_poplns/all_sites.BED"
  threads: 4
  shell:
    """    
    # Conmbine BED format file and check for completeness of data
    cat {input.beds} > {output.all_sites_bed}
    dos2unix {output.all_sites_bed}
    """


rule filter_all_sites_all_populations_calcLR:
  input:
    all_sites_bed="results/bed/hap2/raw_sites/all_poplns/all_sites.BED",
    deviant_snps="results/bed/hap2/deviant_sites/hap2_combined_deviant_SNPs_realign_BH_correction.BED"
  output:
    filtered_sites_bed="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.BED",
    filtered_sites_txt="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt"
  log:
    calcLR_log="results/logs/bedtools/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.all_sites_bed} -b {input.deviant_snps} > {output.filtered_sites_bed} \
    2> {log.calcLR_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3}}' {output.filtered_sites_bed} > {output.filtered_sites_txt}
    """


rule filter_all_sites_all_populations_dupHMM:
  input:
    filtered_calcLR_bed="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.BED",
    dupHMM_sites="results/bed/hap2/deviant_sites/hap2_combined_dupHMM_regions.BED"
  output:
    filtered_dupHMM_bed="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.BED",
    filtered_dupHMM_txt="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap2/canonical_sites/filtered_dupHMM/dupHMM_filtered_sites.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Filter out deviant sites for all populations using bedtools
    bedtools subtract -a {input.filtered_calcLR_bed} -b {input.dupHMM_sites} > {output.filtered_dupHMM_bed} \
    2> {log.dupHMM_log}

    # Convert the filtered BED file to a .txt file formatted for -sites in ANGSD
    awk '{{print $1, $3, $3 + 1}}' {output.filtered_dupHMM_bed} > {output.filtered_dupHMM_txt}
    """


rule index_all_sites_all_popln_calcLR:
  input: 
    calcLR_sites="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_calcLR/calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.calcLR_sites}
    """


rule index_all_sites_all_popln_dupHMM:
  input: 
    dupHMM_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt"
  output: 
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.dupHMM_sites}
    """

# This analysis excludes pruning for LD
# Estimate SAF, HWE, GL with SNPs on entire population
rule angsd_for_PCAngsd:
  input:
    bam_list="data/lists/hap2/all_populations_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt",
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
  output:
    arg_file="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.depthGlobal",
    saf_1="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.idx",
    saf_2="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.gz",
    beagle="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.beagle.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs",
    scaffolds="results/scaffolds/hap2_scaffolds.txt"
  log:
    "results/logs/angsd/hap2/canonical/pcangsd_input/angsd_{hap2scaffold}_canonical_SNPs_all_poplns.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -r {wildcards.hap2scaffold}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 4000\
    -minMapQ 30\
    -minQ 20\
    -minInd 100\
    -minHWEpval 0.01\
    -minMaf 0.0001\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """

# Combine ANGSD beagle outputs files 
rule combine_GLs_all_popln_scaffolds:
  input:
    GL_files=expand("results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.beagle.gz", hap2scaffold=HAP2SCAFFOLDS)
  output:
    all_sites_GL="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.beagle.gz"
  params:
    script="scripts/combine_and_sort_beagle.py"
  threads: 4
  envmodules:
    "python/3.13.2"
  shell:
    """
    python {params.script} {input.GL_files} {output.all_sites_GL}
    """


# Combine ANGSD mafs output files
rule combine_mafs_all_popln_scaffolds:
  input:
    POS_files=expand("results/angsd/hap2/canonical/pcangsd_input/all_poplns_{hap2scaffold}_canonical_SNPs.mafs.gz", hap2scaffold=HAP2SCAFFOLDS)
  output:
    all_sites_POS="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.mafs.gz"
  params:
    script="scripts/combine_and_sort_mafs.py"
  threads: 4
  envmodules:
    "python/3.13.2"
  shell:
    """
    python {params.script} {input.POS_files} {output.all_sites_POS}
    """


# Estimate LD across all samples for LD pruning for each scaffold (to parallelize)
rule ngsLD_all_poplns:
  input:
    bam_list="data/lists/hap2/all_populations_clipped_hap2.txt",
    mafs_file="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.mafs.gz",
    beagle_file="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.beagle.gz"
  output:
    GL_file="results/angsd/hap2/canonical/ngsLD/all_poplns_GL_{hap2scaffold_prefix}.beagle.gz",
    pos_file="results/angsd/hap2/canonical/ngsLD/all_poplns_positions_{hap2scaffold_prefix}.pos.gz",
    ld_output="results/ngsLD/hap2/all_popln/all_poplns_{hap2scaffold_prefix}_ld_output.ld"
  params:
    pop_size=lambda wildcards: sum(1 for _ in open("data/lists/hap2/all_populations_clipped_hap2.txt"))
  log:
    "results/logs/ngsLD/hap2/all_popln_{hap2scaffold_prefix}_ld_estimation.log"
  threads: 4
  shell:
    """
    # Filter beagle file to keep only markers from this scaffold
    zcat {input.beagle_file} | awk -v s="{wildcards.hap2scaffold_prefix}" 'NR==1 || $1 ~ "^"s"__"' > temp_{wildcards.hap2scaffold_prefix}.beagle.txt

    # Strip first three columns and keep GLs only
    awk 'NR==1 {{next}} {{for (i=4; i<=NF; i++) printf "%s%s", $i, (i==NF?"\\n":"\\t")}}' temp_{wildcards.hap2scaffold_prefix}.beagle.txt | gzip > {output.GL_file}
    rm temp_{wildcards.hap2scaffold_prefix}.beagle.txt

    # Filter MAF file and extract chrom+pos
    zcat {input.mafs_file} | awk -v s="{wildcards.hap2scaffold_prefix}" 'NR==1 || $1 ~ "^"s"__"' > temp_{wildcards.hap2scaffold_prefix}.mafs
    awk 'NR>1 {{print $1"\\t"$2}}' temp_{wildcards.hap2scaffold_prefix}.mafs | gzip > {output.pos_file}
    rm temp_{wildcards.hap2scaffold_prefix}.mafs

    # Count number of sites dynamically
    n_sites=$(zcat {output.pos_file} | wc -l)

    # Run ngsLD
    ngsLD --geno {output.GL_file} \
          --probs 1 \
          --n_ind {params.pop_size} \
          --n_sites $n_sites \
          --pos {output.pos_file} \
          --max_kb_dist 50 \
          --out {output.ld_output} \
          --n_threads {threads} \
          --extend_out \
          --verbose 1 \
          &> {log}
    """


# can add --weight_filter to filter edge weights, but need 4th column on weights
# Filter LD SNPs and use info for PCA below
rule prune_graph_all_poplns:
  input:
    ld_input="results/ngsLD/hap2/all_popln/all_poplns_{hap2scaffold_prefix}_ld_output.ld"
  output:
    tsv_file="results/ngsLD/hap2/all_popln/{hap2scaffold_prefix}_ld.tsv",
    pruned_list="results/ngsLD/hap2/all_popln/{hap2scaffold_prefix}_pruned_list.txt",
    pruned_snps="results/ngsLD/hap2/all_popln/{hap2scaffold_prefix}_pruned_snps.txt"
  params:
    prune_graph="/home/socamero/prune_graph/target/release/prune_graph"
  log:
    "results/logs/ngsLD/hap2/all_popln/{hap2scaffold_prefix}_prune_graph.log"
  threads: 4
  shell:
    """
    # Extract snp1, snp2, r2 from ngsLD output (assumes tab-delimited with header)
    awk 'NR>1 {{print $1"\\t"$2"\\t"$7}}' {input.ld_input} > {output.tsv_file}

    # Run prune_graph to get list of pruned SNPs
    {params.prune_graph} --in {output.tsv_file} \
    --out {output.pruned_list} \
    --out-excl {output.pruned_snps} \
    --weight-filter 'column_3 > 0.2' \
    2> {log}
    """

# !!! Need to check that output of Beagle is compatible with removing markers with pruned_list
# Combine ANGSD beagle outputs files 
rule filter_beagle_with_prune_graph:
  input:
    beagle="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.beagle.gz",
    pruned_list=expand("results/ngsLD/hap2/all_popln/{hap2scaffold_prefix}_pruned_list.txt", hap2scaffold_prefix=HAP2SCAFFOLD_PREFIXES)
  output:
    pruned_snps="results/angsd/hap2/canonical/pcangsd_input/all_poplns_pruned_LD_SNPs.beagle.gz"
  log:
    "results/logs/ngsLD/hap2/all_popln/filter_beagle_with_prune_graph.log"
  params:
    script="scripts/filter_beagle_with_prune_graph.py"
  threads: 4
  envmodules:
    "python/3.13.2"
  shell:
    """
    python {params.script} {input.beagle} {input.pruned_list} {output.pruned_snps} 2> {log}
    """


# Calculate PCA on genotype likelihoods using PCAngsd
# .Q output = admixture proportions

rule PCAngsd_all_populations:
  input:
    beagle="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.beagle.gz"
  output:
    admix_Q="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.admix.{K}.Q",
    admix_P="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.admix.{K}.P"
  params:
    file_name="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd"
  log:
    "results/logs/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.{K}.log"
  threads: 8
  envmodules:
    "python/3.12.4"
  shell:
    """
    pcangsd -b {input.beagle}\
    -o {params.file_name}\
    -t {threads}\
    --iter 1000\
    --admix\
    --admix_K {wildcards.K}\
    2> {log}
    """


rule PCAngsd_all_populations_LD_pruned:
  input:
    beagle="results/angsd/hap2/canonical/pcangsd_input/all_poplns_pruned_LD_SNPs.beagle.gz"
  output:
    admix_Q="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd.admix.{K}.Q",
    admix_P="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd.admix.{K}.P"
  params:
    file_name="results/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd"
  log:
    "results/logs/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd.{K}.log"
  threads: 8
  envmodules:
    "python/3.12.4"
  shell:
    """
    pcangsd -b {input.beagle}\
    -o {params.file_name}\
    -t {threads}\
    --iter 1000\
    --admix\
    --admix_K {wildcards.K}\
    2> {log}
    """


# Plot PCAs and admixture plots
# Remove LD pruned datasets for now until that has been resolved
rule PCAngsd_all_populations_plots:
  input:
    pop_info           = "data/lists/hap2/all_samples_to_popln_bam_order.csv",
    admixture_prop     = "results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.admix.{K}.Q"
  output:
    pca_plot        = "results/plots/hap2/PCAngsd/all_popln_SNP_K{K}_PCA.png",
    admix_plot      = "results/plots/hap2/PCAngsd/all_popln_SNP_K{K}_admix.png",
    #ld_pca_plot     = "results/plots/hap2/PCAngsd/all_popln_LD_pruned_SNP_K{K}_PCA.png",
    #ld_admix_plot   = "results/plots/hap2/PCAngsd/all_popln_LD_pruned_SNP_K{K}_admix.png"
  params:
    cov      = "results/pcangsd/hap2/canonical/pcangsd_input/all_popln_canonical_SNP_pcangsd.cov",
    #ld_cov   = "results/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd.cov"
  threads: 2
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/plot_PCA.R  {params.cov}      {input.pop_info} {output.pca_plot}
    Rscript scripts/plot_admixture.R {input.admixture_prop}    {input.pop_info} {output.admix_plot}
    """
    #input ld_admixture_prop  = "results/pcangsd/hap2/canonical/pcangsd_input/all_popln_pruned_LD_SNP_pcangsd.admix.{K}.Q"
    #Rscript scripts/plot_PCA.R  {params.ld_cov}   {input.pop_info} {output.ld_pca_plot}
    #Rscript scripts/plot_admixture.R {input.ld_admixture_prop} {input.pop_info} {output.ld_admix_plot}


# Calculate geographical distances between pairwise individuals between populations
rule PCA_calc_geo_distances:
  input:
    csv="data/lists/hap2/all_samples_geo_coord.csv"
  output:
    dist="results/pcangsd/hap2/canonical/pcangsd_input/pairwise_individ_geodist.csv"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/calculate_popln_geodist.R {input.csv} {output.dist}"


# PROGRAM ngsAdmix - estimate admixture using ngsAdmix
# You can run admixture using PCAngsd OR ngsAdmix.
rule ngsAdmix_analysis:
  input:
    beagle="results/angsd/hap2/canonical/pcangsd_input/all_poplns_canonical_SNPs.beagle.gz"
  output:
    qopt="results/ngsAdmix/hap2/canonical_K{K}.qopt",
    fopt="results/ngsAdmix/hap2/canonical_K{K}.fopt.gz",
  params:
    out_prefix="results/ngsAdmix/hap2/canonical_K{K}"
  log:
    "results/logs/ngsAdmix/hap2/canonical_K{K}.log"
  threads: 8
  envmodules:
    "angsd/0.940"
  shell:
    """
    NGSadmix -likes {input.beagle} \
      -K {wildcards.K} \
      -P {threads} \
      -o {params.out_prefix} &> {log}
    """

# sample_metadata.csv must be in same format as the bamlist from ANGSD
rule plot_ngsAdmix_pie_map:
  input:
    Q="results/ngsAdmix/hap2/canonical_K18.qopt",
    sample_metadata="data/lists/hap2/all_samples_to_popln_bam_order.csv",
    coords="data/lists/hap2/all_popln_geo_coord.csv",
    script="scripts/plot_ngsAdmix_pie_map.R"
  output:
    barplot="results/plots/hap2/ngsAdmix/K18_admixture_barplot.png",
    piemix="results/plots/hap2/ngsAdmix/K18_pie_charts_map.png",
    combined="results/plots/hap2/ngsAdmix/K18_combined_admixture_map.png"
  threads: 4
  envmodules:
    "r/4.4.0",
    "gdal/3.9.1"
  shell:
    """
    Rscript {input.script} \
      {input.Q} \
      {input.sample_metadata} \
      {input.coords} \
      {output.barplot} \
      {output.piemix} \
      {output.combined}
    """

# similar to pie_map except using the R package mapmixture
rule plot_ngsAdmix_mapmixture:
  input:
    Q             = "results/ngsAdmix/hap2/canonical_K18.qopt",
    sample_metadata = "data/lists/hap2/all_samples_to_popln_bam_order.csv",
    coords        = "data/lists/hap2/all_popln_geo_coord.csv",
    script        = "scripts/plot_ngsAdmix_mapmixture.R"
  output:
    barplot       = "results/plots/hap2/ngsAdmix/K18_admixture_barplot_2.png",
    piemix        = "results/plots/hap2/ngsAdmix/K18_pie_charts_map_mapmixture.png",
    combined      = "results/plots/hap2/ngsAdmix/K18_combined_mapmixture.png"
  threads: 4
  envmodules:
    "r/4.4.0",
    "gdal/3.9.1"
  shell:
    """
    Rscript {input.script} \
      {input.Q} \
      {input.sample_metadata} \
      {input.coords} \
      {output.barplot} \
      {output.piemix} \
      {output.combined}
    """

# PROGRAM evalAdmix - evaluate admixture - INCOMPLETE

# PROGRAM: EMU - For non-random missing genetic data - INCOMPLETE

# First call another angsd but output genotypes in PLINK format
rule angsd_for_EMU:
  input:
    bam_list="data/lists/hap{hap}/all_populations_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
  output:
    arg_file="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.depthGlobal",
    beagle="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.beagle.gz",
    plink_bed="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.bed",
    plink_bim="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.bim",
    plink_fam="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs.fam"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/emu_input/all_poplns_canonical_SNPs",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt"
  log:
    "results/logs/angsd/hap{hap}/canonical/emu_input/angsd_canonical_SNPs_all_poplns.log"
  envmodules:
    "angsd/0.940"
  threads: 16
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd 100\
    -minHWEpval 0.01\
    -minMaf 0.0001\
    -geno_minDepth 4\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doGeno -4\
    -doPlink 1\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doPost 1\
    -postCutoff 0.99\
    -doGlf 2\
    &> {log}
    """

## ANALYSIS 4: Runs of Homozygosity (ROH) to look at longer regions of inbreeding across the genome
# PROGRAM 1: RZooRoH | https://cran.r-project.org/web/packages/RZooRoH/vignettes/zooroh-vignette.pdf



# We call for .beagle.gz and .bcf files
# .beagle is based on genotype likelihoods (GL) and .bcf -> .vcf *can* create genotype probabilities, both of which are acceptable for RZooROH
# For this analysis, we use GLs from .beagle.gz. It is also zipped and takes less space. 
# REVIEW IF .BEAGLE.GZ IS BASED ON GL IF -DoGeno 1 (samples Genotype from posterior) 
rule angsd_for_RZooROH:
  input: 
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.depthGlobal",
    beagle="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.beagle.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/RZooROH_input/{population}/{population}_canonical_SNPs",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/RZooROH_input/{population}_angsd_canonical_SNPs.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 4000\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMaf 0.0001\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doPost 1\
    -doCounts 1\
    -doGlf 2\
    -doDepth 1\
    &> {log}
    """


rule RZooROH_get_sample_names:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt"
  output:
    sample_names="data/lists/hap2/{population}_sample_names.txt"
  shell:
    r"""
    awk -F'/' '{{ f=$NF; sub(/_hap2_realign\.bam$/, "", f); print f }}' {input.bam_list} > {output.sample_names}
    """


rule RZooROH_extract_allele_freq:
  input:
    mafs="results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.mafs.gz"
  output:
    allelefreq="results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_allelefreq.txt"
  shell:
    """
    # Skip the header line and print only the sixth column (adjust the column number as required)
    zcat {input.mafs} | awk 'NR>1 {{print $6}}' > {output.allelefreq}
    """


# Convert .beagle.gz files in Oxford Gen format for input into RZooROH
rule RZooROH_prep:
  input:
    beagle_file="results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_canonical_SNPs.beagle.gz",
    script="scripts/convert_beagle_to_rzooroh.py"
  output:
    zoo_txt="results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_Zoo_format.txt"
  envmodules:
    "python/3.11.5"
  shell:
    "python {input.script} {input.beagle_file} {output.zoo_txt}"


# Rscript to run RZooRoH
rule RZooROH_analysis:
  input:
    zoo_txt      = "results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_Zoo_format.txt",
    sample_names = "data/lists/hap2/{population}_sample_names.txt",
    allele_freq  = "results/angsd/hap2/canonical/RZooROH_input/{population}/{population}_allelefreq.txt"
  output:
    placeholder = "results/RZooROH/hap2/RZooROH_analysis/{population}/{population}_placeholder.txt"
  log:
    "results/logs/RZooROH/hap2/{population}_RZooROH_analysis.log"
  envmodules:
    "r/4.4.0"
  params:
    outbase = "results/RZooROH/hap2/RZooROH_analysis/{population}/{population}_summary"
  threads: 4
  shell:
    """
    Rscript scripts/estimate_RZooRoH.R \
      {input.zoo_txt} \
      {input.sample_names} \
      {input.allele_freq} \
      {params.outbase} \
      {wildcards.population} \
      {threads}
    touch {output.placeholder}
    """


# Plot RZooRoH results!
# We plot F (recent) vs F (ancient) with the K value cutoff of 512 for 'recent' inbreeding and anything bigger than K=512 (or smaller fragment) as ancient inbreeding
# We use model 6a from every RZooRoH population run since it was significant!
rule plot_RZooROH_results:
  input:
    realized = expand("results/RZooROH/hap2/RZooROH_analysis/{population}/{population}_realized_MixKR_6a.csv", population = POPULATIONS),
    meta     = "data/lists/hap2/all_popln_geo_coord.csv",
    script   = "scripts/plot_RZooROH.R"
  params:
    recent_thr = 64
  output:
    pdf        = "results/plots/hap2/RZooROH/compare_inbreeding_6a.pdf",
    png_recent = "results/plots/hap2/RZooROH/compare_inbreeding_6a_Frecent.png",
    png_ancient= "results/plots/hap2/RZooROH/compare_inbreeding_6a_Fancient.png",
    png_scatter= "results/plots/hap2/RZooROH/compare_inbreeding_6a_scatter.png"
  threads: 2
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript {input.script} \
      {input.realized} \
      {input.meta} \
      {params.recent_thr} \
      {output.pdf} \
      results/plots/hap2/RZooROH/compare_inbreeding_6a
    """






## ANALYSIS 5: Thetas (nucleotide diversity, etc)
# PROGRAM: ANGSD | https://www.popgen.dk/angsd/index.php/Thetas,Tajima,Neutrality_tests

# We do not filter for MAFs
rule angsd_for_thetas:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.arg",
    mafs_file="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.mafs.gz",
    saf_1="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.saf.idx",
    saf_2="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.saf.pos.gz",
    saf_3="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.saf.gz",
    glf_file="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.glf.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/thetas/by_popln/angsd_canonical_sites_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -doMajorMinor 1\
    -doMaf 1\
    -doGlf 1\
    -doSaf 1\
    -doCounts 1\
    -anc {params.ref}\
    &> {log}
    """

# NOTE: Did not filter with -minMAF 0.001
rule global_SFS_theta_by_population:
  input:
    saf_idx="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.saf.idx",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta"
  output:
    sfs="results/realSFS/hap{hap}/globalSFS/{population}_globalSFS_folded_theta.sfs"
  log:
    "results/logs/realSFS/hap{hap}/globalSFS/{population}_globalSFS_folded_theta.log"
  envmodules:
    "angsd/0.940"
  threads: 40
  shell:
    """
    realSFS {input.saf_idx}\
    -P {threads}\
    -seed 1\
    -fold 1\
    -anc {input.ref}\
    > {output.sfs}\
    2> {log}
    """


rule theta_prep_by_population:
  input:
    sfs="results/realSFS/hap{hap}/globalSFS/{population}_globalSFS_folded_theta.sfs",
    saf_idx="results/angsd/hap{hap}/canonical/thetas/by_popln/{population}_canonical_sites.saf.idx"
  output:
    theta="results/theta/hap{hap}/{population}_out.thetas.gz",
    index="results/theta/hap{hap}/{population}_out.thetas.idx"
  log:
    "results/logs/theta/hap{hap}/{population}_estimate_theta.log"
  params:
    file="results/theta/hap{hap}/{population}_out"
  envmodules:
    "angsd/0.940"
  threads: 4
  shell:
    """
    realSFS saf2theta {input.saf_idx}\
    -sfs {input.sfs}\
    -outname {params.file}\
    -fold 1\
    -P {threads}\
    &> {log}
    """


# Extract log scaled estimates of 
rule estimate_theta_by_sites:
  input:
    theta_index="results/theta/hap{hap}/{population}_out.thetas.idx"
  output:
    file_name="results/theta/hap{hap}/{population}_log_scale.out"
  envmodules:
    "angsd/0.940"
  threads: 4
  shell:
    """
    thetaStat print {input.theta_index}\
    > {output.file_name}
    """

rule estimate_theta_sliding_window:
  input:
    theta_index="results/theta/hap{hap}/{population}_out.thetas.idx"
  output:
    window="results/theta/hap{hap}/{population}_out.thetasWindow.gz.pestPG"
  params:
    file_name="results/theta/hap{hap}/{population}_out.thetasWindow.gz"
  log:
    "results/logs/theta/hap{hap}/{population}_estimate_theta_sliding_window.log"
  envmodules:
    "angsd/0.940"
  threads: 4
  shell:
    """
    thetaStat do_stat {input.theta_index}\
    -win 10000\
    -step 1000\
    -outnames {params.file_name}\
    &> {log}
    """

rule plot_thetas_output:
  input:
    theta_files=expand("results/theta/hap2/{population}_out.thetasWindow.gz.pestPG", population=POPULATIONS)
  output:
    tajimasD_plot="results/plots/hap2/theta/tajimasD_boxplot.tiff",
    nucleotide_div_plot="results/plots/hap2/theta/nucleotide_diversity_violinplot.tiff"
  log:
    "results/logs/theta/hap2/plot_theta_metrics.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript scripts/plot_theta.R
    #Rscript scripts/plot_theta_by_scaffold.R
    """


rule plot_theta_vs_Nc:
  input:
    theta_files = expand("results/theta/hap{hap}/{population}_out.thetasWindow.gz.pestPG",
                         hap=[2], population=POPULATIONS),
    nc_csv      = "data/field_data/Nc_by_site.csv"       # <-- your CSV: Site_code, Site_code2, Range_position, Nc_2023, Nc_2024, Nc_2025
  output:
    scatter = "results/plots/hap{hap}/theta/pi_vs_Nc_{nc_year}.tiff",
    summary = "results/theta/hap{hap}/pi_summary_by_population_{nc_year}.csv"
  params:
    hap = 2,                 # switch to 1 if needed
    nc_year = "2025",        # pick "2023", "2024", or "2025"
    nsites_q = 0.15          # drop bottom 15% nSites within each pop
  log:
    "results/logs/theta/hap{hap}/pi_vs_Nc_{nc_year}.log"
  envmodules:
    "r/4.4.0"
  shell:
    r"""
    Rscript scripts/plot_theta_vs_Nc.R \
      --hap {params.hap} \
      --nc_csv {input.nc_csv} \
      --nsites_q {params.nsites_q} \
      --nc_year {params.nc_year} \
      --scatter {output.scatter} \
      --summary {output.summary} \
      &> {log}
    """

# ANALYSIS 6: Pairwise Fst Between Populations
# We re-use estimated .saf files from the theta analysis

# Use the following in rule all:
#expand("results/realSFS/hap2/fst/{pop1}_{pop2}_fst_global.txt", pop1=[x[0] for x in POP_COMBINATIONS], pop2=[x[1] for x in POP_COMBINATIONS])

rule fst_prep_by_population:
  input:
    pop1_saf_idx="results/angsd/hap2/canonical/thetas/by_popln/{pop1}_canonical_sites.saf.idx",
    pop2_saf_idx="results/angsd/hap2/canonical/thetas/by_popln/{pop2}_canonical_sites.saf.idx"
  output:
    sfs_prior="results/realSFS/hap2/fst/{pop1}_{pop2}_prior.ml"
  log:
    "results/logs/realSFS/hap2/globalSFS_Fst/{pop1}_{pop2}_prior.log"
  envmodules:
    "angsd/0.940"
  threads: 40
  shell:
    """
    realSFS {input.pop1_saf_idx} {input.pop2_saf_idx} \
      -P {threads} \
      -fold 1 \
      > {output.sfs_prior} \
      2> {log}
    """

rule fst_analysis:
  input:
    pop1_saf_idx="results/angsd/hap2/canonical/thetas/by_popln/{pop1}_canonical_sites.saf.idx",
    pop2_saf_idx="results/angsd/hap2/canonical/thetas/by_popln/{pop2}_canonical_sites.saf.idx",
    sfs_prior="results/realSFS/hap2/fst/{pop1}_{pop2}_prior.ml"
  output:
    fst_idx="results/realSFS/hap2/fst/{pop1}_{pop2}.fst.idx"
  params:
    fst_out_prefix="results/realSFS/hap2/fst/{pop1}_{pop2}"
  log:
    "results/logs/realSFS/hap2/globalSFS_Fst/{pop1}_{pop2}_Fst_estimation.log"
  envmodules:
    "angsd/0.940"
  threads: 40
  shell:
    """
    realSFS fst index {input.pop1_saf_idx} {input.pop2_saf_idx} \
      -sfs {input.sfs_prior} \
      -fold 1 \
      -fstout {params.fst_out_prefix} \
      -P {threads} \
      2> {log}
    """

# Review sliding window doesn't include regions where site is low
rule estimate_fst_stats:
  input:
    fst_idx="results/realSFS/hap2/fst/{pop1}_{pop2}.fst.idx"
  output:
    global_fst="results/realSFS/hap2/fst/{pop1}_{pop2}_fst_global.txt"
  params:
    window_size=50000,  # Window size for sliding window Fst
    step_size=10000  # Step size for sliding window Fst
  log:
    "results/logs/realSFS/hap2/globalSFS_Fst/{pop1}_{pop2}_Fst_extract.log"
  envmodules:
    "angsd/0.940"
  shell:
    """
    # Global Fst estimate
    realSFS fst stats {input.fst_idx} > {output.global_fst}
    """

    #window_fst="results/realSFS/hap2/fst/{pop1}_{pop2}_fst_windows.txt"
    # Fst in sliding windows
    # realSFS fst stats2 {input.fst_idx} -win {params.window_size} -step {params.step_size} > {output.window_fst}

# Isolation by distance
rule plot_fst_by_distance:
  input:
    global_fst=expand("results/realSFS/hap2/fst/{pop1}_{pop2}_fst_global.txt", pop1=[x[0] for x in POP_COMBINATIONS], pop2=[x[1] for x in POP_COMBINATIONS]),
    coords="data/lists/hap2/all_popln_geo_coord.csv"
  output:
    fst_plot="results/plots/hap2/fst/fst_by_distance.png"
  log:
    "results/logs/Fst/fst_by_distance.log"
  envmodules:
    "r/4.4.0"
  shell:
    "Rscript scripts/plot_Fst_by_distance.R"


## ANALYSIS 8: Linkage Disequilibrium Decay
# NOTE: this step is necessary for LD pruning prior to the GEA analysis and PCA
# PROGRAM: ngsLD

# We use more slightly stringent rules to call for genotype likelihoods -> minMaf = 0.01 (remove more rare alleles)
# But also liberal for minInd at 50%, and no min/max depth, and no HWE filter for -minHWEpval 0.05
# We use raw genotype likelihoods (GLs) than genotype probabilities (GPs)
# May want to consider exploring minMaf = 0.001 (allow more rare alleles)

rule angsd_for_ngsLD:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    canonical_sites="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap{hap}/lupinehap{hap}.fasta.fai"
  output:
    arg_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.arg",
    mafs_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.mafs.gz",
    glf_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.beagle.gz",
    hwe_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.hwe.gz"
  params:
    ref="data/reference/hap{hap}/lupinehap{hap}.fasta",
    file_name="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites",
    scaffolds="results/scaffolds/hap{hap}_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.5 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap{hap}/canonical/ngsLD/by_popln/angsd_canonical_sites_hap{hap}_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list} \
    -ref {params.ref} \
    -out {params.file_name} \
    -remove_bads 1 \
    -rf {params.scaffolds} \
    -GL 1 \
    -C 50 \
    -sites {input.canonical_sites} \
    -minMapQ 30 \
    -minQ 20 \
    -minInd {params.minInd} \
    -minMaf 0.001 \
    -SNP_pval 1e-6 \
    -baq 2 \
    -only_proper_pairs 1 \
    -nThreads {threads} \
    -doMajorMinor 1 \
    -doHWE 1 \
    -doMaf 1 \
    -doGlf 2 \
    -doCounts 1 \
    -doDepth 1 \
    &> {log}
    """

# Linkage disequilibrium analysis
# Make sure to differ max_kb_dist to see differences!
rule ngsLD_analysis:
  input:
    bam_list="data/lists/hap{hap}/{population}_realign_hap{hap}.txt",
    mafs_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.mafs.gz",
    beagle_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_canonical_sites.beagle.gz"
  output:
    GL_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_GL.beagle.gz",
    pos_file="results/angsd/hap{hap}/canonical/ngsLD/by_popln/{population}_positions.pos.gz",
    ld_output="results/ngsLD/hap{hap}/{population}_ld_output.ld"
  params:
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap{wildcards.hap}/{wildcards.population}_realign_hap{wildcards.hap}.txt")),
    n_sites=lambda wildcards, input: sum(1 for line in gzip.open(input.mafs_file, "rt")) - 1
  log:
    "results/logs/ngsLD/hap{hap}/{population}_ld_estimation.log"
  threads: 4
  shell:
    """
    # Remove the header and first three columns of the Beagle file
    zcat {input.beagle_file} | awk 'NR>1 {{for (i=4; i<=NF; i++) printf "%s%s", $i, (i==NF?"\\n":"\\t")}}' | gzip > {output.GL_file}

    # Extract chromosome and position from the MAF file (no header)
    zcat {input.mafs_file} | awk 'NR>1 {{print $1"\\t"$2}}' | gzip > {output.pos_file}


    ngsLD --geno {output.GL_file} \
    --probs 1 \
    --n_ind {params.pop_size} \
    --n_sites {params.n_sites} \
    --pos {output.pos_file} \
    --max_kb_dist 100 \
    --out {output.ld_output} \
    --n_threads {threads} \
    --extend_out \
    --verbose 1 \
    &> {log}
    """


# identify linkage for pruning (necessary for by_popln) analyses
# can add --weight_filter to filter edge weights, but need 4th column on weights
# use pruned_list = SNPs remaining after pruning; pruned_SNPs = SNPs that were removed 
rule prune_graph_by_popln:
  input:
    ld_input="results/ngsLD/hap{hap}/{population}_ld_output.ld"
  output:
    tsv_file="results/ngsLD/hap{hap}/by_popln/{population}_ld.tsv",
    pruned_list="results/ngsLD/hap{hap}/by_popln/{population}_pruned_list.txt",
    pruned_snps="results/ngsLD/hap{hap}/by_popln/{population}_pruned_snps.txt"
  params:
    prune_graph="/home/socamero/prune_graph/target/release/prune_graph"
  log:
    "results/logs/ngsLD/hap{hap}/by_popln/{population}_prune_graph.log"
  shell:
    """
    # Extract snp1, snp2, r2 from ngsLD output (assumes tab-delimited with header)
    awk 'NR>1 {{print $1"\\t"$2"\\t"$7}}' {input.ld_input} > {output.tsv_file}

    # Run prune_graph to get list of pruned SNPs
    {params.prune_graph} --in {output.tsv_file} \
    --out {output.pruned_list} \
    --out-excl {output.pruned_snps} \
     &> {log}
    """


# Sample a fraction of the LD because there's about 1.5TB of data in total across all populations!
rule sample_ld_values:
  input:
    ld = "results/ngsLD/hap2/{population}_ld_output.ld"
  output:
    sample = "results/ngsLD/hap2/by_popln/{population}_ld_random_sample.txt"
  params:
    frac= 0.001
  shell:
    """ 
    # Skip header, sample column 7 (r2) with prob=frac
    awk -v f={params.frac} 'NR>1 {{ if (rand() < f) print $7 }}' {input.ld} > {output.sample}
    """

rule sample_ld_pairs:
  input:
    ld="results/ngsLD/hap2/{population}_ld_output.ld"
  output:
    sample="results/ngsLD/hap2/by_popln/{population}_ld_pairs_sample.txt"
  params:
    frac=0.001
  shell:
    r"""
    # NR>1 skip header; $3=dist(bp), $7=r2    awk -F '\t' -v f={params.frac} 'NR>1 {{ if (rand()<f) printf "%d\t%f\n", $3, $7 }}' {input.ld} > {output.sample}
    """


# NOTE: At some point this graph needs to incorporate the Hill & Weir (1988) model for expected LD, which accounts for popln size
rule plot_ld_distribution:
  input:
    random_r2=expand("results/ngsLD/hap2/by_popln/{population}_ld_random_sample.txt", population = POPULATIONS)
  output:
    plot="results/plots/hap2/ngsLD/ld_distribution_by_popln.pdf"
  params:
    script="scripts/plot_LD_distribution.R"
  log:
    "results/logs/ngsLD/hap2/plot_ld_distribution.log"
  threads: 2
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript {params.script} {input.random_r2} {output.plot} 2> {log}
    """

rule plot_ld_decay:
  input:
    random_r2_dist=expand("results/ngsLD/hap2/by_popln/{population}_ld_pairs_sample.txt", population = POPULATIONS)
  output:
    plot="results/plots/hap2/ngsLD/ld_decay_with_distance.pdf"
  params:
    script="scripts/plot_LD_decay.R"
  log:
    "results/logs/ngsLD/hap2/plot_ld_decay.log"
  threads: 2
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript {params.script} {input.random_r2_dist} {output.plot} 2> {log}
    """

# We use the Hill and Weir (1988) model and account for population size
# NOTE - This script will *attempt* to plot all of the LD data, but it might be massive depending on the species
# For Lupine, we do NOT use this script. 
# --vanilla for a clean R environment, --quiet to remove verbosity
rule LD_decay_by_popln:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt",
    block_script="/home/socamero/ngsLD/scripts/fit_LDdecay.R",
    ld_input="results/ngsLD/hap2/{population}_ld_output.ld"
  output:
    plot="results/plots/hap2/ngsLD/{population}_plot.pdf"
  params:
    pop_size=lambda wildcards: sum(1 for _ in open(f"data/lists/hap2/{wildcards.population}_clipped_hap2.txt"))
  envmodules:
    "r/4.4.0"
  shell:
    """
    Rscript --vanilla --quiet {input.block_script} \
    --ld_files {input.ld_input} \
    --out {output.plot} \
    --n_ind {params.pop_size}
    """

# Indicate 1. scaffold name 2. starting position 3. end position
rule LD_blocks_by_popln:
  input:
    block_script="/home/socamero/ngsLD/scripts/LD_blocks.sh",
    ld_input="results/ngsLD/hap2/{population}_ld_output.ld"
  output:
    plot="results/plots/hap2/ngsLD/{population}_500k-2000k.pdf"
  shell:
    """
    cat {input.ld_input} | bash {input.block_script} Scaffold_1__1_contigs__length_29266999 500000 2000000

    mv *.pdf {wildcards.population}_500k-2000k.pdf
    mv {wildcards.population}_500k-2000k.pdf /home/socamero/scratch/{output.plot}
    """


## ANALYSIS 9: Contemporary Effective Population Size

# This analysis uses CurrentNe2, which builds upon the theory presented by commonly used NeEstimator
# CurrentNe accounts for full-siblings contributing to LD, while CurrentNe accounts for 
# Confidence intervals is also better estimated in CurrentNe than NeEstimator

# Some criteria for NeEstimator:
  # Since this LD method requires genotype calls, we filter out individuals with coverage lower than 8x
  # We then filter out rare alleles and set a stringent call of removing alleles with <2% freq (-minMaf)
  # We also only allow SNPs where 90% of all individuals are represented (-minInd)
  # Only call genotypes if it reaches beyond 95% cutoff (-postCutoff)

rule angsd_for_NeEstimator:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.mafs.gz",
    depth_sample="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.depthGlobal",
    bcf_file="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.bcf"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs",
    scaffolds="results/scaffolds/hap2_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.9 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap2/canonical/NeEstimator_input/{population}_angsd_canonical_SNPs.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 50\
    -setMaxDepth 4000\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMaf 0.02\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -GL 1\
    -doPost 1\
    -doMajorMinor 1\
    -doMaf 1\
    -dobcf 1\
    --ignore-RG 0\
    -doGeno 2\
    -doCounts 1\
    -doDepth 1\
    -postCutoff 0.95\
    &> {log}
    """


rule convert_bcf_to_vcf:
  input:
    bcf="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.bcf"
  output:
    vcf="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz",
    csi="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz.csi"
  threads: 2
  envmodules:
    "bcftools/1.19"
  shell:
    """
    # just convert the BCF (with GT) to compressed VCF - not necessary to use bcftools since not inferring GT from GL
    bcftools view -Oz -o {output.vcf} {input.bcf}
    bcftools index -f {output.vcf}
    """


rule convert_vcf_GP_to_GT:
  input:
    vcf="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz"
  output:
    vcf_GT="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    vcf_csi="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs_GT.vcf.gz.csi"
  threads: 2
  envmodules:
    "bcftools/1.19"
  shell:
    """
    # 1) Convert GP -> GT (calls GT if max GP >= 0.95, else sets ./.)
    bcftools +tag2tag {input.vcf} -- --GP-to-GT -t 0.95 \
      | bcftools annotate -x FORMAT/AD,FORMAT/DP,FORMAT/PL,FORMAT/GL,FORMAT/GQ,FORMAT/PGT,FORMAT/PS \
      -Oz -o {output.vcf_GT}

    # 2) Index the *VCF*, which produces the .csi next to it
    bcftools index -f {output.vcf_GT}
    """

# check if need to filter out individuals with Q value (admixture coefficient) less than 90%
# however, we don't have many individuals to begin with (<30), so likely won't remove.
rule estimate_Qvalue_by_popln:
  input:
    qopt = "results/ngsAdmix/hap2/canonical_K18.qopt",
    bamlist = "data/lists/hap2/all_populations_clipped_hap2.txt",
    meta = "data/lists/hap2/all_samples_to_popln_bam_order.csv"
  output:
    report = "results/NeEstimator/admixture_q90_flagged.txt"
  log:
    "results/logs/neestimator/flag_admixed_q90.log"
  envmodules:
    "r/4.4.0"
  shell:
    """
    mkdir -p results/logs/neestimator results/NeEstimator
    Rscript scripts/estimate_Qvalue_by_popln.R {input.qopt} {input.bamlist} {input.meta} > {log} 2>&1
    """


# Filter out singletons (based on Waples 2024) and randomly sub-sample to 10,000 SNPs for NeEstimator
rule filter_and_subsample_vcf_for_neestimator:
  input:
    vcf = "results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz"
  output:
    vcf = "results/NeEstimator/{population}_canonical_filtered_10k.vcf.gz",
    tbi = "results/NeEstimator/{population}_canonical_filtered_10k.vcf.gz.tbi"
  threads: 2
  envmodules:
    "bcftools/1.19"
  shell:
    r"""
    # Step 1: Filter VCF for missingness and MAC ≥ 2
    bcftools view -e 'F_MISSING > 0.20 || MAC < 2' -Oz -o tmp_{wildcards.population}.filtered.vcf.gz {input.vcf}
    tabix -f tmp_{wildcards.population}.filtered.vcf.gz

    # Step 2: Randomly select 10,000 variants
    bcftools view -H tmp_{wildcards.population}.filtered.vcf.gz | shuf -n 10000 | cut -f1,2 > tmp_{wildcards.population}_10k_sites.txt

    # Step 3: Subset to those 10,000 SNPs
    bcftools view -R tmp_{wildcards.population}_10k_sites.txt -Oz -o {output.vcf} tmp_{wildcards.population}.filtered.vcf.gz
    tabix -f {output.vcf}

    # Cleanup
    rm -f tmp_{wildcards.population}.filtered.vcf.gz tmp_{wildcards.population}.filtered.vcf.gz.tbi tmp_{wildcards.population}_10k_sites.txt
    """



rule convert_vcf_to_genepop:
  input:
    vcf="results/angsd/hap2/canonical/NeEstimator_input/{population}/{population}_canonical_SNPs.vcf.gz"
  output:
    gen="results/NeEstimator/hap2/{population}_data_input.gen"
  log:
    "results/logs/NeEstimator/hap2/{population}_convert_vcf_to_genepop.log"
  threads: 2
  envmodules:
    "r/4.4.0"
  shell:
    """
    mkdir -p $(dirname {output.gen})
    Rscript scripts/convert_vcf_to_genepop.R {input.vcf} {output.gen} > {log} 2>&1
    """









rule angsd_for_currentNe2:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt",
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.mafs.gz",
    depth_sample="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.depthGlobal",
    bcf_file="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.bcf"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs",
    scaffolds="results/scaffolds/hap2_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.9 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap2/canonical/currentNe2_input/{population}_angsd_canonical_SNPs.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 50\
    -setMaxDepth 4000\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -minMaf 0.01\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -GL 1\
    -doPost 1\
    -doMajorMinor 1\
    -doMaf 1\
    -dobcf 1\
    -doGeno 2\
    -doCounts 1\
    -doDepth 1\
    -postCutoff 0.90\
    &> {log}
    """

rule convert_bcf_to_vcf_currentNe2:
  input:
    bcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.bcf"
  output:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.vcf.gz",
    csi="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.vcf.gz.csi"
  threads: 2
  envmodules:
    "bcftools/1.19"
  shell:
    """
    # just convert the BCF (with GT) to compressed VCF - not necessary to use bcftools since not inferring GT from GL
    bcftools view -Oz -o {output.vcf} {input.bcf}
    bcftools index -f {output.vcf}
    """

rule convert_vcf_GP_to_GT_currentNe2:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs.vcf.gz"
  output:
    vcf_GT="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    vcf_csi="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz.csi"
  threads: 2
  envmodules:
    "bcftools/1.19"
  shell:
    """
    # 1) Convert GP -> GT (calls GT if max GP >= 0.95, else sets ./.)
    bcftools +tag2tag {input.vcf} -- --GP-to-GT -t 0.95 \
      | bcftools annotate -x FORMAT/AD,FORMAT/DP,FORMAT/PL,FORMAT/GL,FORMAT/GQ,FORMAT/PGT,FORMAT/PS \
      -Oz -o {output.vcf_GT}

    # 2) Index the *VCF*, which produces the .csi next to it
    bcftools index -f {output.vcf_GT}
    """

# Use RLPLV .vcf to determine genome size (# of chromosomes)
rule genome_size_morgans_global:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/RLPLV/RLPLV_canonical_SNPs_GT.vcf.gz"
  output:
    genome_size="results/currentNe2/hap2/genome_size_morgans.txt"
  log:
    "results/logs/currentNe2/hap2/genome_size_morgans.log"
  threads: 4
  envmodules:
      "bcftools/1.19"
  shell:
    """
    mkdir -p results/currentNe2/hap2 results/logs/currentNe2/hap2
      if [ -n "{GENOME_MORGANS}" ] && [ "{GENOME_MORGANS}" != "None" ]; then
        echo {GENOME_MORGANS} > {output.genome_size}
        echo "Override genome_morgans={GENOME_MORGANS}" > {log}
      else
        echo "Counting CHROM from RLPLV: {input.vcf}" > {log}
        bcftools query -f '%CHROM
    ' {input.vcf} | sort -u | wc -l | awk '{{print $1}}' > {output.genome_size}
        echo "Derived GS=$(cat {output.genome_size})" >> {log}
        fi
    """

# Run currentNe2 but estimate K parameter (breeding system)
rule currentne2_estK:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    gs="results/currentNe2/hap2/genome_size_morgans.txt"
  output:
    out_all="results/currentNe2/hap2/{population}/{population}_estK_currentNe2_OUTPUT.txt",
    mix="results/currentNe2/hap2/{population}/{population}_estK_currentNe2_mix_OUTPUT.txt"
  log:
   "results/logs/currentNe2/hap2/{population}/{population}_estK.log"
  threads: 4
  shell:
    """
    mkdir -p results/currentNe2/hap2/{wildcards.population} results/logs/currentNe2/hap2/{wildcards.population}
    GS=$(cat {input.gs})
    stem="results/currentNe2/hap2/{wildcards.population}/{wildcards.population}_estK"
    echo "[currentNe2 estK] GS=$GS; VCF={input.vcf}" > {log}
    currentne2 -t {threads} -o $stem {input.vcf} $GS >> {log} 2>&1
    """

# Run currentNe2 but assume panmixia between subpopulations (k = 0)
rule currentne2_k0:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    gs="results/currentNe2/hap2/genome_size_morgans.txt"
  output:
    out_all="results/currentNe2/hap2/{population}/{population}_k0_currentNe2_OUTPUT.txt",
    mix="results/currentNe2/hap2/{population}/{population}_k0_currentNe2_mix_OUTPUT.txt"
  log:
    "results/logs/currentNe2/hap2/{population}/{population}_k0.log"
  threads: 4
  shell:
    """
    mkdir -p results/currentNe2/hap2/{wildcards.population} results/logs/currentNe2/hap2/{wildcards.population}
    GS=$(cat {input.gs})
    stem="results/currentNe2/hap2/{wildcards.population}/{wildcards.population}_k0"
    echo "[currentNe2 k0] GS=$GS; VCF={input.vcf}" > {log}
    currentne2 -t {threads} -k 0 -o $stem {input.vcf} $GS >> {log} 2>&1
    """

# Run currentNe2 assuming (panmixia; fixed k grid)
rule currentne2_kgrid:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    gs="results/currentNe2/hap2/genome_size_morgans.txt"
  output:
    out_all=expand("results/currentNe2/hap2/{{population}}/{{population}}_k{k}_currentNe2_OUTPUT.txt", k=K_GRID),
    out_mix=expand("results/currentNe2/hap2/{{population}}/{{population}}_k{k}_currentNe2_mix_OUTPUT.txt", k=K_GRID)
  log:
    "results/logs/currentNe2/hap2/{population}/{population}_kgrid.log"
  threads: 4
  params:
    # Pass k values to the shell as a space-separated string
    kvals=" ".join(K_GRID)
  shell:
    """
    mkdir -p results/currentNe2/hap2/{wildcards.population} results/logs/currentNe2/hap2/{wildcards.population}
    GS=$(cat {input.gs})
    echo "[currentNe2 kgrid] GS=$GS; VCF={input.vcf}; ks: {params.kvals}" > {log}
    for k in {params.kvals}; do
      stem=results/currentNe2/hap2/{wildcards.population}/{wildcards.population}_k${k}
      echo "[run] k=${k}" >> {log}
      currentne2 -t {threads} -k ${k} -o ${stem} {input.vcf} ${GS} >> {log} 2>&1
    done
        """


# Run currentNe2 and invoke -x allow for population structure within subpoplns
rule currentne2_x:
  input:
    vcf="results/angsd/hap2/canonical/currentNe2_input/{population}/{population}_canonical_SNPs_GT.vcf.gz",
    gs="results/currentNe2/hap2/genome_size_morgans.txt"
  output:
    out="results/currentNe2/hap2/{population}/{population}_x_currentNe2_OUTPUT.txt"
  log:
    "results/logs/currentNe2/hap2/{population}/{population}_x.log"
  threads: 4
  shell:
    """
    mkdir -p results/currentNe2/hap2/{wildcards.population} results/logs/currentNe2/hap2/{wildcards.population}
    GS=$(cat {input.gs})
    stem="results/currentNe2/hap2/{wildcards.population}/{wildcards.population}_x"
    echo "[currentNe2 -x] GS=$GS; VCF={input.vcf}" > {log}
    currentne2 -t {threads} -x -o $stem {input.vcf} $GS >> {log} 2>&1
    """












## ANALYSIS 10: Genotype Environment Association Test

# Data requirements include:
  # (1) Allele frequencies by population
  # (2) Historical and recent climate data
  # (3) Soil data collected from the field
  # (4)



## ANALYSIS 11: Distribution of Fitness Effect (DFE)

# Identify 4fold degenerate sites for inputs into ANGSD (which determines the SFS)
rule find_4fold_degenerate_sites:
  input:
    gff="data/annotation/Lupinus_perennis_genes.gff3",
    fasta="data/reference/hap2/lupinehap2.fasta"
  output:
    fourfold="results/DFE/sites_4D.txt",
    other="results/DFE/sites_CDSother.txt"
  log:
    "results/logs/DFE/find_4D_sites.log"
  shell:
    """
    python scripts/find_4fold_degen.py {input.gff} {input.fasta} > {log} 2>&1
    mv sites_4D.txt {output.fourfold}
    mv sites_CDSother.txt {output.other}
    """

rule convert_DFE_sites_txt_to_bed:
  input:
    fourfold="results/DFE/sites_4D.txt",
    other="results/DFE/sites_CDSother.txt"
  output:
    bed_4fold="results/DFE/sites_4D.BED",
    bed_other="results/DFE/sites_CDSother.BED"
  log:
    "results/logs/bedtools/convert_4fold_and_CDS.log"
  shell:
    """
    awk -F '[:]' '{{OFS="\\t"; print $1, $2-1, $2}}' {input.fourfold} > {output.bed_4fold} 2> {log}
    awk -F '[:]' '{{OFS="\\t"; print $1, $2-1, $2}}' {input.other} > {output.bed_other} 2> {log}
    """


# Filter each population's site list with the 4D and CDS sites
rule filter_by_popln_4D_and_CDS:
  input:
    fourfold_bed="results/DFE/sites_4D.BED",
    cds_bed="results/DFE/sites_CDSother.BED",
    filtered_dupHMM_bed="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.BED",
    filtered_dupHMM_txt="results/bed/hap{hap}/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.txt"
  output:
    filtered_4D_bed="results/bed/hap{hap}/DFE_sites/{population}_4D_filtered.BED",
    filtered_CDS_bed="results/bed/hap{hap}/DFE_sites/{population}_CDS_other_filtered.BED",
    filtered_4D_txt="results/bed/hap{hap}/DFE_sites/{population}_4D_filtered.txt",
    filtered_CDS_txt="results/bed/hap{hap}/DFE_sites/{population}_CDS_other_filtered.txt"
  log:
    dupHMM_log="results/logs/bedtools/hap{hap}/DFE_sites/{population}_filtered_DFE.log"
  envmodules:
    "bedtools/2.31.0"
  shell:
    """
    # Intersect dupHMM-filtered regions with 4D and other CDS sites
    bedtools intersect -a {input.filtered_dupHMM_bed} -b {input.fourfold_bed} > {output.filtered_4D_bed}
    bedtools intersect -a {input.filtered_dupHMM_bed} -b {input.cds_bed} > {output.filtered_CDS_bed}

    # Convert intersected BED files to ANGSD-compatible -sites txt files
    awk '{{print $1, $3}}' {output.filtered_4D_bed} > {output.filtered_4D_txt}
    awk '{{print $1, $3}}' {output.filtered_CDS_bed} > {output.filtered_CDS_txt}
    """


rule index_DFE_4fold_sites:
  input: 
    filtered_4D_txt="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt"
  output: 
    bin_index="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt.bin",
    idx_index="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.filtered_4D_txt}
    """


rule index_DFE_CDS_other_sites:
  input: 
    filtered_CDS_txt="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt"
  output: 
    bin_index="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt.bin",
    idx_index="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt.idx"
  envmodules:
    "angsd/0.940"
  shell: 
    """
    angsd sites index {input.filtered_CDS_txt}
    """


rule angsd_for_DFE_4fold:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt",
    bin_index="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.arg",
    mafs_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.mafs.gz",
    saf_1="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.saf.idx",
    saf_2="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.saf.pos.gz",
    saf_3="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.saf.gz",
    glf_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.glf.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites",
    scaffolds="results/scaffolds/hap2_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap2/canonical/DFE/by_popln/angsd_4fold_sites_hap2_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -baq 2\
    -nThreads {threads}\
    -uniqueOnly 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doGlf 1\
    -doSaf 1\
    -doCounts 1\
    -anc {params.ref}\
    &> {log}
    """


# We do not filter for MAFs
rule angsd_for_DFE_CDS:
  input:
    bam_list="data/lists/hap2/{population}_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt",
    bin_index="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt.bin",
    fasta_fai="data/reference/hap2/lupinehap2.fasta.fai"
  output:
    arg_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.arg",
    mafs_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.mafs.gz",
    saf_1="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.saf.idx",
    saf_2="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.saf.pos.gz",
    saf_3="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.saf.gz",
    glf_file="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.glf.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites",
    scaffolds="results/scaffolds/hap2_scaffolds.txt",
    minInd=lambda wildcards, input: max(1, int(0.8 * sum(1 for _ in open(input.bam_list))))
  log:
    "results/logs/angsd/hap2/canonical/DFE/by_popln/angsd_CDS_other_hap2_{population}.log"
  envmodules:
    "angsd/0.940"
  threads: 8
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -rf {params.scaffolds}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 3500\
    -minMapQ 30\
    -minQ 20\
    -minInd {params.minInd}\
    -baq 2\
    -nThreads {threads}\
    -uniqueOnly 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doGlf 1\
    -doSaf 1\
    -doCounts 1\
    -anc {params.ref}\
    &> {log}
    """
    

rule SFS_4fold_by_population:
  input:
    saf_idx="results/angsd/hap2/canonical/DFE/by_popln/{population}_4fold_sites.saf.idx",
    canonical_sites="results/bed/hap2/DFE_sites/{population}_4D_filtered.txt",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    sfs="results/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_4fold.sfs"
  log:
    "results/logs/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_4fold.log"
  envmodules:
    "angsd/0.940"
  threads: 40
  shell:
    """
    realSFS {input.saf_idx}\
    -P {threads}\
    -seed 1\
    -fold 1\
    -anc {input.ref}\
    > {output.sfs}\
    2> {log}
    """



rule SFS_CDSother_by_population:
  input:
    saf_idx="results/angsd/hap2/canonical/DFE/by_popln/{population}_CDS_other_sites.saf.idx",
    canonical_sites="results/bed/hap2/DFE_sites/{population}_CDS_other_filtered.txt",
    ref="data/reference/hap2/lupinehap2.fasta"
  output:
    sfs="results/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_CDS.sfs"
  log:
    "results/logs/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_CDS.log"
  envmodules:
    "angsd/0.940"
  threads: 40
  shell:
    """
    realSFS {input.saf_idx}\
    -P {threads}\
    -seed 1\
    -fold 1\
    -anc {input.ref}\
    > {output.sfs}\
    2> {log}
    """


# Run DFE - note requires Python 3.10 or 3.12
# NOTE: MUST RUN SNAKEMAKE RULE USING VIRTUAL ENVIRONMENT envs/fastdfe-py312
rule fastdfe_by_population:
  input:
    neut = "results/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_4fold.sfs",
    sel  = "results/realSFS/hap2/DFE/by_popln/{population}_DFE_folded_CDS.sfs"
  output:
    png   = "results/plots/hap2/fastdfe/{population}_fastdfe_plot.png",
    csv   = "results/fastdfe/hap2/by_popln/{population}_fastdfe_discretized.csv",
    json  = "results/fastdfe/hap2/by_popln/{population}_fastdfe_inference.json",
    txt   = "results/fastdfe/hap2/by_popln/{population}_fastdfe_summary.txt"
  log:
    "results/logs/fastdfe/hap2/by_popln/{population}.log"
  envmodules:
    "r/4.5.0",
    "python/3.12.4"
  threads: 2
  params:
    outdir    = "results/fastdfe/hap2/by_popln",
    png       = "results/fastdfe/hap2/by_popln/{population}_fastdfe_plot.png",
    png_location = "results/plots/hap2/fastdfe",
    n_runs    = 100,
    py        = "~/envs/fastdfe-py312/bin/python",
    Rscript     = "scripts/estimate_fastdfe_folded.R"
  shell:
    """
    set -euo pipefail
    mkdir -p $(dirname {log}) {params.outdir}
    MPLBACKEND=Agg \
    PYTHONNOUSERSITE=1 \
    RETICULATE_USE_UV=FALSE \
    RETICULATE_PYTHON="{params.py}" \
    Rscript --vanilla {params.Rscript} \
      --neut "{input.neut}" \
      --sel "{input.sel}" \
      --outdir "{params.outdir}" \
      --pop "{wildcards.population}" \
      --n_runs "{params.n_runs}" \
      > "{log}" 2>&1
    mv {params.png} {params.png_location}
    """


## ANALYSIS 12 : SIFT analysis for mutational load

rule make_ploidy_from_bamlist:
  input:
    bamlist = "data/lists/hap2/{population}_clipped_hap2.txt"
  output:
    ploidy = "data/lists/hap2/ploidy/{population}_ploidy.txt"
  log:
    "results/logs/bcftools/hap2/ploidy/{population}.log"
  envmodules:
    "samtools/1.20"
  shell:
    r"""
    mkdir -p $(dirname {output.ploidy}) $(dirname {log})

    # Pull SM: from @RG lines; \K keeps only the part after SM:
    xargs -a {input.bamlist} -I{{}} bash -lc '
      samtools view -H "{{}}" |
      grep -oP "\tSM:\K[^\t]+" || true
    ' \
    | LC_ALL=C sort -u \
    | awk '{{print $1 "\t2"}}' > {output.ploidy} 2> {log}
    """



# NOTE: we restrict the analysis to the first 24 scaffolds
# SNP‐calling rule, one job per population:
rule bcftools_snp_call_by_population:
  input:
    ref     = "data/reference/hap2/lupinehap2.fasta",
    bamlist = "data/lists/hap2/{population}_clipped_hap2.txt",
    scaffolds = "results/scaffolds/hap2_scaffolds.txt"
  output:
    vcf = "results/bcftools/hap2/vcf_by_popln/raw/{population}.vcf.gz",
    tbi = "results/bcftools/hap2/vcf_by_popln/raw/{population}.vcf.gz.tbi"
  log:
    "results/logs/bcftools/SNP_call_{population}.log"
  threads: 12
  envmodules:
    "bcftools/1.22"
  shell:
    """
    bcftools mpileup -Ou \
      -f {input.ref} \
      --bam-list {input.bamlist} \
      -q 5 \
      --regions-file {input.scaffolds} \
      -I \
      -a FMT/AD,FMT/DP \
    | bcftools call \
      -V indels -f GQ -mv -Oz \
      --threads {threads} \
      -o {output.vcf} 2> {log}
    tabix -f {output.vcf}
    """

# NOTE: we restrict the analysis to non-paralogous regions and the first 24 scaffolds
rule bcftools_snp_call_canonical_by_population:
  input:
    ref     = "data/reference/hap2/lupinehap2.fasta",
    bamlist = "data/lists/hap2/{population}_clipped_hap2.txt",
    canonical_sites = "results/bed/hap2/canonical_sites/filtered_dupHMM/{population}_filtered_sites_dupHMM_calcLR.BED"
  output:
    vcf = "results/bcftools/hap2/vcf_by_popln/canonical/{population}_canonical.vcf.gz",
    tbi = "results/bcftools/hap2/vcf_by_popln/canonical/{population}_canonical.vcf.gz.tbi"
  log:
    "results/logs/bcftools/SNP_call_{population}_canonical.log"
  threads: 12
  envmodules:
    "bcftools/1.22"
  shell:
    """
    bcftools mpileup -Ou \
      -f {input.ref} \
      --bam-list {input.bamlist} \
      -q 5 \
      --targets-file {input.canonical_sites} \
      --max-depth 3500 \
      -I \
      -a FMT/AD,FMT/DP \
      --threads {threads} \
    | bcftools call \
      -V indels -f GQ -mv -Oz \
      --threads {threads} \
      -o {output.vcf} 2> {log}
    tabix -f {output.vcf}
    """


## ANALYSIS 13: Phylogenetic tree based on ML

# samples: TSV mapping sample -> BAM path, pop -> population code
# ref: reference fasta


rule angsd_for_phylotree:
  input:
    bam_list="data/lists/hap2/all_populations_clipped_hap2.txt",
    canonical_sites="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt",
    bin_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.bin",
    idx_index="results/bed/hap2/canonical_sites/filtered_dupHMM/dupHMM_calcLR_filtered_sites.txt.idx"
  output:
    arg_file="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.arg",
    mafs_file="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.mafs.gz",
    hwe_file="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.hwe.gz",
    depth_sample="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.depthSample",
    depth_global="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.depthGlobal",
    saf_1="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.idx",
    saf_2="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.pos.gz",
    saf_3="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.saf.gz",
    beagle="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs.beagle.gz"
  params:
    ref="data/reference/hap2/lupinehap2.fasta",
    file_name="results/angsd/hap2/canonical/phylotree_input/all_poplns_{hap2scaffold}_canonical_SNPs",
    scaffolds="results/scaffolds/hap2_scaffolds.txt"
  log:
    "results/logs/angsd/hap2/canonical/phylotree_input/angsd_{hap2scaffold}_canonical_SNPs_all_poplns.log"
  envmodules:
    "angsd/0.940"
  threads: 12
  shell:
    """
    angsd -bam {input.bam_list}\
    -ref {params.ref}\
    -out {params.file_name}\
    -remove_bads 1\
    -r {wildcards.hap2scaffold}\
    -GL 1\
    -C 50\
    -sites {input.canonical_sites}\
    -setMinDepth 25\
    -setMaxDepth 4000\
    -uniqueOnly 1\
    -minMapQ 30\
    -minQ 20\
    -minInd 100\
    -minHWEpval 0.01\
    -minMaf 0.0001\
    -baq 2\
    -only_proper_pairs 1\
    -nThreads {threads}\
    -SNP_pval 1e-6\
    -doHWE 1\
    -doCounts 1\
    -doDepth 1\
    -doMajorMinor 1\
    -doMaf 1\
    -doSaf 1\
    -anc {params.ref}\
    -doGlf 2\
    &> {log}
    """


rule ngsdist_pairwise:
  input:
    beagle = "results/angsd/hap2/gl_tree/all_samples.beagle.gz",
    mafs   = "results/angsd/hap2/gl_tree/all_samples.mafs.gz",
    labels = "data/sample_labels.txt"   # one label per line, in same order as beagle individuals
  output:
    dist = "results/tree/ngsdist/distances.mat"
  log:
    "results/logs/tree/hap2/ngsdist.log"
  threads: 16
  envmodules:
    "ngstools/1.0"  # adjust to your module naming
  shell:
    r"""
    mkdir -p $(dirname {output.dist}) $(dirname {log})
    ngsDist --geno {input.beagle} --probs \
            --labels {input.labels} \
            --out {output.dist} --n_threads {threads} \
            > {log} 2>&1
    """

rule fastme_tree:
  input:
    dist = "results/tree/ngsdist/distances.mat"
  output:
    tree = "results/tree/ngsdist/tree_fastme.nwk"
  log:
    "results/logs/tree/hap2/fastme.log"
  envmodules:
    "fastme/2.1.6"
  shell:
    r"""
    fastme -i {input.dist} -o {output.tree} -n > {log} 2>&1
    """

















## ARCHIVE

# Previously, the @SM sample metadata was not the same for re-sequenced libraries. 
# This mean't that .bam files from the same sample were mistakenly taken as seaprate indiiduals.
# This problem is now fixed!
# Below is the rule to fix the .bam files. 

rule fix_sm_lb_per_bam:
  input:
    bam="{bam}"
  output:
    done="{bam}.smfix.DONE"
  log:
    "{bam}.smfix.log"
  envmodules:
    "samtools/1.20"
  threads: 2
  shell:
    r"""
    set -euo pipefail
    BAM="{input.bam}"
    base="$(basename "$BAM")"
    sample="${{base%%_hap2_realign.bam}}"
    prefix="${{sample%%_*}}"
    lb="${{prefix}}_LB"

    tmp_hdr="$(mktemp)"
    samtools view -H "$BAM" \
    | awk -v sm="$prefix" -v lb="$lb" -F'\t' 'BEGIN{{OFS="\t"}}
        $1=="@RG"{{smset=0; lbset=0;
          for(i=1;i<=NF;i++){{ if($i ~ /^SM:/){{$i="SM:" sm; smset=1}}
                                else if($i ~ /^LB:/){{$i="LB:" lb; lbset=1}} }}
          if(!lbset){{$0=$0 OFS "LB:" lb}}
          if(!smset){{$0=$0 OFS "SM:" sm}}
          print; next}}
        {{print}}
      ' > "$tmp_hdr" 2>> {log}

    if samtools reheader -i "$tmp_hdr" "$BAM" 2>> {log}; then
      samtools index -@4 "$BAM" 2>> {log}
      rm -f "$tmp_hdr"
      echo "In-place OK: $BAM (SM=$prefix LB=$lb)" >> {log}
    else
      out="${{BAM}}.tmp.smfix.bam"
      echo "In-place failed; rewriting to $out" >> {log}
      samtools reheader "$tmp_hdr" "$BAM" > "$out" 2>> {log}
      samtools index -@4 "$out" 2>> {log}
      mv -f "$out" "$BAM"
      mv -f "$out.bai" "$BAM.bai"
      rm -f "$tmp_hdr"
    fi

    touch {output.done}
    """
